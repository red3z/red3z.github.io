<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>一篇文章讲完HashMap的所有</title>
      <link href="/posts/36d6c821.html"/>
      <url>/posts/36d6c821.html</url>
      
        <content type="html"><![CDATA[<h1>HashMap的实现原理</h1><p>​首先, HashMap是用来干啥的? 我们希望快速的从HashMap查找数据并访问, 要多快? O(1)的时间复杂度. 所有==查找算法==中, 能够到达O(1)的那就是==Hash查找算法==. 所以HashMap其实就是维护了一个数组, 这个数组的每一格存储的是什么东西? <font color='red'>Node</font>对象, Node是==单链表的结构==, 有个属性是Node类型的, 用来指向单链表的下一个结点, Node里面的Kay和Value就是我们所使用的Key和Value.</p><blockquote><p><strong>查找算法</strong>: 1.哈希查找; 2.二分查找; 3.树查找(二叉搜索树, 红黑树, AVL树, B树, B+树)</p><p><strong>Hash查找算法:</strong> Hash查找算法是根据元素, 计算出一个==hashcode==, 再根据这个hashcode来算出元素的存储位置</p><p><strong>单链表结构:</strong> 用来解决hash冲突, 链表的长度超过8时, 变为红黑树</p><p><strong>hashcode():</strong> hashCode()根据对象的内存地址计算哈希值，, 但如果需要根据对象的内容生成哈希值, ‌就需要重写hashCode(), hashCode()得出的范围是有限的，‌而对象的属性是无限的，‌因此‌不同的对象可能会计算出相同的hashCode值</p></blockquote><hr><h1>get(key) put(key, value)的过程</h1><ol><li>先根据key计算出hashcode</li><li>在==根据hashcode计算出key在数组的位置==</li><li>单链表顺序搜索或者红黑树搜索<ul><li>如果要put, 则进行尾插法</li></ul></li></ol><blockquote><p><strong>根据hash得出位置</strong>需要经过两次哈希:</p><ol><li>将hash右移16位 再 和自己异或(利用到高位的数字充分散列)</li><li>key的hashCode &amp; (数组长度-1), 得出存储的位置<ul><li>为什么不用取位操作? 因为如果要扩容取的位数就不一样, 还要单独存储要取多少位的变量</li></ul></li></ol></blockquote><hr><h1>HashMap一般用什么作key?</h1><p>String重写了<code>hashCode()</code>和<code>equals()</code>方法</p><p>String‌作为key时, 因为String==不可变==, 避免因为对象的修改导致哈希值变化, 从而影响查找结果</p><blockquote><p>不可变: 不可变对象是该对象在创建后它的哈希值不能改变</p></blockquote><hr><h1>HashMap的扩容机制</h1><p>​HashMap什么时候开始扩容?  在链表要转红黑树时, 如果数组小于64, 扩容. 当有 capacity * 0.75 个元素的时候, 就扩容, capacity一开始是16. 每次==扩为原来的两倍==</p><p>​扩容之后, 要对之前的元素进行迁移, 如果只有值没有链表, 则位置不变; 如果有链表, key的hashCode &amp; 旧数组的长度, 如果结果是0，那么当前元素的桶位置不变。如果结果为1，那么桶的位置就是原位置+原数组长度; 如果是树, 位置不变.</p><blockquote><p><strong>HashMap容量为什么一定是2的n次方?</strong> 2的n次幂 -1的结果是0后面全是1, 这样才能和hashCode与运算得出存储位置</p></blockquote><hr><h1>为什么要用红黑树? 为什么不用AVL, B树?</h1><p>红黑树更加适合修改密集任务, AVL跟红黑树相比, 要保证绝对平衡, 修改操作比较复杂</p><p>红黑树一般存的结点数不会太多, B树的构建代价比较大</p><p><strong>链表什么时候变成红黑树?</strong></p><p>数组长度大于等于64(如果数组小于64优先扩容), 链表大于8(链表插入第九个Node时)</p><p>为啥是8和64?</p><p>​基于泊松分布的计算, 出现红黑树的概率比较低</p><p><strong>红黑树什么时候退化为链表?</strong></p><p>链表长度为6的时候, 退化.</p><hr><h1>ConcurrentHashMap</h1><p>在对CHM进行写操作也就是put操作要保证线程安全</p><ul><li><p>数据要写在数组上时, 直接采用CAS;</p></li><li><p>数据要写在链表或者红黑树上时, 会基于synchronized加锁, 只要不冲突就不会有并发问题, 效率很高</p></li><li><p>Node的val和next使用volatile修饰; 数组用volatile修饰，保证扩容时被读线程感知</p></li><li><p>扩容时，阻塞所有的读写操作、并发扩容</p></li></ul><p><strong>CHM的计数器怎么保证线程安全?</strong></p><p>计数器记录元素个数, 只需要++和–, 但是要保证线程安全</p><p>如果Atomic原子类, 基于CAS实现, 多个线程等待一个baseCount都在浪费CPU</p><p>CHM用LongAdder除了baseCount, 还提供了CounterCell数组, 最后将多个CounterCell加到一起</p>]]></content>
      
      
      <categories>
          
          <category> Java基础 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>一篇文章讲完数据库索引的所有</title>
      <link href="/posts/36d6c821.html"/>
      <url>/posts/36d6c821.html</url>
      
        <content type="html"><![CDATA[<h1>谈谈索引</h1><p><strong>索引用来做什么?</strong></p><p>MySQL的数据量有时候会比较大, 需要设计一种查找的数据结构和算法来提升查找的效率</p><p>Mysql的索引可以提升对数据访问的效率, 因为更新语句是将数据查出来修改后再写回磁盘, 因此更新的速度也更快</p><p>数据量比较大的时候, 是不能一次性把数据都读取到内存的, 所以要分块读取, ==InnoDB存储引擎每次读16KB的数据==</p><blockquote><p>为什么InnoDB存储引擎每次读取16KB的数据?</p><p>操作系统页大小为4kb，为了让操作系统取出来的页对应我们mysql中的页，我们可以把mysql页大小设置为操作系统页的整数倍.</p></blockquote><p>MySQL数据和索引都是存在磁盘的, 磁盘在与内存交互的时候要进行IO操作, IO操作是比较慢的, 所以设计索引的核心原则就是要尽量的减少IO次数</p><p><strong>那这个索引用什么实现?</strong></p><p>因为本质上是查找问题, 可以用哈希, 也可以用树.</p><p>哈希的缺点:</p><ol><li>不适用于范围查询,</li><li>可能引起数据散列不均匀, 退化成链表,</li><li>需要大量内存空间</li></ol><p>二叉树, 红黑树, AVL树都是二叉树, 构造出来的树都太高了, 会导致IO次数多, 所以用多叉树</p><p>B树的每个结点都存储了索引和数据,</p><ol><li>而B+树的非叶子结点只存储索引不存储数据, 就会比B树更矮, 可以更快的缩小查询范围, 千万级数据量B+只有3-4层</li><li>叶子与叶子结点之间用了双向链表, 支持了范围查询</li><li>查找比较稳定, 便于优化</li></ol><p>所以MySQL用了B+树</p><p><strong>索引的分类</strong></p><ol><li>聚簇与非聚簇: ==回表== ==索引覆盖==</li><li>单列索引, 联合索引(复合索引): ==最左匹配原则==  ==索引失效==</li></ol><blockquote><p>**回表: **从某个索引的叶子结点获取聚簇索引的id值, 根据id再去获取数据</p><p><strong>索引覆盖:</strong>  查询的字段全部在索引的字段中, 从索引的叶子节点能获取到全量查询列, 少了回表操作, 因此速度更快(InnoDB非主键索引的叶子存储的是主键和其他带索引列数据)</p><p>**最左匹配: **</p><p>​最左匹配的原理: 在索引B+树中, A+B的联合索引, A是有序的, B是基于A才有序的.</p><p>​查询的where条件中必须包含联合索引中最左边的列, where只包含a的话, 但由于下一个字段 b 的缺失，所以只能把 a = 1 的数据主键ID都找到, 通过回表查询, 比全表扫描强.</p><p>​范围查询是无序的, 所以匹配到范围查询就停止了</p><p><strong>索引失效:</strong> 联合索引违背最左匹配原则就会发生索引失效, 不走索引</p><p>​索引失效的其他场景: where用了函数, where用了like且第一个是通配符,</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Java基础 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>历史最强LeetCode题单</title>
      <link href="/posts/a8baaf51.html"/>
      <url>/posts/a8baaf51.html</url>
      
        <content type="html"><![CDATA[<h1>ACM风格</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 一个一个读</span></span><br><span class="line"><span class="type">BufferedReader</span> <span class="variable">bufferedReader</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(System.in));</span><br><span class="line"><span class="comment">// 一个一个读数字</span></span><br><span class="line"><span class="type">StreamTokenizer</span> <span class="variable">in</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StreamTokenizer</span>(bufferedReader);</span><br><span class="line"><span class="type">PrintWriter</span> <span class="variable">out</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PrintWriter</span>(<span class="keyword">new</span> <span class="title class_">OutputStreamWriter</span>(System.out));</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (in.nextToken() != StreamTokenizer.TT_EOF) &#123;</span><br><span class="line">    <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> (<span class="type">int</span>) in.nval;</span><br><span class="line">    in.nextToken();</span><br><span class="line">    <span class="type">int</span> <span class="variable">m</span> <span class="operator">=</span> (<span class="type">int</span>) in.nval;</span><br><span class="line"></span><br><span class="line">    out.println(mySolution(m, n));</span><br><span class="line">&#125;</span><br><span class="line">out.flush();</span><br><span class="line">out.close();</span><br><span class="line"><span class="comment">// 一行一行读</span></span><br><span class="line">String line;</span><br><span class="line">String[] parts;</span><br><span class="line"></span><br><span class="line"><span class="type">BufferedReader</span> <span class="variable">in</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(System.in));</span><br><span class="line"><span class="type">PrintWriter</span> <span class="variable">out</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PrintWriter</span>(<span class="keyword">new</span> <span class="title class_">OutputStreamWriter</span>(System.out));</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> ((line = in.readLine()) != <span class="literal">null</span>) &#123;</span><br><span class="line">    ArrayList&lt;Integer&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    parts = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">    <span class="keyword">for</span> (String part: parts) &#123;</span><br><span class="line">        list.add(Integer.parseInt(part));</span><br><span class="line">    &#125;</span><br><span class="line">    out.println(mySolution(list));</span><br><span class="line">&#125;</span><br><span class="line">out.flush();</span><br><span class="line">in.close();</span><br><span class="line">out.close();</span><br></pre></td></tr></table></figure><h1>一些必备知识</h1><table><thead><tr><th>必备知识</th><th></th></tr></thead><tbody><tr><td>最大公约数Greatest Common Divisor</td><td>(a &gt; b)<code>gcd(a, b)&#123;return b == 0 ? a : gcd(b, a % b);&#125;</code></td></tr><tr><td>最小公倍数Least Common Multiple</td><td>(a &gt; b)<code>lcm(a, b)&#123;return a / gcd(a, b) * b;&#125;</code></td></tr><tr><td>同余原理</td><td></td></tr></tbody></table><table><thead><tr><th>一些算法</th><th></th></tr></thead><tbody><tr><td>马拉车算法</td><td></td></tr><tr><td>sweep line算法</td><td></td></tr><tr><td>scan line algorithm扫描线算法</td><td></td></tr><tr><td>Boyer-Moore算法</td><td></td></tr><tr><td>Kadane算法</td><td></td></tr></tbody></table><h1>排序</h1><table><thead><tr><th>排序相关问题</th><th><a href="https://leetcode.cn/problems/sort-an-array/">912.排序数组</a></th></tr></thead><tbody><tr><td>归并排序</td><td><a href="https://leetcode.cn/problems/merge-sorted-array/">88.合并两个有序数组</a> <a href="https://leetcode.cn/problems/count-of-range-sum/">327.区间和的个数</a> <a href="https://leetcode.cn/problems/count-of-smaller-numbers-after-self/">315.右侧小于当前元素的个数</a> <a href="https://leetcode.cn/problems/reverse-pairs/">493.翻转对</a> <a href="https://leetcode.cn/problems/shu-zu-zhong-de-ni-xu-dui-lcof/">LCR170.逆序对</a></td></tr><tr><td>快速排序</td><td>链表的快速排序 <a href="https://leetcode.cn/problems/kth-largest-element-in-an-array/">215.数组中的第K大元素</a></td></tr><tr><td>堆排序</td><td></td></tr><tr><td>基数排序</td><td></td></tr><tr><td>链表排序</td><td><a href="https://leetcode.cn/problems/sort-list/">148.排序链表</a></td></tr></tbody></table><h1>位运算</h1><p>异或的一个性质: x^y=s --&gt; s^y=x</p><p>异或的常用操作: <code>n&amp;(n-1)</code>去除n的最后一个1; <code>n&amp;(~n+1) &lt;==&gt; n&amp;(-n)</code>获取n最后一个1的状态</p><table><thead><tr><th>位运算</th><th></th></tr></thead><tbody><tr><td>位运算</td><td><a href="https://leetcode.cn/problems/missing-number/">268.缺失的数字</a> <a href="https://leetcode.cn/problems/single-number/">136.出现一次的数</a> <a href="https://leetcode.cn/problems/single-number-ii/">137.出现一次的数II</a> <a href="https://leetcode.cn/problems/single-number-iii/">260.出现一次的数III</a> <a href="https://leetcode.cn/problems/number-of-1-bits/">191.位1的个数</a> <a href="https://leetcode.cn/problems/power-of-two/">231.2的幂</a> <a href="https://leetcode.cn/problems/bitwise-and-of-numbers-range/">201.数字范围按位与</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/reverse-bits/">190.颠倒二进制位</a> <a href="https://leetcode.cn/problems/hamming-distance/">461.汉明距离</a> <a href="https://leetcode.cn/problems/divide-two-integers/">29.位运算实现加减乘除</a></td></tr><tr><td>位图</td><td><a href="https://leetcode.cn/problems/design-bitset/">2166.设计位集</a></td></tr></tbody></table><h1>基本数据结构</h1><table><thead><tr><th>基本数据结构的应用</th><th></th></tr></thead><tbody><tr><td>栈的应用</td><td><a href="https://leetcode.cn/problems/evaluate-reverse-polish-notation/">150.逆波兰表达式求值</a> <a href="https://leetcode.cn/problems/longest-absolute-file-path/">388.文件的最长绝对路径</a></td></tr><tr><td>队列的应用</td><td><a href="https://labuladong.online/algo/problem-set/queue/#_362-%E6%95%B2%E5%87%BB%E8%AE%A1%E6%95%B0%E5%99%A8">362.敲击计数器</a> <a href="https://leetcode.cn/problems/number-of-recent-calls/">933.最近的请求次数</a> <a href="https://labuladong.online/algo/problem-set/queue/#_346-%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%AD%E7%9A%84%E7%A7%BB%E5%8A%A8%E5%B9%B3%E5%9D%87%E5%80%BC">346.数据流中的移动平均值</a></td></tr><tr><td>哈希表的应用</td><td>复制(克隆)问题 <a href="https://labuladong.online/algo/problem-set/binary-tree-divide-i/#_1485-%E5%85%8B%E9%9A%86%E5%90%AB%E9%9A%8F%E6%9C%BA%E6%8C%87%E9%92%88%E7%9A%84%E4%BA%8C%E5%8F%89%E6%A0%91">🔒1485.克隆含随机指针的二叉树</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-divide-i/#_1490-%E5%85%8B%E9%9A%86-n-%E5%8F%89%E6%A0%91">🔒1490.克隆N叉树</a> 异位词问题 <a href="https://leetcode.cn/problems/valid-anagram/">242.有效的字母异位词</a> <a href="https://leetcode.cn/problems/group-anagrams/">49.字母异位词分组</a> 其他应用 <a href="https://leetcode.cn/problems/two-sum/">1.两数之和</a> <a href="https://leetcode.cn/problems/first-unique-character-in-a-string/">387.字符串中的第一个唯一字符</a></td></tr><tr><td>堆的应用</td><td>合并k个有序序列 <a href="https://leetcode.cn/problems/merge-k-sorted-lists/">23.合并K个升序链表</a></td></tr><tr><td></td><td>寻找第k大元素 <a href="https://leetcode.cn/problems/find-k-pairs-with-smallest-sums/">373.查找和最小的K对数字</a> <a href="https://leetcode.cn/problems/kth-smallest-element-in-a-sorted-matrix/">378.有序矩阵中第K小的元素</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/kth-largest-element-in-a-stream/">703.数据流的第K大元素</a> <a href="https://leetcode.cn/problems/seat-reservation-manager/">1845.座位预约管理系统</a> <a href="https://leetcode.cn/problems/advantage-shuffle/">870.优势洗牌田忌赛马</a> <a href="https://leetcode.cn/problems/single-threaded-cpu/">1834.单线程CPU</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/top-k-frequent-elements/">347.前K个高频元素</a> <a href="https://leetcode.cn/problems/sort-characters-by-frequency/">451.根据字符出现频率排序</a> <a href="https://leetcode.cn/problems/top-k-frequent-words/">692.前K个高频单词</a></td></tr></tbody></table><h1>数据结构设计</h1><table><thead><tr><th>设计数据结构</th><th></th></tr></thead><tbody><tr><td>频率相关的数据结构</td><td><a href="https://leetcode.cn/problems/maximum-frequency-stack/">895.最大频率栈</a> <a href="https://leetcode.cn/problems/all-oone-data-structure/">432.全 O(1) 的数据结构</a></td></tr><tr><td>LRU与LFU</td><td><a href="https://leetcode.cn/problems/lru-cache/">146. LRU 缓存</a> <a href="https://leetcode.cn/problems/lfu-cache/">460. LFU 缓存</a></td></tr><tr><td>stack</td><td><a href="https://leetcode.cn/problems/min-stack/">155.最小栈</a></td></tr><tr><td>queue</td><td><a href="https://leetcode.cn/problems/design-circular-queue/">622.设计循环队列</a> <a href="https://leetcode.cn/problems/design-circular-deque/">641.设计循环双端队列</a> <a href="https://leetcode.cn/problems/design-front-middle-back-queue/">1670.设计前中后队列</a></td></tr><tr><td></td><td><a href="https://labuladong.online/algo/problem-set/ds-design">353.贪吃蛇</a>  <a href="https://labuladong.online/algo/problem-set/ds-design">379. 电话目录管理系统</a></td></tr><tr><td>heap</td><td><a href="https://leetcode.cn/problems/design-twitter/">355.设计推特</a></td></tr><tr><td>设计迭代器</td><td><a href="https://leetcode.cn/problems/peeking-iterator/">284. 窥视迭代器</a> <a href="https://labuladong.online/algo/problem-set/ds-design/#_251-%E5%B1%95%E5%BC%80%E4%BA%8C%E7%BB%B4%E5%90%91%E9%87%8F">251. 展开二维向量</a></td></tr><tr><td>O(1)删除数组元素</td><td><a href="https://leetcode.cn/problems/insert-delete-getrandom-o1/">380.O(1)插入, 删除和获取随机元素</a> <a href="https://leetcode.cn/problems/insert-delete-getrandom-o1-duplicates-allowed/">381.O(1)插入, 删除和获取随机元素II</a> <a href="https://leetcode.cn/problems/random-pick-with-blacklist/">710.避开黑名单的随机数</a></td></tr><tr><td>中位数</td><td><a href="https://leetcode.cn/problems/find-median-from-data-stream/">295. 数据流的中位数</a></td></tr></tbody></table><h1>二叉树</h1><h2 id="问题类型">问题类型</h2><table><thead><tr><th>二叉树问题类型</th><th>题目</th></tr></thead><tbody><tr><td>序列化</td><td><a href="https://leetcode.cn/problems/serialize-and-deserialize-binary-tree/">297.二叉树序列化与反序列化</a> <a href="https://leetcode.cn/problems/serialize-and-deserialize-bst/">449.BST序列化和反序列化</a> <a href="https://leetcode.cn/problems/find-duplicate-subtrees/">652.寻找重复的子树</a></td></tr><tr><td>迭代遍历</td><td><a href="##%E8%BF%AD%E4%BB%A3%E9%81%8D%E5%8E%86%E4%BA%8C%E5%8F%89%E6%A0%91">迭代遍历二叉树框架</a></td></tr><tr><td>最近公共祖先</td><td><a href="https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-search-tree/">235.BST公共祖先</a> <a href="https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree/">236.公共祖先</a> <a href="https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree-ii/">1644.公共祖先II</a> <a href="https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree-iii/">1650.公共祖先III</a> <a href="https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree-iv/">1676.公共祖先IV</a> <a href="https://leetcode.cn/problems/smallest-subtree-with-all-the-deepest-nodes/">865.具有所有最深节点的最小子树</a></td></tr></tbody></table><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 二叉树遍历时每个结点都要访问3次, 入栈的时候一次, 遍历完一个子树去另一个子树的时候一次(没有右子树则没有这次), 出栈的时候一次</span></span><br><span class="line"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title function_">postorderTraversal</span><span class="params">(TreeNode root)</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指向上一次遍历完的子树根节点,</span></span><br><span class="line">    <span class="type">TreeNode</span> <span class="variable">visited</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TreeNode</span>(-<span class="number">1</span>); <span class="comment">// 一开始指向一个不相关的结点</span></span><br><span class="line">    pushLeftBranch(root);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (!stack.isEmpty()) &#123;</span><br><span class="line">        <span class="comment">// p游离在树上, 不要把他看作游离在栈上</span></span><br><span class="line">        <span class="type">TreeNode</span> <span class="variable">p</span> <span class="operator">=</span> stack.peek();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 每个栈中的结点的左子树天生就是考虑过的, 因此直接判断右子树</span></span><br><span class="line">        <span class="keyword">if</span> (p.right != <span class="literal">null</span> &amp;&amp; p.right != visited) &#123; <span class="comment">// 有右子树且没有被访问过</span></span><br><span class="line">            <span class="comment">// 去遍历 p 的右子树</span></span><br><span class="line">            pushLeftBranch(p.right);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">// 没有右子树, 或者右子树被访问过了, 出栈</span></span><br><span class="line">            visited = stack.pop();</span><br><span class="line">            ans.add(p.val);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">pushLeftBranch</span><span class="params">(TreeNode p)</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (p != <span class="literal">null</span>) &#123;</span><br><span class="line">        stack.push(p);</span><br><span class="line">        p = p.left;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="BST">BST</h2><table><thead><tr><th>BST问题类型</th><th>题目</th></tr></thead><tbody><tr><td>BST的遍历</td><td><a href="https://leetcode.cn/problems/recover-binary-search-tree/">99. 恢复BST</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-traverse-ii/#_270-%E6%9C%80%E6%8E%A5%E8%BF%91%E7%9A%84%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E5%80%BC">270. 最接近的二叉搜索树值</a></td></tr><tr><td>BST操作</td><td><a href="https://leetcode.cn/problems/convert-bst-to-greater-tree/">538.BST转换累加树</a> <a href="https://leetcode.cn/problems/kth-smallest-element-in-a-bst/">230.BST第K小的元素</a> <a href="https://leetcode.cn/problems/insert-into-a-binary-search-tree/">701插入</a>, <a href="https://leetcode.cn/problems/delete-node-in-a-bst/">450.删除</a>, <a href="https://leetcode.cn/problems/validate-binary-search-tree/">98.验证</a> <a href="https://leetcode.cn/problems/binary-search-tree-iterator/">173.BST迭代器</a> <a href="https://labuladong.online/algo/problem-set/bst2/#_1305-%E4%B8%A4%E6%A3%B5%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E4%B8%AD%E7%9A%84%E6%89%80%E6%9C%89%E5%85%83%E7%B4%A0">1305.BST合并</a></td></tr><tr><td>BST的构造</td><td><a href="https://leetcode.cn/problems/unique-binary-search-trees/">96.n个结点构成多少不同BST</a> <a href="https://leetcode.cn/problems/unique-binary-search-trees-ii/">95.n个结点构成多少不同BST II</a></td></tr><tr><td>BST的重构</td><td><a href="https://labuladong.online/algo/problem-set/binary-tree-divide-i">🔒426.BST转化为排序的双向链表</a> <a href="https://leetcode.cn/problems/trim-a-binary-search-tree/">669.修剪BST</a> <a href="https://labuladong.online/algo/problem-set/bst1">776.拆分二叉搜索树</a> <a href="https://leetcode.cn/problems/construct-binary-search-tree-from-preorder-traversal/">1008.前序遍历构造BST</a> <a href="https://leetcode.cn/problems/convert-sorted-array-to-binary-search-tree/">108.有序数组转换为BST</a> <a href="https://leetcode.cn/problems/convert-sorted-list-to-binary-search-tree/">109.有序链表转换二叉搜索树</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/maximum-sum-bst-in-binary-tree/">1373. BST子树的最大键值和</a></td></tr><tr><td></td><td><a href="https://labuladong.online/algo/problem-set/bst1">🔒285.BST中的中序后继</a> <a href="https://labuladong.online/algo/problem-set/bst1">🔒510. BST中的中序后继 II</a></td></tr></tbody></table><p><a href="https://leetcode.cn/problems/second-minimum-node-in-a-binary-tree/">671. 二叉树中第二小的节点</a></p><p><a href="https://labuladong.online/algo/problem-set/bst1">🔒1214. 查找两棵BST之和</a></p><p><a href="https://leetcode.cn/problems/range-sum-of-bst/">938. BST的范围和</a></p><h2 id="题型">题型</h2><table><thead><tr><th>二叉树题型</th><th>题目</th></tr></thead><tbody><tr><td>二叉树的特性</td><td><a href="https://leetcode.cn/problems/check-completeness-of-a-binary-tree/">958.验证完全二叉树</a> <a href="https://leetcode.cn/problems/count-complete-tree-nodes/">222. 完全二叉树的节点个数</a> <a href="https://leetcode.cn/problems/verify-preorder-serialization-of-a-binary-tree/">331. 验证二叉树的前序序列化</a></td></tr><tr><td>路径问题(根节点到叶子结点)</td><td><a href="https://leetcode.cn/problems/binary-tree-paths/">257.路径</a> <a href="https://leetcode.cn/problems/sum-root-to-leaf-numbers/">129.路径的和</a> <a href="https://leetcode.cn/problems/sum-of-root-to-leaf-binary-numbers/">1022.路径的和</a> <a href="https://leetcode.cn/problems/binary-tree-right-side-view/">199. 二叉树的右视图</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-traverse-i/#_298-%E4%BA%8C%E5%8F%89%E6%A0%91%E6%9C%80%E9%95%BF%E8%BF%9E%E7%BB%AD%E5%BA%8F%E5%88%97">🔒298.最长连续路径</a> <a href="https://leetcode.cn/problems/smallest-string-starting-from-leaf/">988.最小字典序的路径</a> <a href="https://leetcode.cn/problems/pseudo-palindromic-paths-in-a-binary-tree/">1457.伪回文路径数量</a> <a href="https://leetcode.cn/problems/path-sum-iii/">437.和为k的子路径个数</a> <a href="https://leetcode.cn/problems/count-good-nodes-in-binary-tree/">1448.好节点的个数</a></td></tr><tr><td>跨层操作(在根节点用孩</td><td><a href="https://leetcode.cn/problems/sum-of-left-leaves/">404.左叶子之和</a> <a href="https://leetcode.cn/problems/add-one-row-to-tree/">623.二叉树增加一行</a> <a href="https://leetcode.cn/problems/flip-binary-tree-to-match-preorder-traversal/">971.翻转二叉树以匹配先序序列</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-traverse-ii/#_1469-%E5%AF%BB%E6%89%BE%E6%89%80%E6%9C%89%E7%9A%84%E7%8B%AC%E7%94%9F%E8%8A%82%E7%82%B9">🔒1469.寻找所有的独生节点</a></td></tr><tr><td>亲戚问题</td><td><a href="https://leetcode.cn/problems/cousins-in-binary-tree/">993.判断是否为堂兄弟</a> <a href="https://leetcode.cn/problems/sum-of-nodes-with-even-valued-grandparent/">1315. 爷爷为偶数的节点总和</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-traverse-ii">🔒1602.找到最近的右侧节点(找下一个兄弟)</a></td></tr><tr><td>相同子树</td><td><a href="https://leetcode.cn/problems/symmetric-tree/">101. 对称二叉树</a> <a href="https://leetcode.cn/problems/linked-list-in-binary-tree/">1367.二叉树中的链表</a> <a href="https://leetcode.cn/problems/flip-equivalent-binary-trees/">951. 翻转等价二叉树</a> <a href="https://leetcode.cn/problems/same-tree/">100.相同的树</a> <a href="https://leetcode.cn/problems/subtree-of-another-tree/">572.另一棵树的子树</a> <a href="https://leetcode.cn/problems/shu-de-zi-jie-gou-lcof/">LCR 143. 子结构判断</a></td></tr><tr><td>分解问题</td><td><a href="https://leetcode.cn/problems/maximum-depth-of-binary-tree/">104.最大深度(高度)</a> <a href="https://leetcode.cn/problems/diameter-of-binary-tree/">543.直径</a> <a href="https://leetcode.cn/problems/increasing-order-search-tree/">897. BST拉平为链表</a> <a href="https://leetcode.cn/problems/flatten-binary-tree-to-linked-list/">114.(拉平)展开为链表</a> <a href="https://leetcode.cn/problems/er-cha-sou-suo-shu-yu-shuang-xiang-lian-biao-lcof/">LCR155.BST拉平为排序双向链表</a> <a href="https://leetcode.cn/problems/binary-tree-maximum-path-sum/">124. 二叉树中的最大路径和</a> <a href="https://leetcode.cn/problems/path-sum/">112.路径总和</a> <a href="https://leetcode.cn/problems/path-sum-ii/">113.路径总和II</a></td></tr><tr><td>嵌套列表</td><td><a href="https://leetcode.cn/problems/flatten-nested-list-iterator/">341. 扁平化嵌套列表迭代器</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-combine-two-view">🔒339. 嵌套列表加权和</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-combine-two-view">🔒364. 嵌套列表加权和 II</a></td></tr><tr><td>后序</td><td><a href="https://leetcode.cn/problems/balanced-binary-tree/">110. 平衡二叉树</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-post-order-i/#_250-%E7%BB%9F%E8%AE%A1%E5%90%8C%E5%80%BC%E5%AD%90%E6%A0%91">🔒250. 统计同值子树</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-post-order-i">🔒366. 寻找二叉树的叶子节点</a> <a href="https://leetcode.cn/problems/most-frequent-subtree-sum/">508. 出现次数最多的子树元素和</a> <a href="https://leetcode.cn/problems/binary-tree-tilt/">563. 二叉树的坡度</a></td></tr><tr><td>后序</td><td><a href="https://labuladong.online/algo/problem-set/binary-tree-post-order-ii/#_663-%E5%9D%87%E5%8C%80%E6%A0%91%E5%88%92%E5%88%86">🔒663. 均匀树划分</a> <a href="https://leetcode.cn/problems/maximum-product-of-splitted-binary-tree/">1339. 分裂二叉树的最大乘积</a> <a href="https://leetcode.cn/problems/distribute-coins-in-binary-tree/">979. 在二叉树中分配硬币</a> <a href="https://leetcode.cn/problems/count-nodes-with-the-highest-score/">2049. 统计最高分的节点数目</a></td></tr><tr><td>后序返回值多个值</td><td><a href="https://leetcode.cn/problems/smallest-subtree-with-all-the-deepest-nodes/">865. 具有所有最深节点的最小子树</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-post-order-i">🔒333. 最大 BST 子树</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-post-order-i">🔒549. 二叉树中最长的连续序列</a> <a href="https://leetcode.cn/problems/maximum-difference-between-node-and-ancestor/">1026. 节点与其祖先之间的最大差值</a>  <a href="https://labuladong.online/algo/problem-set/binary-tree-post-order-ii/#_1120-%E5%AD%90%E6%A0%91%E7%9A%84%E6%9C%80%E5%A4%A7%E5%B9%B3%E5%9D%87%E5%80%BC">🔒1120. 子树的最大平均值</a> <a href="https://leetcode.cn/problems/longest-zigzag-path-in-a-binary-tree/">1372. 二叉树中的最长交错路径</a></td></tr><tr><td>后序参数多个值</td><td><a href="https://leetcode.cn/problems/binary-tree-cameras/">968. 监控二叉树</a> <a href="https://leetcode.cn/problems/insufficient-nodes-in-root-to-leaf-paths/">1080. 根到叶路径上的不足节点</a> <a href="https://leetcode.cn/problems/longest-univalue-path/">687. 最长同值路径</a></td></tr><tr><td>二叉树的构造</td><td><a href="https://leetcode.cn/problems/maximum-binary-tree/">654. 最大二叉树</a> <a href="https://leetcode.cn/problems/maximum-binary-tree-ii/">998. 最大二叉树 II</a> <a href="https://leetcode.cn/problems/construct-binary-tree-from-preorder-and-inorder-traversal/">105.前序与中序构造二叉树 </a> <a href="https://leetcode.cn/problems/construct-binary-tree-from-inorder-and-postorder-traversal/">106. 从中序与后序遍历序列构造二叉</a> <a href="https://leetcode.cn/problems/construct-binary-tree-from-preorder-and-postorder-traversal/">889.前序和后序构造二叉树</a></td></tr><tr><td>二叉树的重构</td><td><a href="https://leetcode.cn/problems/merge-two-binary-trees/">617. 合并二叉树</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-divide-i/#_1660-%E7%BA%A0%E6%AD%A3%E4%BA%8C%E5%8F%89%E6%A0%91">🔒1660. 纠正二叉树</a> <a href="https://leetcode.cn/problems/delete-leaves-with-a-given-value/">1325. 删除给定值的叶子节点</a> <a href="https://leetcode.cn/problems/binary-tree-pruning/">814. 二叉树剪枝</a> <a href="https://leetcode.cn/problems/delete-nodes-and-return-forest/">1110.删点成林</a></td></tr><tr><td>二叉树的排列</td><td><a href="https://leetcode.cn/problems/all-possible-full-binary-trees/">894.所有可能的真二叉树</a></td></tr><tr><td>层序遍历</td><td><a href="https://leetcode.cn/problems/populating-next-right-pointers-in-each-node/">116.结点指向右兄弟</a> <a href="https://leetcode.cn/problems/populating-next-right-pointers-in-each-node-ii/">117.结点指向右兄弟 II</a> <a href="https://leetcode.cn/problems/deepest-leaves-sum/">1302.层数最深节点的和</a> <a href="https://leetcode.cn/problems/binary-tree-zigzag-level-order-traversal/">103.锯齿层序遍历</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-level-ii">🔒431. 将 N 叉树编码为二叉树</a></td></tr><tr><td>树抽象为图的BFS</td><td><a href="https://leetcode.cn/problems/all-nodes-distance-k-in-binary-tree/">863. 二叉树中所有距离为 K 的结点</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-level-ii">🔒742. 二叉树最近的叶节点</a> <a href="https://leetcode.cn/problems/minimum-height-trees/">310. 最小高度树</a></td></tr><tr><td>垂序遍历</td><td><a href="https://leetcode.cn/problems/vertical-order-traversal-of-a-binary-tree/">987. 二叉树的垂序遍历</a></td></tr><tr><td>编号问题</td><td><a href="https://leetcode.cn/problems/path-in-zigzag-labelled-binary-tree/">1104. 二叉树寻路</a> <a href="https://leetcode.cn/problems/maximum-width-of-binary-tree/">662. 二叉树最大宽度</a> <a href="https://labuladong.online/algo/problem-set/binary-tree-traverse-iii">666. 路径总和 IV</a> <a href="https://leetcode.cn/problems/find-elements-in-a-contaminated-binary-tree/">1261. 在受污染的二叉树中查找元素</a></td></tr><tr><td>位运算的使用</td><td><a href="https://leetcode.cn/problems/pseudo-palindromic-paths-in-a-binary-tree/">1457. 二叉树中的伪回文路径</a></td></tr><tr><td>多叉树</td><td><a href="https://labuladong.online/algo/problem-set/binary-tree-post-order-ii/#_1245-%E6%A0%91%E7%9A%84%E7%9B%B4%E5%BE%84">🔒1245. 树的直径</a></td></tr></tbody></table><p><a href="https://labuladong.online/algo/problem-set/binary-tree-level-ii/#_582-%E6%9D%80%E6%8E%89%E8%BF%9B%E7%A8%8B">🔒582. 杀掉进程</a></p><p><a href="https://labuladong.online/algo/problem-set/binary-tree-level-ii/#_536-%E4%BB%8E%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%94%9F%E6%88%90%E4%BA%8C%E5%8F%89%E6%A0%91">🔒536. 从字符串生成二叉树</a></p><p><a href="https://leetcode.cn/problems/find-a-corresponding-node-of-a-binary-tree-in-a-clone-of-that-tree/">1379. 找出克隆二叉树中的相同节点</a></p><p><a href="https://labuladong.online/algo/problem-set/binary-tree-combine-two-view/#_1430-%E5%88%A4%E6%96%AD%E7%BB%99%E5%AE%9A%E7%9A%84%E5%BA%8F%E5%88%97%E6%98%AF%E5%90%A6%E6%98%AF%E4%BA%8C%E5%8F%89%E6%A0%91%E4%BB%8E%E6%A0%B9%E5%88%B0%E5%8F%B6%E7%9A%84%E8%B7%AF%E5%BE%84">1430. 判断给定的序列是否是二叉树从根到叶的路径</a></p><h1>Trie</h1><p>Trie == 前缀树 == 字典树</p><table><thead><tr><th>Trie</th><th></th></tr></thead><tbody><tr><td>Trie的实现</td><td><a href="https://leetcode.cn/problems/implement-trie-prefix-tree/">208.实现Trie</a> <a href="https://labuladong.online/algo/problem-set/trie">1804.实现Trie II</a></td></tr><tr><td>Trie基本API的应用</td><td><a href="https://leetcode.cn/problems/replace-words/">648.单词替换</a> <a href="https://leetcode.cn/problems/design-add-and-search-words-data-structure/">211.添加与搜索单词</a> <a href="https://leetcode.cn/problems/map-sum-pairs/">677.键值映射</a></td></tr><tr><td>Trie解决问题</td><td><a href="https://leetcode.cn/problems/maximum-xor-of-two-numbers-in-an-array/">421.两个数的最大异或值</a> <a href="https://leetcode.cn/problems/word-search-ii/">212.单词搜索II</a></td></tr></tbody></table><h1>链表问题</h1><table><thead><tr><th style="text-align:left">链表的8类问题</th><th>题号</th></tr></thead><tbody><tr><td style="text-align:left">链表反转</td><td><a href="https://leetcode.cn/problems/reverse-linked-list/">206. 反转链表</a> <a href="https://leetcode.cn/problems/reverse-linked-list-ii/">92. 反转链表 II</a> <a href="https://leetcode.cn/problems/reverse-nodes-in-k-group/">25. K 个一组翻转链表</a> <a href="https://leetcode.cn/problems/palindrome-linked-list/">234. 回文链表</a></td></tr><tr><td style="text-align:left">链表合并</td><td><a href="https://leetcode.cn/problems/merge-two-sorted-lists/">21. 合并两个有序链表 </a> <a href="https://leetcode.cn/problems/merge-k-sorted-lists/">23.合并K个有序链表 </a> <a href="##%E4%B8%91%E6%95%B0%E9%97%AE%E9%A2%98">丑数问题</a> <a href="https://leetcode.cn/problems/kth-smallest-element-in-a-sorted-matrix/">378. 有序矩阵中第 K 小的元素</a> <a href="https://leetcode.cn/problems/find-k-pairs-with-smallest-sums/">373. 查找和最小的 K 对数字</a></td></tr><tr><td style="text-align:left">链表第k结点问题</td><td><a href="https://leetcode.cn/problems/remove-nth-node-from-end-of-list/description/">19. 删除链表的倒数第N个结点 </a>   <a href="https://leetcode.cn/problems/middle-of-the-linked-list/">876. 链表的中间结点</a></td></tr><tr><td style="text-align:left">链表环问题</td><td><a href="https://leetcode.cn/problems/linked-list-cycle/description/">141. 判断链表是否有环</a> <a href="https://leetcode.cn/problems/linked-list-cycle-ii/description/">142. 判断链表环的起点 </a></td></tr><tr><td style="text-align:left">链表相交问题</td><td><a href="https://leetcode.cn/problems/intersection-of-two-linked-lists/description/">160. 返回两个相交链表的相交结点 </a></td></tr><tr><td style="text-align:left">链表分隔</td><td><a href="https://leetcode.cn/problems/partition-list/">86. 分隔链表PartitionList</a></td></tr><tr><td style="text-align:left">丑数问题</td><td><a href="##%E4%B8%91%E6%95%B0%E9%97%AE%E9%A2%98">丑数问题相关题目</a></td></tr></tbody></table><h1>前缀和 &amp; 差分</h1><table><thead><tr><th>前缀和问题</th><th></th></tr></thead><tbody><tr><td>前缀和</td><td><a href="https://leetcode.cn/problems/range-sum-query-immutable/">303.区域和检索</a></td></tr><tr><td>前缀积</td><td><a href="https://leetcode.cn/problems/product-of-array-except-self/">238.除自身以外数组的乘积 </a></td></tr><tr><td>整除(模为0)问题</td><td><a href="https://leetcode.cn/problems/continuous-subarray-sum/">523.和为k的倍数的子数组</a> <a href="https://leetcode.cn/problems/make-sum-divisible-by-p/">1590.使数组和能被P整除</a> <a href="https://leetcode.cn/problems/subarray-sums-divisible-by-k/">974.和可被K整除的子数组的个数</a></td></tr><tr><td>位运算表示状态</td><td><a href="https://leetcode.cn/problems/find-the-longest-substring-containing-vowels-in-even-counts/">1371.每个元音包含偶数次的最长子字符串</a></td></tr><tr><td></td><td>(求最长或最短长度)<a href="https://leetcode.cn/problems/contiguous-array/">525. 含有相同数量1和0的最长子数组</a> <a href="https://labuladong.online/algo/problem-set/perfix-sum">325.和为k的最长子数组</a> <a href="https://leetcode.cn/problems/longest-well-performing-interval/">1124.好的最长时间段</a></td></tr><tr><td></td><td>(求子数组个数)<a href="https://leetcode.cn/problems/subarray-sum-equals-k/">560.和为K的子数组的个数 </a> <a href="https://leetcode.cn/problems/path-sum-iii/">437.路径总和III</a></td></tr><tr><td>一维差分</td><td><a href="https://leetcode.cn/problems/car-pooling/">1094.拼车</a> <a href="https://leetcode.cn/problems/corporate-flight-bookings/">1109.航班预订统计</a> <a href="https://leetcode.cn/problems/range-addition/">🔒370.区间加法</a></td></tr><tr><td>二维前缀和</td><td><a href="https://leetcode.cn/problems/range-sum-query-2d-immutable/">304.二维区域和检索</a> <a href="https://leetcode.cn/problems/matrix-block-sum/">1314.矩阵区域和</a> <a href="https://leetcode.cn/problems/largest-1-bordered-square/">1139.最大的以1为边界的正方形</a></td></tr><tr><td>二维差分</td><td>(扫描线问题)<a href="https://leetcode.cn/problems/xepqZ5/">LCP74.最强祝福力场</a></td></tr><tr><td>二维前缀和 + 二维差分</td><td><a href="https://leetcode.cn/problems/stamping-the-grid/">2132.用邮票贴满网格图</a></td></tr></tbody></table><h1>滑动窗口</h1><table><thead><tr><th>滑动窗口问题类型</th><th>题目</th></tr></thead><tbody><tr><td>求最值</td><td><a href="https://leetcode.cn/problems/minimum-size-subarray-sum/">209.和为k的最短子数组</a> <a href="https://leetcode.cn/problems/minimum-operations-to-reduce-x-to-zero/">1658.x减到0的最小操作数</a> <a href="https://leetcode.cn/problems/max-consecutive-ones-iii/">1004.最大连续1的个数III</a></td></tr><tr><td>求个数</td><td><a href="https://leetcode.cn/problems/subarray-product-less-than-k/">713.积小于K的子数组</a> <a href="https://leetcode.cn/problems/subarrays-with-k-different-integers/">992.K个不同数子数组</a></td></tr><tr><td>添加约束</td><td><a href="https://leetcode.cn/problems/longest-substring-with-at-least-k-repeating-characters/">395.每个字符重复K次以上的最长子串</a></td></tr><tr><td>字符串字串问题</td><td><a href="https://leetcode.cn/problems/find-all-anagrams-in-a-string/">438.字符串中所有异位词的位置</a> <a href="https://leetcode.cn/problems/permutation-in-string/">567.字符串排列</a> <a href="https://leetcode.cn/problems/longest-repeating-character-replacement/">424.替换后的重复字符</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/longest-substring-without-repeating-characters/">3.无重复字符的最长子串</a> <a href="https://leetcode.cn/problems/contains-duplicate-ii/">219.存在重复元素II</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/minimum-window-substring/">76.最小覆盖子串</a> <a href="https://leetcode.cn/problems/replace-the-substring-for-balanced-string/">1234.替换最小子串使其平衡</a></td></tr><tr><td>TreeMap为窗口</td><td><a href="https://leetcode.cn/problems/contains-duplicate-iii/">220.存在重复元素III</a></td></tr><tr><td>其他</td><td><a href="https://leetcode.cn/problems/gas-station/">134.加油站</a></td></tr></tbody></table><p>滑动窗口灵魂三问:</p><ol><li>什么时候应该扩大窗口?</li><li>什么时候应该缩小窗口?</li><li>什么时候得到一个合法的答案?</li></ol><h1>双指针</h1><table><thead><tr><th>双指针问题类型</th><th></th></tr></thead><tbody><tr><td>简单问题</td><td><a href="https://leetcode.cn/problems/remove-element/">27.移除元素</a> <a href="https://leetcode.cn/problems/move-zeroes/">283.移动零</a> <a href="https://leetcode.cn/problems/squares-of-a-sorted-array/">977.有序数组的平方</a> <a href="https://leetcode.cn/problems/sort-transformed-array">🔒360.有序转化数组</a></td></tr><tr><td>反转, 回文问题</td><td><a href="https://leetcode.cn/problems/reverse-words-in-a-string/">151.反转字符串中的单词</a> <a href="https://leetcode.cn/problems/reverse-string/">344.反转字符串</a> <a href="https://leetcode.cn/problems/longest-palindromic-substring/">5.最长回文子串</a> <a href="https://leetcode.cn/problems/super-palindromes/">906.超级回文数</a></td></tr><tr><td>发货思想</td><td><a href="https://leetcode.cn/problems/sort-array-by-parity-ii/">922.奇偶排序数组II</a></td></tr><tr><td>重复问题</td><td><a href="https://leetcode.cn/problems/find-the-duplicate-number/">287.寻找重复数</a> <a href="https://leetcode.cn/problems/remove-duplicates-from-sorted-array/">26.有序数组去重</a> <a href="https://leetcode.cn/problems/remove-duplicates-from-sorted-list/">83.排序链表去重</a></td></tr><tr><td>接雨水问题(单调问题)</td><td><a href="https://leetcode.cn/problems/container-with-most-water/">11.盛最多水的容器</a> <a href="https://leetcode.cn/problems/trapping-rain-water/">42.接雨水</a></td></tr><tr><td>贪心思想</td><td><a href="https://leetcode.cn/problems/boats-to-save-people/">881.救生艇</a> <a href="https://leetcode.cn/problems/heaters/">475.供暖器</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/first-missing-positive/">41.缺失的第一个正数</a></td></tr><tr><td>异或</td><td><a href="https://leetcode.cn/problems/find-the-difference/">389.找不同</a></td></tr><tr><td>映射</td><td><a href="https://leetcode.cn/problems/set-mismatch/">645.错误的集合</a> <a href="https://leetcode.cn/problems/find-all-numbers-disappeared-in-an-array/">448.数组中消失的数字</a> <a href="https://leetcode.cn/problems/find-all-duplicates-in-an-array/">442.数组中重复的数据</a></td></tr></tbody></table><h1>单调栈</h1><p>单调栈灵魂三问:</p><ol><li>大压小 还是 小压大?<ul><li>如果求更小元素, 大压小; 如果求更大元素, 小压大.</li></ul></li><li>什么时候要求严格单调, 什么时候不用严格单调?<ul><li>找下(上)一个更小(大) 或等于 元素时, 需要严格单调</li></ul></li><li>出栈时得到答案 还是入栈时得到答案?<ul><li>出栈时</li><li><s>入栈时得到的答案是上一个更小(大)元素; 出栈时得到的答案是下一个更小(大)元素</s></li></ul></li></ol><table><thead><tr><th>单调栈</th><th>单调栈用来找到左右两侧离当前位置最近的更小(更大)的数.</th></tr></thead><tbody><tr><td>单调栈的本体</td><td><a href="https://www.nowcoder.com/practice/2a2c00e7a88a498693568cef63a4b7bb">牛客单调栈</a></td></tr><tr><td>下一个更大元素</td><td><a href="https://leetcode.cn/problems/next-greater-element-i/">496.下一个更大元素I</a> <a href="https://leetcode.cn/problems/daily-temperatures/">739.每日温度</a> <a href="https://leetcode.cn/problems/next-greater-node-in-linked-list/">1019.链表下一个更大节点</a></td></tr><tr><td>下一个更小元素</td><td><a href="https://leetcode.cn/problems/final-prices-with-a-special-discount-in-a-shop/">1475.商品折扣后的价格</a></td></tr><tr><td>上一个更小元素</td><td><a href="https://leetcode.cn/problems/online-stock-span/">901.股票价格跨度</a></td></tr><tr><td>单调栈的应用</td><td><a href="https://leetcode.cn/problems/number-of-visible-people-in-a-queue/">1944.队列可以看到的人数</a> <a href="https://leetcode.cn/problems/remove-k-digits/">402.移掉K位数字</a></td></tr><tr><td>需要枚举所有子数组(字串)的问题</td><td><a href="https://leetcode.cn/problems/count-unique-characters-of-all-substrings-of-a-given-string/">828.所有子串中出现一次的字符个数</a> <a href="https://leetcode.cn/problems/sum-of-subarray-minimums/">907.所有子数组最小值之和</a> <a href="https://leetcode.cn/problems/maximum-subarray-min-product/">1856.所有子数组得分的最大值</a> <a href="https://leetcode.cn/problems/sum-of-subarray-ranges/">2104.所有子数组的最大差值和</a> <a href="https://leetcode.cn/problems/sum-of-total-strength-of-wizards/">2281. 所有子数组得分的和</a></td></tr><tr><td>柱状图中最大的矩形</td><td><a href="https://leetcode.cn/problems/largest-rectangle-in-histogram/">84.柱状图的最大矩形</a> <a href="https://leetcode.cn/problems/maximal-rectangle/">85.最大矩形</a>(压缩数组, 二维压成一维)</td></tr><tr><td>单调思想</td><td><a href="https://leetcode.cn/problems/maximum-width-ramp/">962.最大宽度坡</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/remove-duplicate-letters/">316.字符串去重复</a></td></tr><tr><td>大鱼吃小鱼问题</td><td><a href="https://leetcode.cn/problems/steps-to-make-array-non-decreasing/">2289.使数组递增的操作步数</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/count-submatrices-with-all-ones/">1504.统计全1子矩形</a></td></tr></tbody></table><h1>单调队列</h1><table><thead><tr><th>单调队列</th><th></th></tr></thead><tbody><tr><td>单调队列的本体</td><td><a href="https://leetcode.cn/problems/sliding-window-maximum/">239.滑动窗口最大值</a> <a href="https://leetcode.cn/problems/dui-lie-de-zui-da-zhi-lcof/">LCR184.设计自助结算系统</a> <a href="https://leetcode.cn/problems/maximum-sum-circular-subarray/">918.环形子数组的最大和</a> <a href="https://leetcode.cn/problems/max-value-of-equation/">1499.点的最大得分</a></td></tr><tr><td>单调队列 + 滑动窗口</td><td><a href="https://leetcode.cn/problems/longest-continuous-subarray-with-absolute-diff-less-than-or-equal-to-limit/">1438.绝对差不超过限制的最长连续子数组</a> <a href="https://leetcode.cn/problems/shortest-subarray-with-sum-at-least-k/">862.和至少为K的最短子数组</a></td></tr><tr><td>单调队列 + 动态规划</td><td><a href="https://leetcode.cn/problems/jump-game-vi/">1696.跳跃游戏VI</a> <a href="https://leetcode.cn/problems/constrained-subsequence-sum/">1425.带限制的子序列和</a> <a href="https://labuladong.online/algo/problem-set/monotonic-queue/#_1429-%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%94%AF%E4%B8%80%E6%95%B0%E5%AD%97">🔒1429.第一个唯一数字</a></td></tr></tbody></table><h1>二分答案法</h1><table><thead><tr><th>问题类型</th><th>题目</th></tr></thead><tbody><tr><td>二分搜索的本体</td><td><a href="https://leetcode.cn/problems/binary-search/">704.二分查找</a> <a href="https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array/">34.排序数组查找元素的第一个和最后一个位置</a> <a href="https://leetcode.cn/problems/zai-pai-xu-shu-zu-zhong-cha-zhao-shu-zi-lcof/">LCR172.统计目标成绩的出现次数</a></td></tr><tr><td>二分搜索的变体</td><td><a href="https://leetcode.cn/problems/search-in-rotated-sorted-array/">33.搜索旋转排序数组</a> <a href="https://leetcode.cn/problems/search-in-rotated-sorted-array-ii/">81.搜索旋转排序数组II</a> <a href="https://leetcode.cn/problems/que-shi-de-shu-zi-lcof/">LCR173.寻找缺失的数</a> <a href="https://leetcode.cn/problems/peak-index-in-a-mountain-array/">852.数组的峰顶索引</a> <a href="https://leetcode.cn/problems/find-peak-element/">162.寻找峰值</a></td></tr><tr><td>运用到二分搜索的题目</td><td><a href="https://leetcode.cn/problems/find-k-closest-elements/">658.找到K个最接近的元素</a> <a href="https://leetcode.cn/problems/ugly-number-iii/">1201.丑数III</a> <a href="https://leetcode.cn/problems/nth-magical-number/">878.第N个神奇数字</a></td></tr><tr><td>二分答案法</td><td><a href="https://leetcode.cn/problems/koko-eating-bananas/">875.珂珂吃香蕉</a> <a href="https://leetcode.cn/problems/find-k-th-smallest-pair-distance/">719.找出第K小的数对距离</a></td></tr><tr><td>(画匠问题)</td><td><a href="https://leetcode.cn/problems/split-array-largest-sum/">410.分割数组的最大值</a> <a href="https://leetcode.cn/problems/capacity-to-ship-packages-within-d-days/">1011.在D天内送达包裹的能力</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/maximum-running-time-of-n-computers/">2141.同时运行N台电脑的最长时间</a></td></tr><tr><td>田忌赛马</td><td><a href="https://leetcode.cn/problems/advantage-shuffle/">870.优势洗牌田忌赛马</a> <a href="https://leetcode.cn/problems/maximum-number-of-tasks-you-can-assign/">2071.可以安排的最多任务数目</a></td></tr></tbody></table><p>二分答案法的灵魂三步:</p><ol><li>估计最终答案的范围</li><li>确定 单调函数 f(答案) = 求出的要求</li><li>在答案的范围上二分, 找到一个答案, 使得 f(答案) = 要求</li></ol><h1>回溯算法</h1><table><thead><tr><th>问题类型</th><th>题目</th></tr></thead><tbody><tr><td>运用位运算</td><td><a href="https://leetcode.cn/problems/n-queens/">51.N 皇后</a> <a href="https://leetcode.cn/problems/n-queens-ii/">52.N皇后II</a></td></tr><tr><td>排列问题</td><td><a href="https://leetcode.cn/problems/permutations/">46.全排列</a> <a href="https://leetcode.cn/problems/permutations-ii/">47.全排列II</a></td></tr><tr><td>组合问题(类似nSum问题)</td><td><a href="https://leetcode.cn/problems/combination-sum/">39.组合总和</a> <a href="https://leetcode.cn/problems/combination-sum-ii/">40.组合总和II</a> <a href="https://leetcode.cn/problems/combination-sum-iii/">216.组合总和III</a></td></tr><tr><td>子集问题</td><td><a href="https://leetcode.cn/problems/subsets/">78.子集</a> <a href="https://leetcode.cn/problems/subsets-ii/">90.子集II</a></td></tr><tr><td>集合划分问题</td><td><a href="https://leetcode.cn/problems/partition-to-k-equal-sum-subsets/">698.划分为k个相等的子集</a></td></tr></tbody></table><p>子集/组合问题: 每次循环都从指定的位置开始遍历后半部分, 可复选递归调用时位置不变.</p><p>排列问题: (全排列和普通排列收集结果时机不同)每次循环都从头到尾遍历全部, 可复选不需要isUsed[],</p><table><thead><tr><th>问题类型</th><th>题目</th></tr></thead><tbody><tr><td>DFS</td><td><a href="https://leetcode.cn/problems/unique-paths-iii/">980.不同路径 III</a> <a href="https://leetcode.cn/problems/word-search/">79.单词搜索</a> <a href="https://leetcode.cn/problems/path-with-maximum-gold/">1219.黄金矿工</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/restore-ip-addresses/">93.复原IP</a> <a href="https://leetcode.cn/problems/letter-combinations-of-a-phone-number/">17.电话字母组合</a> <a href="https://leetcode.cn/problems/minimum-moves-to-spread-stones-over-grid/">2850.分散石头</a> <a href="https://leetcode.cn/problems/reconstruct-itinerary/">332.重新安排行程</a></td></tr><tr><td>洪水覆盖算法</td><td><a href="https://leetcode.cn/problems/number-of-islands/">200.岛屿数量</a> <a href="https://leetcode.cn/problems/number-of-closed-islands/">1254.封闭岛屿数量</a> <a href="https://leetcode.cn/problems/number-of-enclaves/">1020.岛面积</a> <a href="https://leetcode.cn/problems/max-area-of-island/"> 695.岛屿最大面积</a> <a href="https://leetcode.cn/problems/count-sub-islands/">1905.统计子岛屿</a> <a href="https://labuladong.online/algo/frequency-interview/island-dfs-summary">694.不同岛屿数量</a> <a href="https://leetcode.cn/problems/surrounded-regions/">130.被围绕区域</a> <a href="https://leetcode.cn/problems/making-a-large-island/">827.最大人工岛</a> <a href="https://leetcode.cn/problems/bricks-falling-when-hit/">803.打砖块</a></td></tr><tr><td>无重不可复选</td><td><a href="https://leetcode.cn/problems/letter-case-permutation/">784.字母大小写全排列</a></td></tr><tr><td>有重不可复选(排序+(nums[i]==nums[i - 1]) 或 HashSet)</td><td><a href="https://leetcode.cn/problems/non-decreasing-subsequences/">491.非递减子序列</a> <a href="https://leetcode.cn/problems/letter-tile-possibilities/">1079.活字印刷</a> <a href="https://leetcode.cn/problems/number-of-squareful-arrays/">996.平方数组数目</a></td></tr><tr><td>可复选</td><td><a href="https://leetcode.cn/problems/numbers-with-same-consecutive-differences/">967.连续差相同的数字</a> <a href="https://leetcode.cn/problems/shopping-offers/">638.大礼包</a></td></tr><tr><td>划分k个子集</td><td><a href="https://leetcode.cn/problems/partition-to-k-equal-sum-subsets/">698. 划分k个子集</a> <a href="https://leetcode.cn/problems/matchsticks-to-square/">473. 火柴拼正方形</a></td></tr><tr><td>字符串拆分</td><td><a href="https://leetcode.cn/problems/splitting-a-string-into-descending-consecutive-values/">1849.字符串拆为递减连续值</a> <a href="https://leetcode.cn/problems/split-a-string-into-the-max-number-of-unique-substrings/">1593.唯一子字符串的最大数目</a> <a href="https://leetcode.cn/problems/palindrome-partitioning/">131. 分割回文串</a></td></tr><tr><td>球盒问题</td><td><a href="https://leetcode.cn/problems/beautiful-arrangement/">526.优美排列</a> <a href="https://leetcode.cn/problems/find-minimum-time-to-finish-all-jobs/">1723.完成所有工作的最短时间</a> <a href="https://leetcode.cn/problems/fair-distribution-of-cookies/">2305.公平分发饼干</a></td></tr><tr><td></td><td><a href="https://labuladong.online/algo/problem-set/backtrack-iii/#_293-%E7%BF%BB%E8%BD%AC%E6%B8%B8%E6%88%8F">🔒293.翻转游戏</a> <a href="https://labuladong.online/algo/problem-set/backtrack-iii/#_294-%E7%BF%BB%E8%BD%AC%E6%B8%B8%E6%88%8F-ii">🔒294.翻转游戏II</a> <a href="https://labuladong.online/algo/problem-set/backtrack-iii/#_291-%E5%8D%95%E8%AF%8D%E8%A7%84%E5%BE%8B-ii">🔒291.单词规律II</a> <a href="https://labuladong.online/algo/problem-set/backtrack-iii/#_267-%E5%9B%9E%E6%96%87%E6%8E%92%E5%88%97-ii">🔒267.回文排列II</a> <a href="https://labuladong.online/algo/problem-set/backtrack-iii/#_254-%E5%9B%A0%E5%AD%90%E7%9A%84%E7%BB%84%E5%90%88">🔒254.因子组合</a></td></tr></tbody></table><h1>BFS算法</h1><table><thead><tr><th>问题类型</th><th>题目</th></tr></thead><tbody><tr><td>例题</td><td><a href="https://leetcode.cn/problems/open-the-lock/">752.打开转盘锁</a> <a href="https://leetcode.cn/problems/sliding-puzzle/">773.滑动谜题</a></td></tr><tr><td>序列上的BFS</td><td><a href="https://leetcode.cn/problems/keys-and-rooms/">841.钥匙和房间</a> <a href="https://leetcode.cn/problems/jump-game-iii/">1306.跳跃游戏III</a> <a href="https://leetcode.cn/problems/minimum-genetic-mutation/">433.最小基因变化</a> <a href="https://leetcode.cn/problems/word-ladder/">127.单词接龙</a></td></tr><tr><td>矩阵的BFS</td><td><a href="https://leetcode.cn/problems/nearest-exit-from-entrance-in-maze/">1926.迷宫出口</a> <a href="https://leetcode.cn/problems/shortest-path-in-binary-matrix/">1091.最短路径</a> <a href="https://leetcode.cn/problems/01-matrix/">542.01矩阵</a> <a href="https://leetcode.cn/problems/minimum-moves-to-spread-stones-over-grid/">2850.分散石头</a> <a href="https://leetcode.cn/problems/pacific-atlantic-water-flow/">417.水流问题</a></td></tr><tr><td></td><td><a href="https://labuladong.online/algo/problem-set/bfs-ii/#_286-%E5%A2%99%E4%B8%8E%E9%97%A8">🔒286.墙与门</a> <a href="https://labuladong.online/algo/problem-set/bfs-ii/#_490-%E8%BF%B7%E5%AE%AB">🔒490.迷宫</a> <a href="https://labuladong.online/algo/problem-set/bfs-ii/#_505-%E8%BF%B7%E5%AE%AB-ii">🔒505.迷宫 II</a> <a href="https://labuladong.online/algo/problem-set/bfs-ii/#_499-%E8%BF%B7%E5%AE%AB-iii">🔒499. 迷宫III</a></td></tr><tr><td>图的BFS</td><td><a href="https://leetcode.cn/problems/minimize-malware-spread/">924.恶意软件传播</a> <a href="https://leetcode.cn/problems/detonate-the-maximum-bombs/">2101.引爆最多的炸弹</a> <a href="https://leetcode.cn/problems/evaluate-division/">399.除法求值</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/water-and-jug-problem/">365.水壶问题</a> <a href="https://leetcode.cn/problems/accounts-merge/">721.账户合并</a></td></tr><tr><td>多源BFS</td><td><a href="https://leetcode.cn/problems/rotting-oranges/">994.腐烂的橘子</a> <a href="https://leetcode.cn/problems/as-far-from-land-as-possible/">1162.地图分析</a></td></tr><tr><td>0-1BFS</td><td><a href="https://leetcode.cn/problems/minimum-obstacle-removal-to-reach-corner/">2290.到达角落移除障碍物最小数目</a> <a href="https://leetcode.cn/problems/minimum-cost-to-make-at-least-one-valid-path-in-a-grid/">1368.到达角落的最小代价</a></td></tr><tr><td>将邻接表作为容器</td><td><a href="https://leetcode.cn/problems/stickers-to-spell-word/">691.贴纸拼词</a></td></tr><tr><td>BFS+堆</td><td><a href="https://leetcode.cn/problems/trapping-rain-water-ii/">407.接雨水II</a></td></tr><tr><td>BFS的所有路径</td><td><a href="https://leetcode.cn/problems/word-ladder-ii/">126.单词接龙II</a>(HashSet代替队列, adjList存所有路径)</td></tr><tr><td>双向BFS</td><td><a href="https://leetcode.cn/problems/word-ladder/">127.单词接龙</a> <a href="https://leetcode.cn/problems/closest-subsequence-sum/">1755.最接近目标值的子序列和</a></td></tr></tbody></table><h1>并查集</h1><table><thead><tr><th>问题类型</th><th>题目</th></tr></thead><tbody><tr><td></td><td><a href="https://leetcode.cn/problems/satisfiability-of-equality-equations/">990.等式方程的可满足性</a> <a href="https://leetcode.cn/problems/similar-string-groups/">839.相似字符串组</a> <a href="https://leetcode.cn/problems/most-stones-removed-with-same-row-or-column/">947.移除同行或同列石头</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/find-all-people-with-secret/">2092.找出知晓秘密的所有专家</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/couples-holding-hands/">765.情侣牵手</a> <a href="https://leetcode.cn/problems/number-of-good-paths/">2421.好路径的数目</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/minimize-malware-spread-ii/">928.恶意软件传播II</a></td></tr><tr><td></td><td><a href="https://labuladong.online/algo/data-structure/union-find/">🔒323.无向图中连通分量的数目</a> <a href="https://leetcode.cn/problems/redundant-connection/">684.冗余连接</a> <a href="https://labuladong.online/algo/data-structure/kruskal/">🔒261.以图判树</a></td></tr></tbody></table><h1>图</h1><p><strong>链式前向星建图</strong></p><ul><li>将每个结点的边为一条链表, 遍历所有边时用 <strong>头插法</strong> 添加边</li><li><code>int head[];</code> head[i] 表示结点i的第一条边</li><li><code>int next[];</code> next[i] 表示第i条边的下一条边</li><li><code>int to[];</code> to[i] 表示第i条边的尾结点</li><li><code>int count = 1;</code> count表示当前遍历到的边的编号</li><li><code>int weight[];</code> weight[i] 表示第i条边的权重</li></ul><table><thead><tr><th>问题类型</th><th>题目</th></tr></thead><tbody><tr><td>拓扑排序</td><td><a href="https://leetcode.cn/problems/course-schedule/">207.课程表</a> <a href="https://leetcode.cn/problems/course-schedule-ii/">210.课程表II </a> <a href="https://leetcode.cn/problems/Jf1JuT/">LCR114.火星词典</a> <a href="https://leetcode.cn/problems/stamping-the-sequence/">936.戳印序列</a></td></tr><tr><td>拓扑排序带信息</td><td>拓扑排序的路径数量 <a href="https://leetcode.cn/problems/loud-and-rich/">851.喧闹和富有</a> <a href="https://leetcode.cn/problems/parallel-courses-iii/">2050.并行课程III</a> <a href="https://leetcode.cn/problems/maximum-employees-to-be-invited-to-a-meeting/">2127.参加会议的最多员工数</a></td></tr><tr><td>最小生成树问题(Kruskal, Prim)</td><td><a href="https://leetcode.cn/problems/min-cost-to-connect-all-points/">1584.连接所有点的最小费用</a> <a href="https://labuladong.online/algo/data-structure/kruskal/">🔒1135.联通城市</a> <a href="https://leetcode.cn/problems/checking-existence-of-edge-length-limited-paths/">1697.查询边长度限制的路径</a></td></tr><tr><td>最短路径问题</td><td><a href="https://leetcode.cn/problems/network-delay-time/">743.Dijkstra</a> <a href="https://leetcode.cn/problems/path-with-minimum-effort/">1631.路径为最大差</a> <a href="https://leetcode.cn/problems/path-with-maximum-probability/">1514.Dijkstra求最长路径</a> <a href="https://leetcode.cn/problems/swim-in-rising-water/">778.路径为路上结点的最大值</a></td></tr><tr><td></td><td>Bellman-Ford与SPFA(可以有负边) <a href="https://leetcode.cn/problems/cheapest-flights-within-k-stops/">787.K站中转内最便宜的航班</a></td></tr><tr><td></td><td>Floyd求所有结点相互之间的最短路径(可以有负边)</td></tr><tr><td></td><td>A*求源结点到目标结点最短路径, 堆按 源点到当前结点距离+当前点到终点的预估距离 排序</td></tr><tr><td>分层图最短路(扩点最短路)</td><td><a href="https://leetcode.cn/problems/shortest-path-to-get-all-keys/">864.获取所有钥匙的最短路径</a> <a href="https://leetcode.cn/problems/DFPeFJ/">LCP35.电动车游城市</a></td></tr><tr><td>名人问题</td><td><a href="https://labuladong.online/algo/frequency-interview/find-celebrity/">🔒277. 搜寻名人</a></td></tr><tr><td>二分图问题</td><td><a href="https://leetcode.cn/problems/is-graph-bipartite/">785.判断二分图</a> <a href="https://leetcode.cn/problems/possible-bipartition/">886.可能的二分法</a></td></tr></tbody></table><p><strong>Kruskal算法思想</strong>: 将边按权重从小到大排序, 从小到大遍历边, 如果边的两头结点不连通, 将边的两头结点用并查集连通</p><p><strong>Prim算法思想</strong>: 从任意一个结点开始作为树根, 选一条最小的边加入当前的树, 直到所有结点都加入当前的树.</p><p>**Dijkstra算法: **</p><ol><li><p>维护一个dist[n]存储指定结点到i的最短距离;</p></li><li><p>以指定结点为根开始BFS遍历, 每遍历到一个结点就将dist[n]更新为更小的.</p></li><li><p>BFS遍历用优先级队列, 优先级队列的元素是一个新的类(结点, 根到结点的距离)</p></li><li><p>当 从==队列中取出的元素== 到根的距离 大于 当前dist[]中对应的值 就不需要遍历该结点后面的路径了, 直接去取下一个值.</p><ol><li>为什么 等于 的时候还要遍历? 因为等于的时候说明刚从队里出来, 它的邻居可能还没入队; 大于的时候至少是第二次到这个结点了, 他的邻居已经入过队了</li></ol></li><li><p>当 ==结点的邻居== 到根的距离(按当前路径到达邻居) 大于 当前dist[]中邻居对应的值 就不需要将路径入队</p></li></ol><h1>还没分类</h1><table><thead><tr><th>解题方法</th><th>题目</th></tr></thead><tbody><tr><td>Rabin Karp字符匹配算法</td><td></td></tr><tr><td>二维数组</td><td><a href="https://leetcode.cn/problems/rotate-image/">48.旋转矩阵</a> <a href="https://leetcode.cn/problems/spiral-matrix/">54.螺旋矩阵</a> <a href="https://leetcode.cn/problems/spiral-matrix-ii/">59.螺旋矩阵 II</a></td></tr><tr><td>nSum问题</td><td><a href="https://leetcode.cn/problems/two-sum-ii-input-array-is-sorted/">167.两数之和II</a> <a href="https://leetcode.cn/problems/3sum/">15.三数之和</a> <a href="https://leetcode.cn/problems/4sum/">18.四数之和</a> <a href="https://leetcode.cn/problems/maximum-xor-of-two-numbers-in-an-array/">421.两个数的最大异或值</a></td></tr><tr><td>区间问题</td><td><a href="https://leetcode.cn/problems/remove-covered-intervals/">1288.删除被覆盖区间</a> <a href="https://leetcode.cn/problems/merge-intervals/">56.合并区间</a> <a href="https://leetcode.cn/problems/interval-list-intersections/">986.区间列表的交集</a></td></tr><tr><td>随机问题</td><td><a href="https://leetcode.cn/problems/random-pick-with-weight/">528.按权重随机选择</a></td></tr><tr><td>考场座位分配</td><td><a href="https://leetcode.cn/problems/exam-room/">855.考场就座</a></td></tr></tbody></table><h1>动态规划</h1><h2 id="DP基础">DP基础</h2><table><thead><tr><th></th><th>题目</th></tr></thead><tbody><tr><td>一维DP</td><td><a href="https://leetcode.cn/problems/minimum-cost-for-tickets/">983.最低票价</a> <a href="https://leetcode.cn/problems/decode-ways/">91.解码方法</a> <a href="https://leetcode.cn/problems/decode-ways-ii/">639.解码方法II</a></td></tr><tr><td></td><td>子数组问题 <a href="https://leetcode.cn/problems/longest-valid-parentheses/">32.最长有效括号</a></td></tr><tr><td></td><td>需要消除重复的问题 <a href="https://leetcode.cn/problems/unique-substrings-in-wraparound-string/">467.环绕字符串</a> <a href="https://leetcode.cn/problems/distinct-subsequences-ii/">940.不同子序列II</a></td></tr><tr><td>二维DP</td><td><a href="https://leetcode.cn/problems/minimum-path-sum/">64.最小路径和</a> <a href="https://leetcode.cn/problems/longest-increasing-path-in-a-matrix/">329.矩阵中的最长递增路径</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/longest-common-subsequence/">1143.最长公共子序列</a> <a href="https://leetcode.cn/problems/distinct-subsequences/">115.不同的子序列</a> <a href="https://leetcode.cn/problems/edit-distance/">72.编辑距离</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/longest-palindromic-subsequence/">516.最长回文子序列</a></td></tr><tr><td></td><td>是否问题 <a href="https://leetcode.cn/problems/interleaving-string/">97.交错字符串</a></td></tr><tr><td>三维DP</td><td><a href="https://leetcode.cn/problems/ones-and-zeroes/">474.一和零</a> <a href="https://leetcode.cn/problems/profitable-schemes/">879.盈利计划</a> <a href="https://leetcode.cn/problems/knight-probability-in-chessboard/">688.骑士在棋盘上的概率</a> <a href="https://leetcode.cn/problems/paths-in-matrix-whose-sum-is-divisible-by-k/">2435.矩阵中和能被K整除的路径数</a></td></tr><tr><td></td><td>是否问题 <a href="https://leetcode.cn/problems/scramble-string/">87.扰乱字符串</a></td></tr></tbody></table><p>子数组问题:</p><ol><li>dp[i]是以s[i]结尾的字串答案</li><li>dp[i]是必须含有且结尾为s[i]的答案, 也就是s[i]往前最多推多远的合法子串</li><li>最终的答案要遍历dp[]</li></ol><p>需要消除重复的问题:</p><ul><li>dp[] = new int[26]; dp[i]表示以char(‘a’+i)结尾的子串答案</li></ul><p>子序列问题的两种方法:</p><ul><li>第一种方法: dp[i]表示以i结尾的子序列答案, 时间复杂度O(n^2)</li><li>第二种方法:<ul><li>对于2个串: dp[i][j]表示s1[0…i]和s2[0…j]的答案</li><li>对于1个串: dp[i][j]表示s[i…j]的答案</li></ul></li></ul><h2 id="子数组问题">子数组问题</h2><table><thead><tr><th>子数组问题</th><th></th></tr></thead><tbody><tr><td></td><td><a href="https://leetcode.cn/problems/maximum-subarray/">53.最大子数组和</a> <a href="https://leetcode.cn/problems/house-robber/">198.打家劫舍</a> <a href="https://leetcode.cn/problems/maximum-product-subarray/">152.乘积最大子数组</a></td></tr><tr><td>二分答案法+打家劫舍</td><td><a href="https://leetcode.cn/problems/house-robber-iv/">2560.打家劫舍IV</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/maximum-sum-of-3-non-overlapping-subarrays/">689.三个无重叠子数组的最大和</a></td></tr><tr><td>环形子数组问题</td><td><a href="https://leetcode.cn/problems/maximum-sum-circular-subarray/">918.环形子数组的最大和</a> <a href="https://leetcode.cn/problems/house-robber-ii/">213.打家劫舍II</a></td></tr><tr><td>子矩阵问题</td><td>(压缩数组)<a href="https://leetcode.cn/problems/max-submatrix-lcci/">面试题17.24.最大子矩阵</a></td></tr></tbody></table><h2 id="递增子序列问题">递增子序列问题</h2><table><thead><tr><th>递增子序列问题</th><th></th></tr></thead><tbody><tr><td></td><td><a href="https://leetcode.cn/problems/longest-increasing-subsequence/">300.最长递增子序列</a> <a href="https://leetcode.cn/problems/russian-doll-envelopes/">354.俄罗斯套娃信封问题</a> <a href="https://leetcode.cn/problems/minimum-operations-to-make-the-array-k-increasing/">2111.使数组K递增的最少操作次数</a></td></tr><tr><td>end更新和查询的分离</td><td><a href="https://leetcode.cn/problems/maximum-length-of-pair-chain/">646.最长数对链</a></td></tr></tbody></table><h2 id="背包DP">背包DP</h2><table><thead><tr><th>背包问题</th><th></th></tr></thead><tbody><tr><td>0-1背包问题</td><td>不可复选, 0-1背包问题本体是求可以不装满的最大价值</td></tr><tr><td></td><td>求最大 <a href="https://leetcode.cn/problems/ones-and-zeroes/">474.一和零</a> <a href="https://leetcode.cn/problems/tJau2o/">bytedance-006.夏季特惠</a> <a href="https://leetcode.cn/problems/last-stone-weight-ii/">1049.最后一块石头的重量II</a></td></tr><tr><td></td><td>恰好装满 <a href="https://leetcode.cn/problems/partition-equal-subset-sum/">416.分割数组</a> <a href="https://leetcode.cn/problems/length-of-the-longest-subsequence-that-sums-to-target/">2915.和为目标值的最长子序列</a> <a href="https://leetcode.cn/problems/target-sum/">494.目标和</a>  <a href="https://leetcode.cn/problems/ways-to-express-an-integer-as-sum-of-powers/">2787.数字表示成幂的和</a></td></tr><tr><td></td><td>最接近: 一个变量记录大于的情况, 求不超过容量的最大, 两者取近的 <a href="https://leetcode.cn/problems/closest-dessert-cost/">1774.最接近目标价格的甜点成本</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/maximum-multiplication-score/">3290.最高乘法得分</a></td></tr><tr><td>分组背包</td><td>一组内的物品只能或者最多选1个 <a href="https://leetcode.cn/problems/maximum-value-of-k-coins-from-piles/">2218.从栈中取出K个硬币的最大面值和</a></td></tr><tr><td>完全背包</td><td>可复选 <a href="https://leetcode.cn/problems/regular-expression-matching/">10.正则表达式匹配</a> <a href="https://leetcode.cn/problems/wildcard-matching/">44.通配符匹配</a> <a href="https://leetcode.cn/problems/coin-change/">322.凑零钱</a> <a href="https://leetcode.cn/problems/coin-change-ii/">518.凑零钱II</a> <a href="https://leetcode.cn/problems/perfect-squares/">279.完全平方数</a> <a href="https://leetcode.cn/problems/form-largest-integer-with-digits-that-add-up-to-target/">1449.固定成本的最大数字</a></td></tr><tr><td>多重背包</td><td>可复选, 有个数限制</td></tr></tbody></table><h2 id="区间DP">区间DP</h2><table><thead><tr><th>区间DP</th><th></th></tr></thead><tbody><tr><td>基于两侧端点讨论</td><td><a href="https://leetcode.cn/problems/minimum-insertion-steps-to-make-a-string-palindrome/">1312.让字符串成为回文串的最少插入次数</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/predict-the-winner/">486.预测赢家</a></td></tr><tr><td>基于范围上划分点的讨论</td><td><a href="https://leetcode.cn/problems/minimum-score-triangulation-of-polygon/">1039.多边形三角剖分的最低得分</a> <a href="https://leetcode.cn/problems/minimum-cost-to-cut-a-stick/">1547.切棍子的最小成本</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/burst-balloons/">312.戳气球</a> <a href="https://leetcode.cn/problems/boolean-evaluation-lcci/">面试题08.14.布尔运算</a></td></tr><tr><td>两种方法综合</td><td><a href="https://leetcode.cn/problems/strange-printer/">664.奇怪的打印机</a></td></tr><tr><td>带信息递归</td><td><a href="https://leetcode.cn/problems/remove-boxes/">546.移除盒子</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/minimum-cost-to-merge-stones/">1000.合并石头的最低成本</a> <a href="https://leetcode.cn/problems/count-different-palindromic-subsequences/">730.统计不同回文子序列</a></td></tr></tbody></table><h2 id="树形DP">树形DP</h2><table><thead><tr><th>树形DP</th><th></th></tr></thead><tbody><tr><td>二叉树DP</td><td><a href="https://leetcode.cn/problems/maximum-sum-bst-in-binary-tree/">1373.树的BST子树的最大键值和</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/diameter-of-binary-tree/">543.二叉树的直径</a> <a href="https://leetcode.cn/problems/distribute-coins-in-binary-tree/">979.在二叉树中分配硬币</a> <a href="https://leetcode.cn/problems/house-robber-iii/">337.打家劫舍III</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/binary-tree-cameras/">968.监控二叉树</a> <a href="https://leetcode.cn/problems/path-sum-iii/">437.路径总和III</a></td></tr><tr><td>多叉树DP</td><td><a href="https://leetcode.cn/problems/minimum-fuel-cost-to-report-to-the-capital/">2477.到达首都的最少油耗</a> <a href="https://leetcode.cn/problems/longest-path-with-different-adjacent-characters/">2246.相邻字符不同的最长路径</a></td></tr><tr><td>dfn序(Depth First Numbering)</td><td><a href="https://leetcode.cn/problems/height-of-binary-tree-after-subtree-removal-queries/">2458.移除子树后的二叉树高度</a> <a href="https://leetcode.cn/problems/minimum-score-after-removals-on-a-tree/">2322.从树中删除边的最小分数</a></td></tr></tbody></table><h2 id="状压DP">状压DP</h2><table><thead><tr><th>状态压缩DP</th><th></th></tr></thead><tbody><tr><td></td><td><a href="https://leetcode.cn/problems/can-i-win/">464.我能赢吗</a></td></tr><tr><td>分割等和子集(状态为1维)</td><td><a href="https://leetcode.cn/problems/matchsticks-to-square/">473.火柴拼正方形</a> <a href="https://leetcode.cn/problems/partition-to-k-equal-sum-subsets/">698.划分为k个相等的子集</a></td></tr><tr><td>TSP(Traveling Salesman Problem)问题</td><td></td></tr><tr><td>状态为2维</td><td><a href="https://leetcode.cn/problems/number-of-ways-to-wear-different-hats-to-each-other/">1434.每个人戴不同帽子的方案数</a> <a href="https://leetcode.cn/problems/the-number-of-good-subsets/">1994.好子集的数目</a> <a href="https://leetcode.cn/problems/distribute-repeating-integers/">1655.分配重复整数</a></td></tr></tbody></table><h2 id="状态机DP">状态机DP</h2><table><thead><tr><th>状态机DP</th><th></th></tr></thead><tbody><tr><td>买卖股票问题</td><td><a href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock/">121.买卖股票1次</a> <a href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-ii/">122.买卖股票无限次</a> <a href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-with-cooldown/">309.买卖股票无限次含冷冻期</a> <a href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-with-transaction-fee/">714.买卖股票无限次含手续费</a> <a href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-iii/">123.买卖股票2次</a> <a href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-iv/">188.买卖股票k次</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/valid-permutations-for-di-sequence/">903.DI序列的有效排列</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/maximum-profit-in-job-scheduling/">1235.规划兼职工作</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/k-inverse-pairs-array/">629.K个逆序对数组</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/freedom-trail/">514.自由之路</a></td></tr></tbody></table><h2 id="数位DP">数位DP</h2><table><thead><tr><th>数位DP</th><th></th></tr></thead><tbody><tr><td></td><td><a href="https://leetcode.cn/problems/count-numbers-with-unique-digits/">357. 统计各位数字都不同的数字个数</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/numbers-at-most-n-given-digit-set/">902.最大为N的数字组合</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/count-of-integers/">2719.统计整数数目</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/count-special-integers/">2376.统计特殊整数</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/numbers-with-repeated-digits/">1012.至少有1位重复的数字</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/non-negative-integers-without-consecutive-ones/">600.不含连续1的非负整数</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/number-of-digit-one/">233.数字1的个数</a></td></tr></tbody></table><h2 id="技巧">技巧</h2><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>得到具体方案技巧</td><td><a href="https://leetcode.cn/problems/smallest-sufficient-team/">1125.最小的必要团队</a></td></tr><tr><td>根据数据量猜解法</td><td><a href="https://leetcode.cn/problems/make-array-strictly-increasing/">1187.使数组严格递增</a></td></tr></tbody></table><table><thead><tr><th>DP基础</th><th>题目</th></tr></thead><tbody><tr><td>记忆化搜索</td><td><a href="https://leetcode.cn/problems/longest-increasing-path-in-a-matrix/">329.矩阵最长递增路径</a> <a href="https://leetcode.cn/problems/word-break/">139.单词拆分</a> <a href="https://leetcode.cn/problems/word-break-ii/">140.单词拆分II</a> <a href="https://leetcode.cn/problems/longest-increasing-path-in-a-matrix/">329.矩阵的最长递增路径</a></td></tr><tr><td>爬楼梯问题</td><td><a href="https://leetcode.cn/problems/climbing-stairs/">70.爬楼梯</a> <a href="https://leetcode.cn/problems/min-cost-climbing-stairs/">746.爬楼梯最小花费</a> <a href="https://leetcode.cn/problems/combination-sum-iv/">377.组合总和Ⅳ</a> <a href="https://leetcode.cn/problems/count-ways-to-build-good-strings/">2466.构造好字符串方案数</a> <a href="https://leetcode.cn/problems/count-number-of-texts/">2266.打字方案数</a> <a href="https://leetcode.cn/problems/number-of-good-binary-strings/">🔒2533.好二进制字符串数量</a></td></tr><tr><td>打家劫舍问题</td><td><a href="https://leetcode.cn/problems/house-robber/">198.打家劫舍</a> <a href="https://leetcode.cn/problems/house-robber-ii/">213.打家劫舍II</a> <a href="https://leetcode.cn/problems/house-robber-iii/">337.打家劫舍III</a> <a href="https://leetcode.cn/problems/count-number-of-ways-to-place-houses/">2320.放置房子的方案数</a> <a href="https://leetcode.cn/problems/house-robber-iv/">2560.打家劫舍IV</a> <a href="https://leetcode.cn/problems/delete-and-earn/">740.删除获得点数</a> <a href="https://leetcode.cn/problems/maximum-total-damage-with-spell-casting/">3186.施咒的最大总伤害</a></td></tr><tr><td>子数组问题</td><td><a href="https://leetcode.cn/problems/maximum-subarray/">53.子数组最大和</a> <a href="https://leetcode.cn/problems/find-the-substring-with-maximum-cost/">2606.子串的最大开销</a> <a href="https://leetcode.cn/problems/maximum-absolute-sum-of-any-subarray/">1749.子数组和的绝对值的最大值</a> <a href="https://leetcode.cn/problems/maximum-sum-circular-subarray/">918.环形数组子数组最大和</a> <a href="https://leetcode.cn/problems/k-concatenation-maximum-sum/">1191.K次串联后子数组最大和</a> <a href="https://leetcode.cn/problems/maximum-score-of-spliced-array/">2321.拼接数组的最大分数</a></td></tr></tbody></table><table><thead><tr><th>矩阵上的DP</th><th></th></tr></thead><tbody><tr><td>路径和</td><td><a href="https://leetcode.cn/problems/li-wu-de-zui-da-jie-zhi-lcof/">LCR166.最大路径和</a> <a href="https://leetcode.cn/problems/minimum-path-sum/">64.最小路径和</a> <a href="https://leetcode.cn/problems/minimum-falling-path-sum/">931.下降最小路径和</a> <a href="https://leetcode.cn/problems/triangle/">120.三角形下降最小路径和</a> <a href="https://leetcode.cn/problems/minimum-path-cost-in-a-grid/">2304.下降最小路径和</a> <a href="https://leetcode.cn/problems/minimum-falling-path-sum-ii/">1289.下降最小路径和II</a></td></tr><tr><td>路径数量</td><td><a href="https://leetcode.cn/problems/unique-paths/">62.路径数</a> <a href="https://leetcode.cn/problems/unique-paths-ii/">63.路径数加障碍II</a></td></tr><tr><td>路径积</td><td><a href="https://leetcode.cn/problems/maximum-product-subarray/">152.子数组最大积</a> <a href="https://leetcode.cn/problems/maximum-non-negative-product-in-a-matrix/">1594.最大路径积</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/dungeon-game/">174.地下城游戏</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/maximum-number-of-moves-in-a-grid/">2684.矩阵中移动的最大次数</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/cherry-pickup/">741.摘樱桃</a></td></tr></tbody></table><table><thead><tr><th>序列问题</th><th></th></tr></thead><tbody><tr><td>LCS</td><td><a href="https://leetcode.cn/problems/distinct-subsequences/">115.不同子序列</a> <a href="https://leetcode.cn/problems/longest-common-subsequence/">1143.最长公共子序列</a></td></tr><tr><td>编辑距离问题</td><td><a href="https://leetcode.cn/problems/delete-operation-for-two-strings/">583.两个字符串的删除操作</a> <a href="https://leetcode.cn/problems/minimum-ascii-delete-sum-for-two-strings/">712.两个字符串的最小ASCII删除和</a> <a href="https://leetcode.cn/problems/edit-distance/">72.编辑距离</a></td></tr><tr><td>LIS</td><td><a href="https://leetcode.cn/problems/longest-increasing-subsequence/">300.最长递增子序列 </a> <a href="https://leetcode.cn/problems/russian-doll-envelopes/">354.信封问题</a></td></tr><tr><td>回文子序列</td><td><a href="https://leetcode.cn/problems/longest-palindromic-subsequence/">516.最长回文子序列</a> <a href="https://leetcode.cn/problems/minimum-insertion-steps-to-make-a-string-palindrome/">1312.字符串成为回文串的最少插入次数</a></td></tr></tbody></table><h1>贪心</h1><h1>博弈</h1><h1>丑数问题</h1><table><thead><tr><th>题号</th><th>解决方法</th></tr></thead><tbody><tr><td><a href="https://leetcode.cn/problems/count-primes/">204.统计质数</a> 计算小于n的质数的数量</td><td>从2开始, 2*2不是质数, 2*3不是质数…, 3*2不是质数, 3*3不是质数, …</td></tr><tr><td><a href="https://leetcode.cn/problems/ugly-number/">263.丑数 </a> 判断一个数是否是丑数(丑数是只有2, 3, 5为因子的数)</td><td></td></tr><tr><td><a href="https://leetcode.cn/problems/ugly-number-ii/">264.丑数 II</a> 返回第n个丑数</td><td></td></tr><tr><td><a href="https://leetcode.cn/problems/super-ugly-number/">313.超级丑数</a> 返回第n个超级丑数(超级丑数是只有指定数组内的数为因子的数)</td><td></td></tr><tr><td><a href="https://leetcode.cn/problems/ugly-number-iii/">1201.丑数 III</a></td><td></td></tr></tbody></table><h1>括号问题</h1><table><thead><tr><th>括号问题</th><th></th></tr></thead><tbody><tr><td>有效括号的性质</td><td><a href="https://leetcode.cn/problems/valid-parentheses/">20.有效的括号</a> <a href="https://leetcode.cn/problems/minimum-add-to-make-parentheses-valid/">921.使括号有效的最少添加</a> <a href="https://leetcode.cn/problems/minimum-insertions-to-balance-a-parentheses-string/">1541.平衡括号字符串的最少插入次数</a> <a href="https://leetcode.cn/problems/generate-parentheses/">22.括号生成</a> <a href="https://leetcode.cn/problems/remove-invalid-parentheses/">301.删除无效的括号</a> <a href="https://leetcode.cn/problems/longest-valid-parentheses/">32.最长有效括号</a></td></tr><tr><td>括号嵌套问题</td><td><a href="https://leetcode.cn/problems/longest-valid-parentheses/description/#">思路</a><a href="https://leetcode.cn/problems/basic-calculator/">224.基本计算器</a> <a href="https://leetcode.cn/problems/basic-calculator-ii/">227.基本计算器 II</a> <a href="https://leetcode.cn/problems/basic-calculator-iii/">772.基本计算器 III🔒</a></td></tr><tr><td></td><td><a href="https://leetcode.cn/problems/decode-string/">394.字符串解码</a> <a href="https://leetcode.cn/problems/number-of-atoms/">726. 原子的数量</a></td></tr></tbody></table><h1>树状数组</h1><table><thead><tr><th>树状数组问题</th><th>题目</th></tr></thead><tbody><tr><td></td><td><a href="https://leetcode.cn/problems/range-sum-query-mutable/">307.区域和检索-数组可修改</a></td></tr></tbody></table><h1>Segment Tree</h1><h1>摩尔投票</h1><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td></td><td><a href="https://leetcode.cn/problems/majority-element/">169.多数元素</a></td></tr></tbody></table><h1>KMP算法</h1><ol><li>next[]是什么? <code>next[i]</code>是 ==s2[0] ~ s2[i-1] 的前后缀最大匹配长度==(不包含s2[i] 这个字符, 也不包括整体字符), <code>next[0] = -1, next[1] = 0</code></li><li>怎么通过 next[] 加速匹配过程?<ul><li>当遇到不匹配的字符时: p1指针不动; p2指针回到next[p2]</li><li>(p2+1和p1+1匹配失败时, next[p2+1] = val)<ol><li>为什么p1 ~ p1-val 这部分开始不可能匹配成功?<ol><li>p2和p1匹配失败时, s1[0, p1] == s2[0, p2]</li><li>如果存在 0 ~ p1-val 之间的位置 k 匹配成功, 则三部分相等: s1[k, p1] = s2[k, p1] = s2[0, p1-k], 因为 k &lt; p1 - val, 所以这三个相等的部分的长度 &gt; val了, 矛盾</li></ol></li><li>为什么从s2的next[p2]位置开始匹配, s2的next[p2]前面的不管?<ol><li>p2和p1匹配失败时, s1[0, p1] == s2[0, p2]整体相等, s2[0, p2]的前val个字符 和 s2[0, p2]的后val个字符 相等</li><li>重新开始匹配时, s2的前val个字符不需要匹配了, 直接从第val+1个字符开始</li></ol></li></ol></li></ul></li></ol><h1>B站考过的题目</h1><p>求子数组最大乘积</p><p><a href="https://leetcode.cn/problems/lru-cache/">146. LRU Cache</a></p><p>全排列</p><p>链表判断是否有环</p><p>数组合并</p><p>合并两个有序数组（不使用额外空间）</p><p>反转层序遍历</p><p>非递归中序遍历</p><p><a href="https://leetcode.cn/problems/search-in-rotated-sorted-array/">33. Search in Rotated Sorted Array</a></p><p><a href="https://leetcode.cn/problems/reorder-list/">143. Reorder List</a></p><p><a href="https://leetcode.cn/problems/binary-tree-maximum-path-sum/">124. Binary Tree Maximum Path Sum</a></p><p><a href="https://leetcode.cn/problems/decode-ways/">91. Decode Ways</a></p><p><a href="https://leetcode.cn/problems/same-tree/">100. Same Tree</a></p><p><a href="https://leetcode.cn/problems/3sum/">15. 3Sum</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> 外挂标签 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GCN-VAE</title>
      <link href="/posts/2013454d.html"/>
      <url>/posts/2013454d.html</url>
      
        <content type="html"><![CDATA[<h1>摘要</h1><p>知识图是表示实体之间关系事实的强大抽象。由于大多数现实世界的知识图都是人工收集的，而且很大程度上是不完整的，因此预测给定图中的缺失链接对于知识图的使用变得至关重要。</p><p>近年来，在向量空间中嵌入实体和关系的研究取得了可喜的成果。</p><p><u>然而，大多数点估计方法的一个共同问题是，它们可能无法捕获更复杂的结构，例如一对多或多对多关系，它们也不能适应实体和关系具有内在不确定性和多重语义含义的场景。</u></p><p>为此，我们考虑了一个概率框架，并提出了变分自编码器GCN-VAE，它将实体的邻域结构编码为紧凑的隐嵌入分布，并通过解码器估计边缘似然来重建实体之间的链接。我们在两个流行的基准测试上展示了GCN-VAE的有效性，并在FB15k-237的最先进基准上获得了改进的性能。</p><h1>1 介绍</h1><p>知识图(KG)是一种表示关系数据的强大抽象。它在问答、数据洞察、搜索、医学本体等许多应用程序中都很有用，在这些应用程序中，可以从图结构数据中高效地推断出有效的关系提取。知识图谱由事实三元组(头实体、关系、尾实体)的集合组成，例如巴拉克·奥巴马，美国总统。它也可以转换成由实体作为节点和关系作为不同类型边组成的异构图。</p><p>大多数现实世界的知识图谱都是由人类收集和创建的，这是一个漫长的人工过程，有很多事实是不完整的(缺少实体或它们之间缺少联系)。因此，许多链接预测算法都对捕获图信息感兴趣，以便从给定的不完全图中推断可能的未知事实。</p><p>研究发现，现有的链接预测方法主要集中在计算知识图中每个实体和关系的点向量表示。然而，<u>它们中的许多都不能捕捉对称的、复合的、逆的关系。</u>虽然有些人可以在复杂空间中巧妙地设计算法来解决上述场景，例如RotateE [Sun等人，2019]，但<u>这些方法仍然可能在(非内射或多对多关系) 或 (实体或关系具有固定向量值无法捕获的多个语义含义)中失败</u>，</p><p>为此，我们为知识图嵌入方法设计了一个概率框架，旨在对<u>实体和关系的不确定性进行建模。</u></p><p>虽然最近深度生成方法在图像、文本和一般图形结构化数据上取得了优异的成绩，但很少有知识图领域的成功。先前使用生成模型的研究之一是KG2E [He et al.， 2015]，它是使用高斯模型的概率版本的TransE [Bordes et al.， 2013]。</p><p>我们希望通过设计一个更复杂的生成模型来进一步探索生成模型在知识图上的表示能力。本文介绍了一种基于关系图卷积网络的变分自编码器GCN-VAE，它将知识图邻域编码为非高斯隐分布，并通过预测每个可能三元组的似然来重建图。代码可在https://github.com/karenyang/GCN-VAE上获得。</p><h1>2 相关工作</h1><p>TransE, TransH, DistMult</p><p>DeepWalk, node2vec</p><p>ConvE, R-GCN,</p><p>GraphVAE, Graphite, GraphRNN</p><h1>3 方法</h1><h2 id="3-1-问题描述">3.1 问题描述</h2><h3 id="3-1-1-输入">3.1.1 输入</h3><p>假设我们有一个三元组$\Large D = {(e_h， τ， e_t)}$形式的知识图数据集。eh, et是头部实体和尾部实体，n表示数据集中实体的总数。τ是它们之间的边或关系，R表示数据集中所有关系类型的集合。</p><p>我们把它转换成一个由定义的图结构: $\Large G = (E, A)$</p><p>实体属性矩阵: $\Large E\in R^{n\times d_e}$</p><p>关系特定邻接表: $\Large  A = \lbrace A_\tau, \forall\tau \in R \rbrace $</p><h3 id="3-1-2-KG-completion-Task">3.1.2 KG completion Task</h3><p>![image-20230906162003742](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230906162003742.png)</p><p>该模型被训练来学习给定输入图G = (E, A)的实体嵌入和关系嵌入，并预测任何给定三元组在Link Prediction任务中执行的可能性，如图1所示。</p><p>我们只提供从数据集中采样到模型的输入图中50%的边作为输入，因为我们希望模型从图结构的子集中学习并推断看不见的边。</p><p>重构图的联合概率可表示为:</p><p>![image-20230906161937171](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230906161937171.png)</p><h3 id="3-1-3-训练">3.1.3 训练</h3><p>由于存在大量可能的三元组(O(|Ne| x |R| x |Ne|))，直接对它们进行推理在计算上太昂贵了。这也会导致一个不平衡的训练集，负三元组比正三元组要多得多</p><p>因此，我们通过负抽样来训练我们的模型，其中我们通过破坏正确三元组的头部或尾部实体(但不是两者)来采样每个正样本的k个负样本。<strong>重构损失可以看作是正样本集D+和负样本集D−上的二分类交叉熵损失。</strong></p><p>该模型的目标是==最小化近似的负ELBO==(证据下限)，定义为:</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230906162503122.png" alt="image-20230906162503122" style="zoom:75%;" /><p>第一项和第二项是采样输入图的重构损失。第三项是变分条件分布与真条件分布之间的KL散度。为了避免过拟合，我们还添加了正则化项。</p><p>KL散度用来描述学习的分布和高斯分布之间的相似性, ==KL散度逼迫Encoder向着正态分布方向解码原始数据==</p><p>![image-20230906170907550](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230906170907550.png)</p><p>Figure 2: GCN-VAE模型体系结构。给定在局部邻域G = (E, A)上定义的输入图，编码器使用图卷积神经网络对实体的邻域进行聚合，并输出潜在嵌入分布的均值和方差。然后从分布中采样一个潜在代码z。在我们提出的利用IAF(逆自回归流)的模型之一中，潜在代码Z首先经过3层IAF变换以形成更复杂的非高斯后验。最后，在关系的条件下，解码器接受采样的潜在代码和关系嵌入，并为可能的三元组输出似然分数。</p><h2 id="3-2-预处理">3.2 预处理</h2><p>在这里，我们解释将三元组转换为给定实体的局部图邻域的处理步骤，作为3.1中定义的输入表示G。</p><p>为了处理像FB15k-237这样的大型数据集，我们将局部邻域作为子图进行采样。我们收集了一个大小为n的局部邻域作为一个批次。从一个根实体开始，我们对所有关系边类型使用BFS来收集本地邻域，直到我们到达终点(并跳转到另一个根实体)或到达n个节点。因此，大多数采样子图包含一个或多个连通分量。</p><p>在抽取子图中的n个实体后，构建实体和关系的嵌入。我们考虑了实体的最简单的实体表示形式——这批中每个节点的one-hot id，它将被传递到模型的嵌入层，该嵌入层初始化实体嵌入矩阵$\Large E\in R^{n\times d_e}$。对于每个关系，我们将反向关系视为关系类型的另一个独立实例。该模型为每个关系和逆关系构建一个one-hot id，以及子图上一个关系特定的邻接表$ \Large  A = \lbrace A_\tau, \forall\tau \in R \rbrace$</p><h2 id="3-3-编码器">3.3 编码器</h2><h3 id="3-3-1-multi-relation-GCN">3.3.1 multi-relation GCN</h3><p>该编码器被建模为称为R-GCN的图卷积网络的多关系扩展[Schlichtkrull等人，2018]，以提取每个实体的邻域信息。</p><p>在每一层，图的传播规则建模为:  R-GCN公式</p><p>我们设计了具有2层R-GCN块的模型，该块具有捕获2跳邻域信息的能力。</p><p>编码器的输出是潜在表示z的分布参数。我们将其建模为高斯后验，以生成z的均值和方差: $ \Large\mu ,\delta = q_\phi(Z|R, E) $</p><h3 id="3-3-2-IAF-Non-Gaussian-posterior-with-Inverse-Autoregressive-Flow-非高斯后验逆自回归流">3.3.2  (IAF)Non-Gaussian posterior with Inverse Autoregressive Flow 非高斯后验逆自回归流</h3><p>为了将后验$\Large q_\phi (z|G)$建模为更灵活的非高斯分布，而不是简单的多变量高斯分布，我们还研究了逆自回归流(IAF)的概念, 该方法在图像生成任务中取得了成功，有助于改进节点嵌入分布的密度估计。</p><p>在获得潜在表示的均值和方差后，该模型首先对一个潜在嵌入z(0)进行采样，并将其通过3层掩码变换，每个变量以其前一个变量为条件，随机任意排序。</p><p>![image-20230907172256900](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230907172256900.png)</p><p>其中$\bigodot$为元素积，$\mu_i$和$\sigma_i$为流变换层的参数。在这样的变换之后，我们需要从密度估计中减去变换的雅可比矩阵的对数行列式，密度估计是方程2中定义的模型损失中期望项的一部分:</p><p>![image-20230907172436745](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230907172436745.png)</p><h3 id="3-3-3-理论分析">3.3.3 理论分析</h3><p>我们证明了GCN-VAE编码器可以对任意给定知识图上的计算进行主观编码。</p><h2 id="3-4-解码器">3.4 解码器</h2><h3 id="3-4-1-Sampling-采样">3.4.1 Sampling 采样</h3><p>对于每个由头或尾实体e∈Ne和关系r∈r组成的查询三元组，解码器绘制嵌入向量z ~ qφ(z|G)和关系嵌入张量Eτ ~ pθ(Eτ |G)。在这里，关系嵌入也作为一个独立的潜在变量从实体嵌入中撤退，用θ参数化。</p><p>为了对实体和关系的语义歧义进行建模，我们引入高斯混合分布作为嵌入空间的先验分布。我们假设，如果在嵌入空间中存在多个实体和关系的聚类，混合高斯先验可以更好地建模分布。我们将混合物的数量设计为k = 10。</p><p>在生成任务中，我们首先从10个混合物中的一个中抽样，然后从高斯分布中抽样。![image-20230907180935771](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230907180935771.png)</p><h3 id="3-4-2-Likelihood-Estimation-似然估算-最大似然估计">3.4.2 Likelihood Estimation 似然估算, 最大似然估计</h3><p>在采样步骤之后，我们使用双线性DistMult操作(实体和关系嵌入的点积)和一个sigmoid层来对每个三元组的边缘似然进行评分:![image-20230906165248885](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230906165248885.png)</p><p>在训练时，解码器对关系类型进行条件设置，并输出所有正、负三元组的似然，如3.1.3所述。</p><p>在求值时，解码器枚举给定一个头和一个关系的所有可能的尾实体(或枚举给定一个尾和一个关系的所有头实体)，它使用批处理矩阵乘法运算有效地计算所有可能的三元组的可能性。</p><h1>4 实验</h1><h2 id="4-1-数据集">4.1 数据集</h2><p>![image-20230906165449454](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230906165449454.png)</p><h2 id="4-2-Link-prediction链接预测">4.2 Link prediction链接预测</h2><p>我们预测一对实体之间是否存在关系。我们使用1:10作为正负样本的比例进行训练，整体的负抽样训练过程见3.1.3。</p><p>链路预测任务的常用评价指标是平均倒数排名(MRR)和正确实体在前N个排名中的比例(命中数Hits)。函数rank(·)通过用N~e~中的所有其他实体替换尾实体，比较正确三元组在所有损坏三元组中的得分。</p><p>![image-20230906165914393](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230906165914393.png)</p><p>我们将TransE和R-GCN(强基线)作为baseline。原始的R-GCN使用整个数据集作为输入图，无法扩展到两个大数据集，因此我们按照3.2的描述对子图进行采样作为输入，用于R-GCN的训练和评估。我们还采用了DistMult[Yang等人，2014]和RotateE[Sun等人，2019]上的论文作为比较指标。见表2: 基于MRR和Hits@m的链路预测任务性能比较。我们提出了3种变体，分别是单一高斯先验(k=1)的GCN-VAE、混合高斯先验(k=10)的GCN-GMVAE，以及混合高斯先验(k=10)和3层IAF变换的GCN-GMVAE-IAF。*表示结果直接取自相应论文。</p><h1>5 Conclusion and Future Work</h1><p>本文提出了一种新的概率框架GCN-VAE，用于学习大规模知识图中实体和关系的表示。GCN-VAE由一个关系图卷积网络编码器和一个解码器组成，前者用于推导给定输入图的嵌入分布，后者通过预测每个可能三元组的似然来重构图。我们学习了知识图的关系结构的强大表示，并在链接预测基准上取得了优越或接近SOTA的结果。我们发现，<u>在知识图中对实体和关系的不确定性进行建模有助于提高链接预测任务的准确性和泛化性。</u>这可以看作是来自R-GCN模型的GCN-GMVAE(k=10)模型与我们模型的单高斯先验变量之间性能的提升。由于我们模型的编码器是建立在R-GCN之上的，所以性能的提高可以用它们的主要不同来解释:GCN-VAE模型是嵌入空间的概率分布。我们认为，允许具有更复杂上下文的三元组具有更多的不确定性是知识图嵌入的理想特征。我们还发现，通过改进变分推理过程，用基于流的层建模更灵活的非高斯后验提供了额外的收益。在未来，我们将通过实验不同的解码器结构(如RotateE、ConvE或端到端解码器结构)来扩展这项研究。我们还想可视化潜在嵌入空间，以更好地理解不同类型的实体和关系的不确定性。最后，我们可以通过提供更多的对抗性负样本来改进模型训练过程。</p>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> 外挂标签 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac配置</title>
      <link href="/posts/2013454d.html"/>
      <url>/posts/2013454d.html</url>
      
        <content type="html"><![CDATA[<h1>快捷键</h1><h2 id="编辑快捷键">编辑快捷键</h2><p>control + L: 跳回光标位置</p><p>control + D: 往后删除</p><p>control + K: 删除全部光标之后的内容</p><p>control + a/e:</p><p>control + b/n/f/p: 实现方向键</p><h2 id="macOS技巧快捷键">macOS技巧快捷键</h2><p>Command + I: 显示简介</p><p>移动文件: command + C  然后 Command + option + V</p><p>Finder: Command + N 新建窗口</p><p>快速开关Dock: Command + option + D</p><p>打开文件: commamd + O</p><h2 id="typora快捷键">typora快捷键</h2><p>公式块：⌥⌘B</p><p>任务列表：⌥⌘X</p><p>链接引用：⌥⌘L</p><p>脚注：⌥⌘R</p><p>水平分割线：⇧⌘- (shift + command + -)</p><p>内联公式：⌃M (control + M)<br>删除线：⌃~ (control + ~)<br>注释：⌃- (control + -)</p><p>超链接: ⌘K (command + K)<br>清除样式：⌘\</p><p>显示/隐藏侧边栏：⇧⌘L (shift + command + L)<br>大纲视图：⌃⌘1 (control + command + 1)<br>文档列表视图：⌃⌘2 (control + command +2)<br>文件树视图：⌃⌘3 (control + command + 3)</p><p>段落：⌘o 不生效，快捷键冲突，使用⌃o (control + o)</p><h1>快速打开访达</h1><p>我设置了 <code>control + 空格</code>快速打开访达</p><p>自动操作.app -&gt; 新建文稿 -&gt; 快速操作 -&gt; 打开应用程序 -&gt; 开启应用程序-其他-访达 -&gt; 右上角测试运行 -&gt; 保存</p><p>键盘快捷键 -&gt; 服务 -&gt; 通用来设置快捷键</p><h1>Rime输入法</h1><p><a href="https://github.com/ssnhd/rime">https://github.com/ssnhd/rime</a></p><p>全局设置文件 <code>default.custom.yaml</code>，包含输入方案、候选词个数、中英文切换、快捷键。</p><h4 id="中英文切换">中英文切换</h4><p>下面代码表示使用 <code>Caps</code> 键切换大小写，使用 <code>Shift</code> 键切换中英文。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ascii_composer/good_old_caps_lock:</span> <span class="literal">true</span> <span class="comment"># 若为 true， Caps 只切换大小写</span></span><br><span class="line"><span class="attr">ascii_composer/switch_key:</span></span><br><span class="line">  <span class="attr">Caps_Lock:</span> <span class="string">commit_code</span>                <span class="comment"># Caps 键</span></span><br><span class="line">  <span class="attr">Shift_L:</span> <span class="string">commit_code</span>                  <span class="comment"># 左 Shift，切换中英文</span></span><br><span class="line">  <span class="attr">Shift_R:</span> <span class="string">commit_code</span>                  <span class="comment"># 右 Shift，切换中英文</span></span><br><span class="line">  <span class="attr">Control_L:</span> <span class="string">noop</span>                       <span class="comment"># 左 Control，屏蔽该切换键</span></span><br><span class="line">  <span class="attr">Control_R:</span> <span class="string">noop</span>                       <span class="comment"># 右 Control，屏蔽该切换键</span></span><br></pre></td></tr></table></figure><blockquote><p>注：如果 Caps 键不能切换大小写，打开 Mac 系统偏好设置 &gt; 键盘 &gt; 输入法 &gt; 取消【使用大写锁定键切换“美国”输入模式】。</p></blockquote><p>其他切换策略：</p><ul><li><strong>inline_ascii</strong>：在输入法的临时英文编辑区内输入字母、数字、符号、空格等，回车上屏后自动复位到中文</li><li><strong>commit_text</strong>：已输入的候选文字上屏并切换至英文输入模式</li></ul><h4 id="翻页快捷键">翻页快捷键</h4><ul><li><strong>when</strong>：有几种状态 <code>composing</code>、<code>has_menu</code>、<code>paging</code></li><li><strong>accept</strong>：控制接受的按键 <code>minus</code>、<code>equal</code>,、<code>period</code>、<code>comma</code>、<code>bracketleft</code>、<code>bracketright</code></li><li><strong>send</strong>：控制动作 <code>Page_Up</code>、<code>Page_Down</code>、<code>Escape</code>(清空输入码)</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 翻页</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">has_menu</span>, <span class="attr">accept:</span> <span class="string">Tab</span>, <span class="attr">send:</span> <span class="string">Page_Down</span> &#125;            <span class="comment"># &quot;tab&quot; 键翻页, 和下一条 &quot;tab&quot; 键分词只能二选一</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">composing</span>, <span class="attr">accept:</span> <span class="string">Shift+Tab</span>, <span class="attr">send:</span> <span class="string">Shift+Left</span> &#125;    <span class="comment"># &quot;Shift+Tab&quot; 键向左选拼音分词</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">paging</span>, <span class="attr">accept:</span> <span class="string">minus</span>, <span class="attr">send:</span> <span class="string">Page_Up</span> &#125;              <span class="comment"># &quot;-&quot; 上一页</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">has_menu</span>, <span class="attr">accept:</span> <span class="string">equal</span>, <span class="attr">send:</span> <span class="string">Page_Down</span> &#125;          <span class="comment"># &quot;=&quot; 下一页</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">paging</span>, <span class="attr">accept:</span> <span class="string">comma</span>, <span class="attr">send:</span> <span class="string">Page_Up</span> &#125;              <span class="comment"># &quot;,&quot; 上一页</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">has_menu</span>, <span class="attr">accept:</span> <span class="string">period</span>, <span class="attr">send:</span> <span class="string">Page_Down</span> &#125;         <span class="comment"># &quot;.&quot; 下一页</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">paging</span>, <span class="attr">accept:</span> <span class="string">bracketleft</span>, <span class="attr">send:</span> <span class="string">Page_Up</span> &#125;        <span class="comment"># &quot;[&quot; 上一页</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">has_menu</span>, <span class="attr">accept:</span> <span class="string">bracketright</span>, <span class="attr">send:</span> <span class="string">Page_Down</span> &#125;   <span class="comment"># &quot;]&quot; 下一页</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 快捷键</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">has_menu</span>, <span class="attr">accept:</span> <span class="string">semicolon</span>, <span class="attr">send:</span> <span class="number">2</span> &#125;              <span class="comment"># &quot;:&quot; (分号)选择第 2 个候选词</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">has_menu</span>, <span class="attr">accept:</span> <span class="string">apostrophe</span>, <span class="attr">send:</span> <span class="number">3</span> &#125;             <span class="comment"># &quot;&#x27;&quot; (引号)选择第 3 个候选词</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">composing</span>, <span class="attr">accept:</span> <span class="string">Shift+Tab</span>, <span class="attr">send:</span> <span class="string">Shift+Left</span> &#125;    <span class="comment"># &quot;Shift+Tab&quot; 键向左选拼音分词</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">composing</span>, <span class="attr">accept:</span> <span class="string">Control+a</span>, <span class="attr">send:</span> <span class="string">Home</span> &#125;          <span class="comment"># &quot;Control+a&quot; 光标移至首</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">composing</span>, <span class="attr">accept:</span> <span class="string">Control+e</span>, <span class="attr">send:</span> <span class="string">End</span> &#125;           <span class="comment"># &quot;Control+e&quot; 光标移至尾</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">composing</span>, <span class="attr">accept:</span> <span class="string">Control+g</span>, <span class="attr">send:</span> <span class="string">Escape</span> &#125;        <span class="comment"># &quot;Control+g&quot; 清码</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">composing</span>, <span class="attr">accept:</span> <span class="string">Return</span>, <span class="attr">send:</span> <span class="string">Escape</span> &#125;           <span class="comment"># &quot;Return&quot; 回车清码</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">always</span>, <span class="attr">accept:</span> <span class="string">Control+Shift+1</span>, <span class="attr">select:</span> <span class="string">.next</span> &#125;             <span class="comment"># 切换输入方案</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">always</span>, <span class="attr">accept:</span> <span class="string">Control+Shift+2</span>, <span class="attr">toggle:</span> <span class="string">ascii_mode</span> &#125;        <span class="comment"># 中/英文切换</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">always</span>, <span class="attr">accept:</span> <span class="string">Control+Shift+3</span>, <span class="attr">toggle:</span> <span class="string">full_shape</span> &#125;        <span class="comment"># 全角/半角切换</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">always</span>, <span class="attr">accept:</span> <span class="string">Control+Shift+4</span>, <span class="attr">toggle:</span> <span class="string">simplification</span> &#125;    <span class="comment"># 繁简体切换</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">always</span>, <span class="attr">accept:</span> <span class="string">Control+Shift+5</span>, <span class="attr">toggle:</span> <span class="string">extended_charset</span> &#125;  <span class="comment"># 通用/增广切换（显示生僻字）</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">composing</span>, <span class="attr">accept:</span> <span class="string">Control+b</span>, <span class="attr">send:</span> <span class="string">Left</span> &#125;           <span class="comment"># &quot;Control+b&quot; 移动光标</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">composing</span>, <span class="attr">accept:</span> <span class="string">Control+f</span>, <span class="attr">send:</span> <span class="string">Right</span> &#125;          <span class="comment"># &quot;Control+f&quot; 向右选择候选词</span></span><br><span class="line"><span class="bullet">-</span> &#123; <span class="attr">when:</span> <span class="string">composing</span>, <span class="attr">accept:</span> <span class="string">Control+h</span>, <span class="attr">send:</span> <span class="string">BackSpace</span> &#125;      <span class="comment"># &quot;Control+h&quot; 删除输入码</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 演示 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> 外挂标签 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python</title>
      <link href="/posts/2013454d.html"/>
      <url>/posts/2013454d.html</url>
      
        <content type="html"><![CDATA[<h1>虚拟环境</h1><p><strong>验证anaconda环境是否安装成功</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda --version</span><br></pre></td></tr></table></figure><p><strong>查看==当前环境==已经安装了哪些包</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure><p><strong>创建虚拟环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n env_name python=3.9</span><br></pre></td></tr></table></figure><p>创建一个名为env_name的虚拟环境, 3.9是python的版本, 一定要指定具体 python 版本，指定多少都可以</p><p><strong>查看所有环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda envs list</span><br></pre></td></tr></table></figure><p><strong>激活conda创建的虚拟环境</strong></p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda <span class="built_in">activate</span> env_name</span><br></pre></td></tr></table></figure><p><strong>激活pycharm创建的虚拟环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd xxx/venv/bin</span><br><span class="line">source activate</span><br></pre></td></tr></table></figure><p><strong>关闭虚拟环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactive</span><br></pre></td></tr></table></figure><p><strong>删除环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove -n [env_name] --all</span><br></pre></td></tr></table></figure><h1>附录</h1><h2 id="关于环境">关于环境</h2><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230613203104426.png" alt="image-20230613203104426" style="zoom:50%;" /><h2 id="关于homebrew">关于homebrew</h2><p>查找在homebrew安装软件的路径</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">brew</span> list python@<span class="number">3</span>.<span class="number">8</span></span><br></pre></td></tr></table></figure><h1>PyTorch笔记</h1><h2 id="张量数据类型">张量数据类型</h2><p><strong>dimension/rank = 0</strong></p><p>用来当loss</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">1.3</span>)</span><br><span class="line">a.shape  <span class="comment"># torch.Size([])</span></span><br><span class="line">a.size()  <span class="comment">#torch.Size([])</span></span><br></pre></td></tr></table></figure><p><strong>dimension/rank = 1</strong></p><p>用来当bias</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定数值</span></span><br><span class="line">torch.tensor([<span class="number">1.3</span>])</span><br><span class="line">torch.tensor([<span class="number">1.3</span>, <span class="number">1.5</span>])</span><br><span class="line"><span class="comment"># 随机初始化，需要指定类型</span></span><br><span class="line">torch.FloatTensor(<span class="number">1</span>)</span><br><span class="line">torch.FloatTensor(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p><code>dim</code>指<code>size(shape)</code>的长度</p><p><strong>dimension/rank = 2</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">a = torch.FloatTensor(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">a.shape  <span class="comment"># torch.size([2, 3])</span></span><br><span class="line">a.shape[<span class="number">0</span>]  <span class="comment"># 2</span></span><br><span class="line">a.shape[<span class="number">1</span>]  <span class="comment"># 3</span></span><br><span class="line">a.size(<span class="number">0</span>)  <span class="comment"># 2</span></span><br><span class="line">a.size(<span class="number">1</span>)  <span class="comment"># 3</span></span><br></pre></td></tr></table></figure><p><strong>dimension/rank = 3</strong></p><p>==几维就是几个中括号==</p><p>3维主要是RNN处理语言，4维主要是CNN处理图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;tensor([[[0.5464, 0.0631, 0.3082],</span></span><br><span class="line"><span class="string">         [0.7046, 0.3507, 0.9463]]])&#x27;&#x27;&#x27;</span></span><br><span class="line">a[<span class="number">0</span>]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;tensor([[0.5464, 0.0631, 0.3082],</span></span><br><span class="line"><span class="string">        [0.7046, 0.3507, 0.9463]])&#x27;&#x27;&#x27;</span></span><br><span class="line">a.shape  <span class="comment"># torch.Size([1, 2, 3])</span></span><br><span class="line"><span class="built_in">list</span>(a.shape)  <span class="comment"># [1, 2, 3]</span></span><br></pre></td></tr></table></figure><p><strong>Mixed</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">a.numel()  <span class="comment"># 1*2*3=6</span></span><br><span class="line">a.dim()  <span class="comment"># 3</span></span><br><span class="line">a.<span class="built_in">type</span>()  <span class="comment"># torch.FloatTensor</span></span><br><span class="line"><span class="comment"># 使用GPU</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;mps&#x27;</span>) </span><br><span class="line">a = a.to(device)</span><br><span class="line">a.<span class="built_in">type</span>()  <span class="comment"># torch.mps.FloatTensor</span></span><br></pre></td></tr></table></figure><h2 id="创建Tensor">创建Tensor</h2><p><strong>import from numpy</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">2</span>, <span class="number">3.2</span>])</span><br><span class="line">torch.from_numpy(a)  <span class="comment"># tensor([2.0000, 3.2000], dtype=torch.float64)</span></span><br><span class="line">a = np.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">torch.from_numpy(a)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]], dtype=torch.float64)</span></span><br></pre></td></tr></table></figure><p><strong>import from List</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">2</span>, <span class="number">3.2</span>])  <span class="comment"># tensor([2.0000, 3.2000])</span></span><br><span class="line">torch.Tensor(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment"># 小写tensor接受现有数据；大写Tensor接受shape,生成没有初始化的tensor,也可接受现有数据</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;tensor([[5.3838e-36, 1.4013e-45, 5.3930e-36],</span></span><br><span class="line"><span class="string">        [1.4013e-45, 1.1905e-30, 1.4013e-45]])</span></span><br></pre></td></tr></table></figure><p><strong>set default type</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1.3</span>, <span class="number">2</span>]).<span class="built_in">type</span>()  <span class="comment"># torch.FloatTensor</span></span><br><span class="line">torch.set_default_tensor_type(torch.DoubleTensor)</span><br><span class="line">torch.tensor([<span class="number">1.2</span>, <span class="number">3</span>]).<span class="built_in">type</span>()  <span class="comment"># torch.DoubleTensor</span></span><br></pre></td></tr></table></figure><p><strong>uninitialized</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.empty(<span class="number">3</span>)  <span class="comment"># tensor([0.0000e+00, 6.1065e-04, 1.4013e-45])</span></span><br><span class="line">torch.empty(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;tensor([[ 1.5846e+29, -9.3984e-36,  2.5250e-29],</span></span><br><span class="line"><span class="string">        [ 6.1013e-31,  1.4013e-45,  1.4013e-45]])&#x27;&#x27;&#x27;</span></span><br><span class="line">torch.Tensor(<span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p><strong>rand/rand_like, randiant</strong></p><p><code>torch.rand(d1, d2...)</code>返回一个张量，包含了从区间[0,1)的均匀分布中抽取一组随机数，形状由参数size定义</p><p><code>torch.rand_like(a)</code>把a的shape读出来再喂给<code>torch.rand()</code></p><p><code>torch.randint(min, max, shape[])</code>值域为<code>[min, max)</code></p><p><strong>randn</strong></p><p><code>torch.randn(d1, d2...)</code>返回一个符合均值为0，方差为1的正态分布（标准正态分布）中填充随机数的张量</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.normal(mean=torch.full([<span class="number">10</span>], <span class="number">0.</span>), std=torch.arange(<span class="number">1</span>, <span class="number">0</span>, -<span class="number">0.1</span>))  </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;tensor([ 0.1269,  0.7113,  0.0566,  1.1826, -0.0949, -0.0872,  0.1874, -0.2995, 0.3418,  0.2775])</span></span><br></pre></td></tr></table></figure><p><strong>full</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.full([<span class="number">2</span>, <span class="number">3</span>], <span class="number">8</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;tensor([[8, 8, 8],</span></span><br><span class="line"><span class="string">        [8, 8, 8]])&#x27;&#x27;&#x27;</span></span><br><span class="line">torch.full([], <span class="number">8</span>)  <span class="comment"># tensor(8)  dim=0</span></span><br><span class="line">torch.full([<span class="number">3</span>], <span class="number">8</span>)  <span class="comment"># tensor([8, 8, 8])  dim=1</span></span><br></pre></td></tr></table></figure><p><strong>arrange</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.arrange(<span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line">torch.arrange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p><strong>linespace</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.linespace(<span class="number">1</span>, <span class="number">10</span>, steps=<span class="number">4</span>)</span><br><span class="line">torch.linespace(<span class="number">1</span>, <span class="number">10</span>, steps=<span class="number">10</span>)</span><br><span class="line">torch.linespace(<span class="number">1</span>, <span class="number">10</span>, steps=<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line">torch.logspace(<span class="number">1</span>, -<span class="number">1</span>, steps=<span class="number">10</span>)</span><br><span class="line">torch.logspace(<span class="number">1</span>, <span class="number">1</span>, steps=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><strong>ones/zeros/eye</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.ones(d1, d2...)</span><br><span class="line">torch.zeros(d1, d2...)</span><br><span class="line">torch.eye(d1, d2...)</span><br><span class="line">torch.eye(<span class="number">3</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;tensor([[1., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 1., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 1.]])&#x27;&#x27;&#x27;</span></span><br><span class="line">torch.ones_like(a)  <span class="comment"># 把a的shape提取出来喂给torch.ones()</span></span><br></pre></td></tr></table></figure><p><strong>randperm</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.randperm(<span class="number">10</span>)  <span class="comment"># tensor([5, 8, 9, 3, 7, 1, 0, 4, 6, 2])</span></span><br><span class="line">a = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.rand(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">idx = torch.randperm(<span class="number">2</span>)</span><br><span class="line">a = a[idx]</span><br><span class="line">b = b[idx]</span><br></pre></td></tr></table></figure><h2 id="索引与切片">索引与切片</h2><p><code>a[start:end:step]</code></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a[:, :, <span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>, <span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>].shape()</span><br><span class="line">a.index_select(<span class="number">0</span>, tensor[<span class="number">0</span>, <span class="number">2</span>]) <span class="comment"># 取前两张图片</span></span><br><span class="line">a.index_select(<span class="number">1</span>, tensor[<span class="number">1</span>, <span class="number">2</span>]) <span class="comment"># 取第二个通道</span></span><br><span class="line">a.index_select(<span class="number">2</span>, torch.arrange(<span class="number">8</span>)) <span class="comment"># 取前8行</span></span><br></pre></td></tr></table></figure><h2 id="维度变换">维度变换</h2><p><strong>.view</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">a.view(<span class="number">4</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br></pre></td></tr></table></figure><p><strong>unsqueeze/squeeze</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">b = torch.rand(<span class="number">32</span>)</span><br><span class="line">f = torch.rand(<span class="number">4</span>, <span class="number">32</span>, <span class="number">14</span>, <span class="number">14</span>)</span><br><span class="line"><span class="comment"># 要实现f+b</span></span><br><span class="line">b = b.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>).unsqueeze(<span class="number">0</span>)  <span class="comment"># torch.Size([1, 32, 1, 1])</span></span><br><span class="line">b.squeeze().shape  <span class="comment"># torch.Size([32])</span></span><br><span class="line">b.squeeze(<span class="number">0</span>).shape  <span class="comment"># torch.Size([32, 1, 1])</span></span><br><span class="line">b.squeeze(-<span class="number">1</span>).shape  <span class="comment"># torch.Size([1, 32, 1])</span></span><br></pre></td></tr></table></figure><p><strong>expand/expand_as</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b.expand(<span class="number">4</span>, <span class="number">32</span>, <span class="number">14</span>, <span class="number">14</span>).shape  <span class="comment"># torch.Size([4, 32, 14, 14])</span></span><br><span class="line"><span class="comment"># 这样就可以实现f+b</span></span><br></pre></td></tr></table></figure><p><strong>.t</strong></p><p>矩阵转置, 仅能转置dim=2的tensor</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.t()</span><br></pre></td></tr></table></figure><h2 id="拼接与拆分">拼接与拆分</h2><p><strong>cat</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">4</span>, <span class="number">32</span>, <span class="number">8</span>)</span><br><span class="line">b = torch.rand(<span class="number">5</span>, <span class="number">32</span>, <span class="number">8</span>)</span><br><span class="line">torch.cat([a, b], dim=<span class="number">0</span>).shape  <span class="comment"># torch.Size([9, 32, 8])</span></span><br></pre></td></tr></table></figure><p><strong>stack</strong></p><p>加1维</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">32</span>, <span class="number">8</span>)</span><br><span class="line">b = torch.rand(<span class="number">32</span>, <span class="number">8</span>)</span><br><span class="line">torch.stack([a, b], dim=<span class="number">0</span>).shape  <span class="comment"># torch.Size([2, 32, 8])</span></span><br></pre></td></tr></table></figure><p><strong>split</strong></p><p>按长度拆分</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">6</span>, <span class="number">32</span>, <span class="number">8</span>)  <span class="comment"># torch.Size([6, 32, 8])</span></span><br><span class="line">aa, bb, cc = a.split([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dim=<span class="number">0</span>)</span><br><span class="line">aa.shape, bb.shape, cc.shape  <span class="comment"># torch.Size([1, 32, 8]) torch.Size([2, 32, 8]) torch.Size([3, 32, 8])</span></span><br></pre></td></tr></table></figure><p><strong>chunk</strong></p><p>按数量拆分</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">6</span>, <span class="number">32</span>, <span class="number">8</span>)</span><br><span class="line">a1, a2, a3 = a.chunk(<span class="number">3</span>, dim=<span class="number">0</span>)  <span class="comment"># 分3个，torch.Size([2, 32, 8]) torch.Size([2, 32, 8]) torch.Size([2, 32, 8])</span></span><br></pre></td></tr></table></figure><h2 id="数学运算">数学运算</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.matmul(a, b)  <span class="comment"># 矩阵相乘</span></span><br></pre></td></tr></table></figure><h2 id="统计属性">统计属性</h2><p><code>.argmax()</code>返回最大值的一维索引</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a.argmax()  <span class="comment"># a.argmin()</span></span><br><span class="line">a.argmax(dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="梯度">梯度</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.tensor([<span class="number">2.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">mse = F.mse_loss(torch.ones(<span class="number">1</span>), x * w)</span><br><span class="line"><span class="built_in">print</span>(torch.autograd.grad(mse, [w]))  <span class="comment"># tensor([2.]) autograd.grad(y, [a, b, c])返回一个张量</span></span><br><span class="line"></span><br><span class="line">mse = F.mse_loss(torch.ones(<span class="number">1</span>), x * w)</span><br><span class="line">mse.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)  <span class="comment"># tensor([2.])</span></span><br></pre></td></tr></table></figure><p><strong>F.softmax</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(<span class="number">3</span>)</span><br><span class="line">a.requires_grad_()</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">p = F.softmax(a, dim=<span class="number">0</span>)</span><br><span class="line">p[<span class="number">0</span>].backward()</span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br><span class="line">p = F.softmax(a, dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.autograd.grad(p[<span class="number">0</span>], [a], retain_graph=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure><p><strong>全链接层求梯度</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">w = torch.randn(<span class="number">2</span>, <span class="number">10</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">a = F.sigmoid(x @ w.t())  <span class="comment"># torch.Size([1,2]) @为矩阵乘</span></span><br><span class="line">loss = F.mse_loss(torch.ones(<span class="number">1</span>, <span class="number">2</span>), a)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br></pre></td></tr></table></figure><h2 id="GPU">GPU</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;mps&#x27;</span>)</span><br><span class="line">net = MLP().to(device)</span><br><span class="line">criteon = nn.CrossEntropyLoss().to(device)</span><br><span class="line">data, target = data.to(device), target.to(device)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> 外挂标签 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>写论文</title>
      <link href="/posts/f41d85d2.html"/>
      <url>/posts/f41d85d2.html</url>
      
        <content type="html"><![CDATA[<h1>Abstract(摘要)</h1><ol><li>定义好研究任务: 知识图谱补全</li><li>目前的挑战, 可从前人工作的角度出发, 怎么做的, 有什么缺点</li><li>为了解决这个问题, 提出什么模型或者方法<ul><li>模型包括几部分</li><li>核心模块是什么, 作用是什么</li><li>其余模块的功能和效果</li></ul></li><li>实验结果表明</li></ol><h1>1 Introduction(引言)</h1><p><strong>第一段: 介绍知识图谱, 引出知识图谱补全</strong></p><p>In 2012, Google proposed the concept of knowledge graph(KG), which has attracted wide attention in the industry.</p><p>In KG, structured knowledge called facts is organized in the form of triples (subject, relation, object) or simply (s, r, o).</p><p>Knowledge graph has a good performance in question answering system [2], dialogue system [3], recommendation system [4], information extraction [5], etc.</p><p>Although modern KGs such as FreeBase [6], WordNet [7], Nell [8] have already contained millions of entities and triplets, they are still far away from complete due to the constantly emerging new knowledge.</p><p>Therefore, KG completion(KGC, also referred to as link prediction) has attracted increasing attention to predict new triples based on existing knowledge.</p><p><strong>第二段: 介绍主流KGE方法</strong></p><p>Recently, one of the promising research areas for knowledge completion is embedding-based method, a,k.a Knowlege Graph Embedding(KGE).</p><p>KGE focuses on learning low-dimensional embedding for entities and relations based on triples in KG.</p><p>Entity embeddings and relation embeddings are obtained by optimizing a scoring function defined on each fact (s,r,o) to measure its plausibility.</p><p>These models could be classified into three types:</p><p>a) bilinear translational distance models, which use a distance-based scoring function to learn the embeddings of entities and relations, such as TransE[5], TransH[6], TransR[7], TransD[8], TransSparse[9], and RotatE[10];</p><p>b) semantic matching models, which utilize product-based score functions to match latent semantics of entity and relation embeddings, such as RESCAL[15], DistMult[16], HolE[17], Complex[18];</p><p>c) convolutional neural network models, which leverage non-linear neural networks like convolutional neural networks to estimate scores, such as ConvE[19], ConvR[20], InteractE[21].</p><p><strong>第三段: 引出图神经网络</strong></p><p>Although these models have achieved excellent performance, they still ignored the meaningful graph context such as vast structural information.</p><p>In this way, Graph neural network based models become an active research focus for knowledge graph completion.</p><p>Most of these GNN based models can be seen as instantiation of Message Passing Neural Networks (MPNN) [22] framework.</p><p>Message embeddings are constructed from neighboring entity and relation embeddings and then are aggregated to update central entity embeddings.</p><p>R-GCN[23] proposes a novel framework for combining the aggregation ability of the GCN and the representative score function to construct the encoder-decoder paradigm in knowledge graph completion.</p><p>A GCN based model performs as an encoder used to capture graph information, and thereafter, a Convolutional Neural Networks (CNN) based model is employed as a decoder to predict scores.</p><p><strong>第三段: 讲问题</strong></p><p>Even though the KGE model based on GCN has achieved great success, there are still some unsolved problems.</p><p>In the process of learning the semantic feature of the central node, the feature of the neighbor node has a great contribution to the central node. Besides, the indirect neighbors of the central node also play an indispensable role in the semantic information of the central node.</p><p>As shown in Figure 1, 想个例子</p><p>GNN-based KGC model learn node embedding by aggregation of direct neighboring nodes embedding.</p><p>It is not enough to only aggregate one-hop neighbors. The semantically-related information can appear in both direct and indirect neighbors of central entities.</p><p>And the inderect neignboring nodes are captured by layer-by-layer propagation in traditional GCN network.</p><p>However, these GCN models exhibit lower performance with two layers and above than with one layers. This is enough to show that the traditinal GCN method of aggregating indirect neighbor nodes is inefficient.</p><p>This will result in these models are prone to underreact to an relevant inderect neighbor and they wipe out the rich structural and context information for KGs itself accordingly.</p><p><strong>第四段: 我的模型以及贡献</strong></p><p>In this paper, In order to solve the problems mentioned above, we propose a novel graph neural network, named TelepassGCN which can make better use of the two categories of neighboring node. Unlike traditional GCN, TelepassGCN aggregates indirect neighbor no longer in the layer-to-layer manner. Concretely, every TelepassGCN layer considers multiple hops neighbors to be one-hop neighbors. The central node is considered to have a virtual relationship with each multi-hop neighbor. To validate our hypothesis, we leverage virtual relation parameters matrix to enhance the expressive power of several existing message functions. Direct neighbors and indirect neighbors are aggregated using different aggregation functions to get different types of messages. TelepassGCN also follows MPNN framework. The features of indirect neighbors are not only aggregated by Telepass layer, but also by the layer-to-layer manner. We summarize the main contributions of our work as follows:</p><ul><li>We propose a novel Telepass GCN framework to improve the problem of inefficient aggregation of remote neighbor features. To the best of our knowledge, this is the first attempt to incorporate direct neighbors and inderect neighbors simultaneously in knowledge graph completion.</li><li>In order to collect indirect neighbor information, we propose the concept of multi-hop relation virtual graph. Beside, we propose a random sampling training method so that large graph can be trained on GPU.</li><li>In order to reduce the noise information, we further use the attention mechanism of remote neighborhood aggregation to find the important neighborhood in an end-to-end manner.</li><li>In order to combine the output representations of the different kinds of messages, we propose a multi-end balanced gate mechanism, obtaining the hidden representations in the current layer.</li><li>We conduct extensive experiments on real-world benchmark datasets to validate the effectiveness of our proposal.  Our TelepassGCN outperforms state-of-the-arts KGE models.</li></ul><p>(摘要的扩展)</p><p>第一段: 介绍KG, KG怎么来的, 以什么方式表示, 有哪些KG, KG应用在哪些地方, ==但是这些KG都不完整, 因此有了KGC==</p><p>第二段: KGC的主流方法是KGE, 主要分为三类: 1.翻译距离模型(translational distance models); 2.语义匹配模型(双线性模型Bilinear); 3.神经网络(neural network)</p><p>第三段: 这几类方法的缺点不足</p><p>第四段: 别人的图神经网络有什么不足</p><ul><li><p>Alinet:</p><ul><li><p>GNN由于具有识别同构子图的能力, GNN已成为基于嵌入的实体对齐的强大范例。然而，在知识图谱中，<strong>对应实体通常具有非同构的邻域结构，这很容易导致gnn对它们产生不同的表示。由于模式异质性，对等实体的直接邻居往往不相同，AliNet引入远邻居来扩大其邻居结构之间的重叠</strong></p><p>基于==语义相关信息可以同时出现在对等实体的直接邻域和远邻域的特点==，我们提出了聚合直接邻域和远邻域信息的KG对齐网络AliNet。</p><p>为了减少噪声信息，我们进一步采用了远程邻域聚合的注意机制，以端到端方式找出重要的邻域</p><p>虽然具L层的GCN可以捕获实体L跳邻居中的结构信息，但这种逐层传播的效率并不高。</p><p>对于两跳邻域聚合，我们引入了注意力机制，因为直接使用GCN的原始聚合会导致噪声信息跨层传播。</p></li></ul></li></ul><p>第六段: 我的模型, 本文贡献1,2,3,</p><p>==图1 表述问题==</p><ol><li>研究的任务定义, 研究该任务的意义</li><li>目前的挑战是什么, 举例说明</li><li>阐述显存工作, 有什么问题<ol><li>现在的工作, 分几大类</li><li>存在的缺点, 或者挑战还没有解决</li></ol></li><li>为了解决该问题, 提出了什么模型, 包括几部分, 展开介绍每个部分的功能和效果</li><li>本文贡献: 一般3点, 重点突出模块的作用和效果</li></ol><h1>2 Related Word(相关工作)</h1><h3 id="A-Distributed-Representation-Model">A. Distributed Representation Model</h3><p><strong>第一段</strong></p><p>In recent years, much research work has been devoted to KG completion. One of the most successful method is to project the entities and relations into continuous vector space to get distributed representation. Researchers have proposed lots of distributed representation methods including distance based models, semantic matching models and neural network based models.</p><p><strong>第二段</strong></p><p>Distance based models interpret relationships as translations operating on the low-dimensional embeddings of the entities.</p><p>TransE is a widely used distance based model whose score function is translating the head entity to tail entity according to the corresponding relation. The embedding of object eo entity eo should be close to the embedding of head entity es plus relation embedding er if (s,r, o) holds. This simple assumption of TransE is not suitable for 1-to-N, N-to-1, and N-to-N relations. So extensions of TransE have been developed like TransH, TransG, TransR.</p><p><strong>第三段</strong></p><p>Semantic Matching Models utilize similarity-based scoring function.</p><p>RESCAL is proposed to decompose a three-way tensor X into three matrices that are regarded as a process of factorizing a highdimensional matrix into multiple low-dimensional matrices.</p><p>DistMult proposes a multiplication model to represent the likelihood of a fact.</p><p>ComplEx   aims to deal with both symmetric and antisymmetric relations. It is proposed based on complex embeddings where each entity and relation is represented by a complex vector.</p><p>RotatE   attempts to model and infer different relation types including: symmetry/antisymmetry, inversion, and composition. Each entity is represented as a complex vector and each relation is regarded as a rotation from the source entity to the target entity in the complex vector space.</p><p><strong>第四段</strong></p><p>ConvE   uses convolution over embeddings and multiple layers of nonlinear features to extract the connection between entities and relations. It uses 1-N scoring to increase speed for inference stage.</p><p>InteractE   improves the performance of ConvE by feature permutation, checkered reshaping, and circular convolution.</p><p>ConvKB   adopts CNNs for modeling global relationships among same dimensional entries of the entity and relation embeddings without reshaping. It can explore transitional characteristics that are useful for KG completion.</p><ul><li><p>distance based models: TransE, TransH, TransR, TransD, TransSparse</p></li><li><p>semantic matching models: RESCAL, DistMult, HolE, ComplEx, RotatE,</p></li><li><p>neural network based models: ConvE, ConvKB</p></li></ul><p><strong>第五段</strong></p><p>The above methods do not take into account graph structure information, and the surrounding nodes and relations of the entity contain abundant and valuable structure context information that can help to learn more expressive entity embeddings. Thus, the graph neural networks are introduced to collect local information of the neighborhood for entities.</p><h3 id="B-Graph-Neural-Network">B. Graph Neural Network</h3><p><strong>第一段</strong></p><p>Graph is a non-Euclidean data whose nodes have different numbers of neighbors so that some operations cannot be directly used for graph data anymore.</p><p>Graph Convolutional Networks(GCN) to address the limitations of traditional neural network architectures like CNN that are constrained to handle non-Euclidean data.</p><p>GCNs are deep learning based methods and have been widely used for graph analysis because of its competitive performance and high interpretability.</p><p>Most of the existing GCN methods follow Message Passing Neural Networks (MPNN) framework for node aggregation.</p><p>For GCNs, all neighbors share fixed weights and thus contributing equally during information passing. To address this shortcoming, GAT introduce an attention based architecture to learn the weights of nodes in a neighborhood. The idea is to compute the hidden representations of each node in the graph, by attending over its neighbors, following a self-attention strategy.</p><p><strong>第二段</strong></p><p>GCN-based Models tend to capture the structure characteristics of KGs</p><p>R-GCN introduces a relation-specific transformation and develops GCN to merge the relation information when neighbor aggregating.</p><p>SACN utilizes W-GCN which learns weights that adapt the amount of information from neighbors</p><p>CompGCN proposes various composition operations for neighbor aggregation to model the structure pattern of multirelational graph.</p><p>In addition, Alinet is also based on the GCN model which is applied to knowledge graph alignment tasks.</p><p><strong>第三段</strong></p><p>Different from the above methods, we consider the influence of multi-hop neighbors on target nodes, and propose a novel graph data network architecture to efficiently aggregate implicit information about indirect neighbors</p><h1>3 Methodology(方法)</h1><p>In this section, we describe the detailed framework of the proposed TelepassGCN.</p><p>The target entity representations are learned by a controlled aggregation of their direct and indirect neighborhood information by the gating mechanism.</p><p>The model consists of the following modules:</p><p>a). Multi-hop Neighborhood passing moudule directly collects the information of indirect neighbors of the target node;</p><p>b). Attention mechanism for indirect neightbor moudule is applied to indirect neighbor information to distinguish between different weights;</p><p>c). End to end balancing gate mechanism combines information about direct and indirect neighbors.</p><p>d). Multi-hop virtual graph and random sampling.</p><p>The network architecture is illustrated in Figure ?.</p><p><strong>Multi-hop Neighborhood passing</strong></p><p>The direct neighbors of an entity are the most important neighborhood for GNNs to characterize the entity. In our model, the method of aggregating these neighbor representations is similar to [CompGCN].</p><p>As discussed before, it is not enough to only aggregate onehop neighbors. Although a GCN with L layers can capture the structural information within the entity’s L-hop neighbors, such layer-by-layer propagation is not efficient.</p><p>For multi-hop neighborhood aggregation, we introduce the attention mechanism because directly employing the original aggregation of GCN would cause noise information to propagate through layers.</p><p>Specifically, $W_k^{(l)}$ is the weight matrix. The hidden representation of entity i by aggregating its k-hop neighborhood information at the l-th layer, denoted as $h_{i,m}^{(l)}$, is computed as follows:<br>$$<br>\Large h^{(l)}<em>{i,m}=\sum</em>{k=2}^m \sigma (\sum_{j\in N_k(i)\cup{i}}\alpha_{ij}^{(l)}W_k^{(l)}h_j^{(l-1)})<br>$$<br>where $\alpha_{ij}^{l}$ is a learnable attention weight for entity i and its neighbor j. Let N~k~(·) be the set of k-hop neighbors of the given entity. $h_j$ is the k-hops neighbor of $h_i$. The computation of attention weights is introduced in the next subsection.</p><p><strong>Attention for indirect neightbor</strong></p><p>The number of the more distant neighbors of an entity can grow exponentially than the number of its one-hop neighbors.  It is intuitive that not all the distant neighbors contribute to the characterization of the central entity.  Hence, for two-hop neighborhood aggregation, we compute the attention weights between entities for highlighting useful neighbors.</p><p>As suggested by GAT [35], the attention mechanism applied in our approach is a single layer feedforward neural network parameterized by weight matrix $W_{att} \in R^{1\times d_1}$ and applying LeakyReLU non-linearity.<br>$$<br>\Large b_{u,r} = LeakyReLU(W_{a}m_{(u,v)}) \ \ <br>\Large b_{j} = LeakyReLU(W_a m_{(j,i)})<br>$$</p><p>bu,r denotes absolute attention coefficient of each message eu→v. To get relative attention value, softmax is applied over bu,r :<br>$$<br>\Large<br>\begin{align*}<br>\alpha_{u,r} &amp;= softmax(b_{u,r}) \<br>&amp;= \frac{exp(b_{u,r})}{\sum_{i\in N_v}\sum_{r_\in R_{i,u}}\exp(b_{i,r})} \<br>\end{align*}<br>$$</p><p>$$<br>\Large<br>\begin{align*}<br>\alpha_{u} &amp;= softmax(b_{j}) \<br>&amp;= \frac{exp(b_{j})}{\sum_{t\in N_k{(i)}}\exp(b_{t})} \<br>\end{align*}<br>$$</p><p><strong>End to end balancing gate mechanism</strong></p><p>Inspired by the skipping connections in neural networks. We propose to use the gating mechanism to combine the information from one-hop and two-hop neighbors directly. Specifically, the hidden representation h(l) i of entity i at the l-th layer is computed as follows:<br>$$<br>\Large h_i^{(l)}=g(h_{i,m}^{(l)})\cdot h_{i,1}^{(l)}+(1-g(h_{i,m}^{(l)}))\cdot h_{i,m}^{(l)}<br>$$</p><p>where $g(h_{i,m}^{(l)})=\sigma(Mh_{i,m}^{(l)}+b)$ serves as the gate to control the combination of both one-hop and two-hop neighborhood. M and b are the weight matrix and bias vector, respectively.</p><ol><li><p>这一部分我主要介绍提出的模型, 为了同时收集多阶邻居和一阶邻居, a. 我设计了xxx模块. 通过跳跃传递的方式直接收集目标结点的二阶邻居的信息; b. 对二阶邻居的信息采用注意力机制以区分不同的权重; c. 设计了两端平衡门模块结合一阶信息和多阶信息; d. 对二阶虚拟图进行无放回随机采样, 以确保图可以放在GPU上进行训练</p></li><li><p>分段阐述每个模块(为了达到什么目的, 用什么方法, 怎么做, 目的和作用, 操作和过程(公式))</p><ol><li>Gated Multi-hop Neighborhood Aggregation门控多跳邻居聚合</li><li>注意力</li><li>两端平衡门模块</li></ol></li><li><p>解码器, 训练损失</p></li><li><p>预测: 任务是预测三元组(h, r, t)中缺失的h或t。</p></li></ol><h1>4 Experiments实验(至少30%)</h1><h2 id="Datasets">Datasets</h2><p>==Table I Datasets statistics of FB15k-237 and WN18RR==</p><p>To verify the performance of TelepassGCN, we select two most commonly used public datasets: FB15k-237 and WN18RR.  The detailed statistics of all used dataset are shown in Table I.</p><p>FB15k-237 is a subset of FB15k originally derived from Freebase, which contains general facts of the world. However, the FB15K dataset suffers from the problem of major test leakage through inverse relations, where a large number of test triples could be obtained by inverting triples in the training set. In order to solve this problem, FB15k-237 is created to remove the inverse relations in FB15K. FB15k-237 consists of 14541 entities and 237 relations.</p><p>WN18RR is a subset of WN18 originally derived from WordNet, which mainly consist of hyponym and hypernym relations. WN18 also suffers from the test leakage problem since many test triples are obtained just inverting triples from the training set. WN18RR is created to solve this problem by removing the inverse relations in WN18. WN18RR consists of 40943 entities and 11 relations.</p><blockquote><ul><li><p>FB15K-237</p><ul><li>三元组: 272115个, 去重246236个</li><li>二阶关系图: 12796572, 12792938(去重)</li></ul></li><li><p>WN18RR</p></li><li><p>三元组: 86835个, 去重86726个</p></li><li><p>二阶关系图: 207461, 207376(去重), 去重的意思是: 两个实体之间有多重关系, 去重则意味着只认为有一重关系, 在二阶图中, 认为只有一重关系</p></li></ul></blockquote><h2 id="Baseline">Baseline</h2><p>To demonstrate the effectiveness, we compare the proposed TelepassGCN with the following state-of-the-art methods, which are categorized by us: distanced-based models(TransE , RotatE ), semantic matching models(RESCAL , Distmult [43]), neural network based models(ConvE  , ConvKB ) and GCN-based models(R-GCN , SACN  , COMPGCN  )</p><ul><li>TransE   is the first translation based work in knowledge graph embedding by assuming that the superposition of head and relation embedding h+r should be close to the tail embedding of t.</li><li>RESCAL   is the three-way rank-𝑟 factorization methods which treats each relation as a full rank matrix.</li><li>DistMult   puts forward a simplified bilinear formulation which deals with each relation as a diagonal matrix.</li><li>ComplEx  is proposed to represent entities and relations by complex vectors.</li><li>RotatE   is built upon TransE, which regards relation as a kind of rotation from head entiy to tail entity.</li><li>ConvE   is a representative neural network based models, which leverage 2D convolution and multiple layers of nonlinear transformation to model the complex interaction between entities and relations.</li><li>ConvKB   is also a neural network based model which is an extension of ConvE.</li><li>R-GCN  is an extension of GCNs that operates on local graph neighborhoods to large-scale relational data, which can deal with the highly multi-relational data.</li><li>SACN   proposes an end-to-end structure aware convolutional Network that takes the benefit of GCN and ConvE together.</li><li>COMPGCN   is a general framework which incorporates graph neural network with knowledge graph embedding.</li></ul><h2 id="Evaluation-protocal">Evaluation protocal</h2><p>The link prediction evaluation follows the same protocol as previous works [ConE], in which for each test triple (h,r, t), h and t are replaced by all entities in dataset to calculate scores. Then, following [transE], we apply the filter setting where valid triples already existing in train, valid, and test set are filtered before ranking. Our evaluation protocol is similar to RANDOM evaluation protocol proposed by [re-evaluation of KGC], which is rigorous and fair for knowledge graph completion task to deal with triples with same scores.</p><p>Three standard metrics are reported to evaluate performance, mean reciprocal rank (MRR), mean rank (MR), and the proportion of ranking scores within N of all test triples (Hits@N) for N = 1, 3, and 10. Higher values MRR and Hits@n, lower values of MR demonstrate better performance in KGC task. The following is the formula mode of three standards:</p><ul><li>$ \Large MR=\frac{1}{N}\sum\limits_{i=1}\limits^N rank_i $, N is the number of samples evaluated, rank~i~ represents the sorting value of the correct label for the i-th sample</li><li>$ \Large MRR=\frac{1}{N}\sum\limits_{i=1}\limits^N\frac{1}{rank_i} $</li><li>$\Large Hits@k=\frac{1}{N}\sum\limits_{i=1}\limits^N I(rank_i≤k) $, Hits@k counts the proportion of correct label ranking in the first k of all the evaluated samples, I(x) Is an indicative function. The value of this function is 1 if the argument x is true, and 0 otherwise</li></ul><h2 id="Experimental-Setup">Experimental Setup</h2><p>We implement our model using Pytorch [43] with Adam [44] optimizer. Final parameters of TelepassGCN are determined according to the mean reciprocal rank (MRR) evaluated on validation set. The hyperparameters we find work well are as follows: learning rate 0.001, label smoothing 0.1, 1 layer of GNN and each layer combines the one-hop and two-hop neighbors, initial embedding size 100, output embedding size 200, and batch size 1024 for FB15k-237, batch size 256 for WN18RR.</p><h2 id="Results">Results</h2><p>(分析为主, 效果的提升不要重点说, 为什么有提升, 为什么效果不好, 重点写)</p><blockquote><p>The scores of all baselines reported are directly taken from previous papers [26], [27], [29], [38], [39].</p><p>GNN based methods generally achieve better performance than conventional models like TransE and RAGAT improves upon CrossE’s MRR by a margin of 22%, Hit@10 by a margin of 15.1% on FB15k-237, which shows the effect of leveraging graph structures.</p><p>Compared to other baselines, RAGAT outperforms all other methods 4 out of 5 metrics on FB15k-237 and 3 out of 5 metrics on WN18RR, which indicates the whole effectiveness of our model.  Compared with KBGAT, A2N, and G2SKGE which also utilize attention to aggregate messages, the improvement of RAGAT demonstrates the performance of our proposed enhanced message functions.</p></blockquote><p>The results of TelepassGCN compared against existing knowledge graph embedding methods are summarized in Table 2.  We can see our proposed TelepassGCN obtains competitive results compared with state-of-the-art models.</p><p>Sun et al.   investigated the inappropriate evaluation problem that occurred in KBGAT.  Hence, we take the results from [36] for KBGAT.</p><p>In particular, TelepassGCN achieve a considerable improvement on the FB15k-237 dataset with respect to MRR, Hits@1, and Hits@3, it is also superior to other models using MR on the WN18RR dataset.</p><p>Experimental results are shown in Tab. III. We use MRR, MR and Hits@n to evaluate the performance of different models for KG completion task on each dataset.</p><p>From Tab. III, we can see that our proposed method obtains better performance than the state-of-the-art results on ==all== metrics for FB15K-237, WN18RR.</p><p>For none-GCN based models, though they modify the score functions from different perspectives and learn different representations for different relations, they do not consider the the graph structure information such as direct neighbors, indirect neighbors and implicit instructual information. Thus, TelepassGCN performs better than these methods in expectation, since the proposed method consider the graph structual information.</p><p>For these GCN-based models, even if they can aggregate remote neighbors information of central nodes by increasing the number of GCN layers, this approach is inefficient. In comparison, TelepassGCN integrates information and gets updated from its one-hop and multi-hop neighbors so that the learned embedding contains the structure information which proves to be useful, and thus the performance of TelepassGCN is better than that of these baselines. The results validate the effectiveness of TelepassGCN.</p><p>==TABLE II EXPERIMENTAL RESULTS ON FB15K-237 AND WN18RR TEST SETS==</p><h2 id="Ablation-Study">Ablation Study</h2><ul><li>主实验–表格双栏</li><li>ablation实验</li><li>分析实验, 解释一下内部的机理或者为啥有效果</li><li>可视化实验</li><li>参数实验</li></ul><h1>5 Conclusion结论</h1><ol><li>我们提出了什么<ol><li>包含几个模块</li><li>每个模块的作用, 达到的效果</li></ol></li><li>实验数据<ol><li>实验结果说明了什么</li><li>直接给结论, 不用再写分析之类的</li><li>未来工作(考虑其他因素, 模型部分可以改进点)</li></ol></li></ol><h1>-----------------------------------------</h1><h1>FB数据集</h1><p>奥巴马 /m/02mjmr</p><p>美国 /m/09c7w0</p><h1>实验</h1><p>还要做的事情:</p><ol><li>不同的注意力机制</li><li>不同的一阶与二阶信息结合方式</li><li>不同的解码器, 原本是conve</li></ol><p><strong>CompGCN</strong></p><p>![image-20231114155922723](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20231114155922723.png)</p><p><strong>我的结果</strong></p><p>WN:</p><pre><code>WN_corr_gate_edge_Att_07_11_2023_10:13:02MRR: Tail : 0.50905, Head : 0.45845, Avg : 0.48375MR: Tail : 2894.8, Head : 3092.2, Avg : 2993.5Hit-1: Tail : 0.47192, Head : 0.42438, Avg : 0.44815Hit-3: Tail : 0.52297, Head : 0.46905, Avg : 0.49601Hit-10: Tail : 0.58392, Head : 0.52999, Avg : 0.55696WN512_07_12_2023_08:43:02MRR: Tail : 0.50752, Head : 0.46071, Avg : 0.48412MR: Tail : 2936.8, Head : 3126.9, Avg : 3031.8Hit-1: Tail : 0.47001, Head : 0.42884, Avg : 0.44943Hit-3: Tail : 0.5217, Head : 0.46682, Avg : 0.49426Hit-10: Tail : 0.57945, Head : 0.52808, Avg : 0.55377 WN1024_15_12_2023_16:06:51MRR: Tail : 0.5104, Head : 0.46265, Avg : 0.48653MR: Tail : 3062.9, Head : 3095.0, Avg : 3078.9Hit-1: Tail : 0.47607, Head : 0.42821, Avg : 0.45214Hit-3: Tail : 0.52138, Head : 0.47415, Avg : 0.49777Hit-10: Tail : 0.57881, Head : 0.53095, Avg : 0.55488MRR: Tail : 0.50862, Head : 0.46353, Avg : 0.48607MR: Tail : 3006.9, Head : 3105.2, Avg : 3056.0Hit-1: Tail : 0.47288, Head : 0.42853, Avg : 0.4507Hit-3: Tail : 0.52202, Head : 0.47543, Avg : 0.49872Hit-10: Tail : 0.57881, Head : 0.53255, Avg : 0.55568</code></pre><p>FB:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">FB_test_50_28_11_2023_14</span>:<span class="number">01</span>:<span class="number">14</span></span><br><span class="line"><span class="attribute">MRR</span>: Tail : <span class="number">0</span>.<span class="number">45451</span>, Head : <span class="number">0</span>.<span class="number">25942</span>, Avg : <span class="number">0</span>.<span class="number">35697</span></span><br><span class="line"><span class="attribute">MR</span>: Tail : <span class="number">137</span>.<span class="number">33</span>, Head : <span class="number">321</span>.<span class="number">13</span>, Avg : <span class="number">229</span>.<span class="number">23</span></span><br><span class="line"><span class="attribute">Hit</span>-<span class="number">1</span>: Tail : <span class="number">0</span>.<span class="number">3584</span>, Head : <span class="number">0</span>.<span class="number">17273</span>, Avg : <span class="number">0</span>.<span class="number">26556</span></span><br><span class="line"><span class="attribute">Hit</span>-<span class="number">3</span>: Tail : <span class="number">0</span>.<span class="number">49878</span>, Head : <span class="number">0</span>.<span class="number">28364</span>, Avg : <span class="number">0</span>.<span class="number">39121</span></span><br><span class="line"><span class="attribute">Hit</span>-<span class="number">10</span>: Tail : <span class="number">0</span>.<span class="number">64409</span>, Head : <span class="number">0</span>.<span class="number">43492</span>, Avg : <span class="number">0</span>.<span class="number">5395</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">FB_100_03_12_2023_13</span>:<span class="number">19</span>:<span class="number">06</span></span><br><span class="line"><span class="attribute">MRR</span>: Tail : <span class="number">0</span>.<span class="number">45419</span>, Head : <span class="number">0</span>.<span class="number">25925</span>, Avg : <span class="number">0</span>.<span class="number">35672</span></span><br><span class="line"><span class="attribute">MR</span>: Tail : <span class="number">140</span>.<span class="number">04</span>, Head : <span class="number">303</span>.<span class="number">03</span>, Avg : <span class="number">221</span>.<span class="number">54</span></span><br><span class="line"><span class="attribute">Hit</span>-<span class="number">1</span>: Tail : <span class="number">0</span>.<span class="number">35938</span>, Head : <span class="number">0</span>.<span class="number">17287</span>, Avg : <span class="number">0</span>.<span class="number">26612</span></span><br><span class="line"><span class="attribute">Hit</span>-<span class="number">3</span>: Tail : <span class="number">0</span>.<span class="number">49565</span>, Head : <span class="number">0</span>.<span class="number">28457</span>, Avg : <span class="number">0</span>.<span class="number">39011</span></span><br><span class="line"><span class="attribute">Hit</span>-<span class="number">10</span>: Tail : <span class="number">0</span>.<span class="number">64092</span>, Head : <span class="number">0</span>.<span class="number">43692</span>, Avg : <span class="number">0</span>.<span class="number">53892</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">FB30_07_12_2023_08</span>:<span class="number">53</span>:<span class="number">35</span></span><br><span class="line"><span class="attribute">MRR</span>: Tail : <span class="number">0</span>.<span class="number">45618</span>, Head : <span class="number">0</span>.<span class="number">25771</span>, Avg : <span class="number">0</span>.<span class="number">35695</span></span><br><span class="line"><span class="attribute">MR</span>: Tail : <span class="number">141</span>.<span class="number">39</span>, Head : <span class="number">299</span>.<span class="number">15</span>, Avg : <span class="number">220</span>.<span class="number">27</span></span><br><span class="line"><span class="attribute">Hit</span>-<span class="number">1</span>: Tail : <span class="number">0</span>.<span class="number">36016</span>, Head : <span class="number">0</span>.<span class="number">16935</span>, Avg : <span class="number">0</span>.<span class="number">26476</span></span><br><span class="line"><span class="attribute">Hit</span>-<span class="number">3</span>: Tail : <span class="number">0</span>.<span class="number">50039</span>, Head : <span class="number">0</span>.<span class="number">28252</span>, Avg : <span class="number">0</span>.<span class="number">39145</span></span><br><span class="line"><span class="attribute">Hit</span>-<span class="number">10</span>: Tail : <span class="number">0</span>.<span class="number">64507</span>, Head : <span class="number">0</span>.<span class="number">43814</span>, Avg : <span class="number">0</span>.<span class="number">54161</span></span><br></pre></td></tr></table></figure><h1>目标会议</h1><p><strong>AISTATS</strong></p><p>摘要截稿: 2023-10-06 全文截稿: 2023-10-13 开会时间: 2024-05-02</p><p>摘要截止日期:2023年10月6日(地球上任何地方)</p><p>论文提交截止日期:2023年10月16日(地球上任何地方)</p><p>附录提交截止日期:2023年10月23日(地球上任何地方)</p><p>评审发布日期:2023年11月27日</p><p>作者反驳截止日期:2023年12月5日(地球上任何地方)</p><p>书面决定通知:2024年1月19日</p><p>会议日期:2024年5月2日- 5月4日</p><p><strong>ACCV</strong></p><p>2024年4月初:工作坊提案截止日期  2024年6月初:教程建议截止日期</p><p>2024年7月初:定期论文提交截止日期  2024年9月中旬:定期论文录用通知</p><p><strong>ACML</strong></p><p>全文截稿: 2023-06-23 开会时间: 2023-11-11</p><p>2023年6月30日2023年6月23日提交截止日期(请注意，提交截止日期已延长一周)</p><p>2023年8月18日2023年8月11日向作者发布综述</p><p>2023年8月25日2023年8月18日作者反驳截止日期</p><p>2023年9月8日验收通知</p><p>2023年9月29日提交截止日期</p><p>杂志跟踪</p><p>2023年6月2日2023年5月26日提交截止日期(请注意，提交截止日期已延长一周，并且“S.I: ACML 2023”将很快在b施普林格的编辑管理器中激活)</p><p>2023年7月28日第一轮评审结果(接受、轻微修改或拒绝)</p><p>2023年9月1日2023年8月11日修改稿提交截止日期(小修改论文)</p><p>202308年9月29日2023年9月验收通知</p><p>2023年9月29日提交截止日期</p><p><strong>BMVC</strong></p><p>全文截稿: 2023-05-12 开会时间: 2023-11-20</p><p>工作坊提交截止日期:2023年5月5日，星期五</p><p>==论文提交截止日期:2023年5月12日，星期五==</p><p>补充材料提交作者2023年5月26日星期五</p><p>2023年5月30日，星期二</p><p>评审期开始评审人员2023年6月6日星期二</p><p>提交审稿人2023年7月13日星期四</p><p>博士联合体提交截止日期 2023年7月14日(星期五)</p><p>2023年7月14日星期五，开始反驳期</p><p>反驳期结束2023年7月21日星期五</p><p>审稿人讨论和最终版本审稿人ac 2023年7月31日星期一</p><p>Meta-Reviews于2023年8月6日星期日提交</p><p>论文决策和Meta-Rev整合ac, 2023年8月13日星期日</p><p>作者通知, 2023年8月21日，星期一</p><p>准备好相机的作者, 2023年9月17日星期日</p><p>相机准备工作坊作家(工作坊)2023年10月1日星期日</p><p>海报提交作者2023年11月1日星期三</p><p>视频提交作者2023年11月1日星期三</p><p><strong>NLPCC</strong> 五月截止, 7月通知, 10月开会</p><p><strong>CoNLL</strong> 6月30日截止, 10月通知, 12月开会</p><p><strong>GECCO</strong></p><p>接受率34.7</p><p>论文全文(传统类) : zb</p><ul><li>截稿日期:2024年1月25日</li><li>论文全文提交日期:2024年2月1日</li><li>录用/退稿通知:2024年3月21日</li><li>拍摄截止日期:2024年4月11日</li></ul><p>Poster-only论文:</p><ul><li>海报投稿:2024年2月1日</li><li>录用/退稿通知:2024年3月21日</li><li>拍摄截止日期:2024年4月11日</li></ul><p>论文全文(传统类):</p><p>摘要截稿日期:2023年2月3日</p><p>论文全文提交日期:2023年2月10日</p><p>录用/退稿通知:2023年3月31日</p><p>拍摄截止日期:2023年4月20日</p><p>Poster-only论文:</p><p>海报投稿:2023年2月10日</p><p>录用/退稿通知:2023年3月31日</p><p>拍摄截止日期:2023年4月20日</p><p><strong>ICTAI(Topic 写了知识表示)</strong></p><p>论文提交日期:2023年7月21日</p><p>论文通知:2023年8月31日</p><p>拍摄准备:2023年9月20日</p><p><strong>IROS</strong></p><p>请于2022年1月21日提交RC(机器人竞赛)提案</p><p>Call RC(机器人竞赛)受理通知2022年2月13日</p><p>RA-Letter与IROS期权提交于2022年2月24日</p><p>论文提交日期:2022年3月1日</p><p>工作坊/教程提案2022年3月15日</p><p>工作坊/教程接受通知，2022年4月22日</p><p>论文录用通知，2022年6月30日</p><p>期末论文提交日期:2022年7月31日</p><p>2023年1月15日:特别会议、竞赛和论坛提案截止日期。2023年2月1日:特别会议、竞赛和论坛提案接受通知。</p><p>2023年3月1日延长24小时至2023年3月2日(由于服务器崩溃):论文提交截止日期。</p><p>2023年3月5日:提交纸质视频的截止日期。</p><p>2023年3月15日:工作坊/教程提案截止日期。</p><p>2023年4月28日:工作坊/教程提案接受通知。</p><p>2023年6月30日:论文录用通知。</p><p>2023年7月31日:提交最终论文的截止日期。</p><p>2023年7月15日:提交最新结果的截止日期。</p><p>2023年7月31日:收到迟报结果的通知。</p><p>2023年8月10日:提交最终结果的截止日期</p><p><strong>ALT</strong></p><p>论文提交截止日期: 2023年9月26日</p><p>作者的反馈: 2023年11月11日至17日</p><p>作者通知: 2023年12月中旬</p><p><strong>ICANN</strong></p><p>第二家</p><p>投稿开放时间:2022年12月15日</p><p>特别会议和研讨会提案截止日期:2023年2月28日</p><p>全文和扩展摘要提交截止日期:2023年4月9日2023年4月19日</p><p>录用通知:2023年6月30日</p><p>照片上传时间:2023年7月20日</p><p>作者报名及提前报名截止日期:2023年7月25日</p><p>会议日期:2023年9月26日至29日</p><p><strong>FG</strong></p><p>主要会议:2024年5月28日至30日</p><p>工作坊、教程和比赛:2024年5月27日和5月31日</p><p>主要会议:</p><p>第一轮</p><p>第一轮论文提交(延伸至):2023年10月2日太平洋标准时间晚上11:59(公司)</p><p>作者通知:2023年12月13日</p><p>第二轮</p><p>论文提交时间:2024年1月22日(太平洋时间晚上11:59)</p><p>作者通知:2024年2月28日</p><p>相机准备(所有人):2024年4月22日</p><p>研讨会提案截止日期:2023年11月13日;</p><p>录用通知:2023年11月20日。</p><p>特别会议提案截止日期:2023年11月13日;</p><p>录用通知:2023年11月20日。</p><p><strong>ICDAR</strong></p><p>两年一届 2023, 2025</p><p>重要的日子</p><p>论文提交截止日期:2023年6月30日</p><p>验收通知:2023年7月31日</p><p>摄像机准备及注册:2023年9月15日</p><p>会议日期:2023年11月20-23日</p><p><strong>ILP</strong></p><p>摘要截稿: 2022-05-25</p><p>全文截稿: 2022-05-31</p><p>开会时间: 2022-09-28</p><p>摘要提交2023年7月6日</p><p>论文提交 2023年7月13日</p><p>作者通知2023年8月25日</p><p>2023年9月8日可拍摄</p><p>最新的摘要&amp; 已发表论文提交2023年9月1日</p><p>作者通知2023年9月15日</p><p>ILP会议2023年11月13-15日</p><p><strong>KSEM</strong></p><p>论文提交日期(全文截稿):2024年1月15日</p><p>作者通知:2024年3月1日</p><p>拍摄时间:2024年5月1日</p><p>报名日期:2024年5月1日</p><p>会议日期:2024年8月16-18日</p><p>全文截稿: 2023-04-28</p><p>论文提交(全文截稿):2023年4月28日2023年5月5日</p><p>作者通知:2023年5月25日2023年6月1日</p><p>拍摄时间:2023年6月11日</p><p>提前报名:2023年6月11日2023年6月18日</p><p>会议日期:2023年8月16日至18日</p><p>所有截止日期为23:59 AoE(地球上任何地方)。</p><p><strong>ICONIP</strong></p><p>论文提交截止日期:2023年6月10日2023年6月30日</p><p>验收通知:2023年7月31日</p><p>摄像机准备及注册:2023年9月15日</p><p>会议日期:2023年11月20-23日</p><p><strong>ICPR</strong></p><p>全文截稿: 2024-05-01</p><p>开会时间: 2024-12-01</p><p>论文首次征稿时间:2022年8月</p><p>第二次征稿:2023年8月</p><p>论文投稿开放时间:2024年1月20日</p><p>论文投稿截止日期:2024年3月20日</p><p>接受/拒绝/修订:2024年6月20日</p><p>修改/反驳提交截止日期:2024年7月10日</p><p>最终验收通知:2024年8月5日</p><p>拍照提交时间:2024年8月31日</p><p>会议:2024年12月1-5日</p><p><strong>IJCB</strong></p><p>第二家</p><p>全文截稿: 2023-04-17</p><p>开会时间: 2023-09-25</p><p>拍摄截止日期:2023年7月31日</p><p>特别会议论文截止日期:2023年7月31日</p><p>DC申请截止日期:2023年8月10日</p><p>导师建议截止日期:2023年8月1日</p><p>演示截止日期:2023年8月10日</p><p>提前报名截止日期:2023年8月18日</p><p>会议:2023年9月25-28日</p><p>**IJCNN **</p><p>录用率58.22</p><p>全文截稿: 2023-01-31 zb 开会时间: 2023-06-18</p><p>论文提交截止日期  2024年1月15日</p><p>票据承兑  通知 2024年3月15日</p><p>期末论文提交&amp; 提早报名截止日期 2024年5月1日</p><p>Ieee wcci 2024  日本横滨 2024年6月30日至7月5日</p><p>2022年11月15日 特别会议提案</p><p>2022年12月15日 教程、工作坊及 竞争的建议</p><p>2023年2月7日 提交论文期限</p><p>2023年3月31日 书面决定通知</p><p><strong>PRICAI</strong></p><p>全文截稿: 2023-06-12</p><p>开会时间: 2023-11-17</p><p>-主干线:2023年6月12日(关闭)</p><p>- AI-Impact专场:2023年6月26日(截止)</p><p>录用通知:2023年8月7日(截止)</p><p>提交:2023年8月22日(截止)</p><p>研讨会和教程:2023年11月15日至16日</p><p>主会议:2023年11月17-19日</p>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> 外挂标签 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>复现代码</title>
      <link href="/posts/2013454d.html"/>
      <url>/posts/2013454d.html</url>
      
        <content type="html"><![CDATA[<h1>DisenKGAT</h1><blockquote><p>知识图补全(Knowledge graph completion, KGC)因其对许多下游任务的杰出贡献而成为深度学习领域关注的焦点。尽管最近在KGC上的工作激增，但它们仍然不足以准确捕获复杂的关系，因为它们采用单一和静态的表示。在这项工作中，我们提出了一种新的解纠缠知识图注意网络(DisenKGAT)，它利用微观解纠缠和宏观解纠缠来利用知识图(KGs)背后的表示。为了实现微观解纠缠，我们提出了一种新的关系感知聚合来学习不同的组件表示。对于宏观解纠缠，我们利用互信息作为正则化来增强独立性。在解纠缠的帮助下，我们的模型能够根据给定的场景生成自适应表示。此外，我们的工作具有很强的鲁棒性和灵活性，可以适应各种分数函数。在公共基准数据集上进行了大量实验，以验证DisenKGAT在准确性和可解释性方面优于现有方法。</p></blockquote><p><strong>概率论</strong></p><p>概率密度函数$f(x)$: 面积为1的那条线; 分布函数$F(x)=\int_{-\infin}^xf(t)dt$</p><p><u>联合概率密度: 多个随机变量; 联合分布函数</u>$F(x)=\int_{-\infin}^x\int_{-\infin}^yf(u,v)dudv=P(X\leq x,Y\leq y)$</p><p><u>边缘分布（Marginal Distribution):</u></p><ul><li><u>边缘分布函数:</u> $F_x(x)=P(X\leq x)$</li><li><u>边缘概率密度, 类比偏导数, 边缘的意思就是不管另一个</u> $f_x(x)=\int_{-\infin}^{+\infin}f(x,y)dy$</li></ul><p><strong>熵</strong></p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/149186719">https://zhuanlan.zhihu.com/p/149186719</a></p></blockquote><p>$$<br>\Large Entropy=H§=-\sum\limits_i P(i)log_2P(i)=\mathbb{E}_{x\sim P}[-logP(x)]<br>$$</p><p><strong>交叉熵</strong><br>$$<br>\Large H(P,Q)=\mathbb{E}_{x\sim P}[-logQ(x)]<br>$$<br>使用P计算期望，使用Q计算编码长度；所以H(P,Q)并不一定等于H(Q,P)，除了在P=Q的情况下，H(P,Q) = H(Q,P) = H§。</p><p>期望使用真实概率分布P来计算; 编码长度使用假设的概率分布Q来计算，因为它是预估用于编码信息的</p><p>熵是理论上的平均最小编码长度，所以交叉熵只可能大于等于熵。换句话说，如果我们的估计是完美的，即Q=P，那么有H(P,Q) = H§，否则，H(P,Q) &gt; H§。</p><p><strong>KL-Divergence KL散度</strong></p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/438129018">https://zhuanlan.zhihu.com/p/438129018</a></p></blockquote><p>KL散度描述了用分布Q来估计数据的真实分布P的编码损失<br>$$<br>\Large\mathbb{D}_{KL}(P||Q)=\sum\limits_iP(i)ln(\frac{P(i)}{Q(i)})<br>$$<br>性质: ≥0</p><p><strong>互信息Mutual Information</strong></p><p>互信息用来度量两个变量间的相互依赖性<br>$$<br>I(X; Y)=\sum \limits_{x,y}log\frac{p(x,y)}{p(x)p(y)}=H(X)-H(X|Y)=H(Y)-H(Y|X)=H(X)+H(Y)-H(X,Y)<br>$$<br>其中, I表示互信息, H表示熵</p><p><strong>模型</strong></p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230709170153774.png" alt="image-20230709170153774" style="zoom:50%;" /><ol><li><p>将实体u通过K个矩阵映射到不同的隐空间中, $h^0_{u,k}=\sigma(W_kx_u)$</p></li><li><p>通过信息传递聚合邻居的信息</p></li><li><p>互信息解耦</p><ul><li><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230714182105184.png" alt="image-20230714182105184" style="zoom:50%;" /></li></ul></li><li><p>解码器自适应评分</p></li></ol><h1>RED-GNN</h1><blockquote><p>gpu: 0<br>n_train: 43410 n_valid: 5562 n_test: 5716<br>0.0003, 0.9940, 0.000140,  64, 5, 5, 50, 0.0200,idd</p><p>49[VALID] MRR:0.5398 H@1:0.4926 H@10:0.6290 [TEST] MRR:0.5391 H@1:0.4903 H@10:0.6308 [TIME] train:5956.3370 inference:82.9004</p><p>[VALID] MRR:0.5398 H@1:0.4926 H@10:0.6290 [TEST] MRR:0.5391 H@1:0.4903 H@10:0.6308 [TIME] train:5956.3370 inference:82.9004</p><p>Process finished with exit code 0</p></blockquote><h1>RAGAT</h1><p>解决了什么问题?</p><p>现有的基于GNN的模型的特点的缺点是平等地对待关系并学习固定的网络参数, 如SACN的WGCN, CompGCN, 这些WGCN是每一层共享一个参数矩阵, 也就是每层对应一个矩阵进行线性变换; CompGCN是根据关系的指向共享三个参数矩阵</p><p><strong>pervious message:</strong> $\Large \pmb{m}<em>{(u,r,v)}=M(\pmb{c}</em>{(u,r,v)},\pmb{\theta}_g)$</p><p><strong>pervious message function:</strong> $ \Large \pmb{c}_{(u,r,v)}=\phi(\pmb{e}_u,\pmb{e}_r,\pmb{e}_v)$</p><p><strong>–&gt; RAGAT message:</strong> $\Large \pmb{m}<em>{(u,r,v)}=M(\pmb{c}^r</em>{(u,r,v)},\pmb{\theta}_g)$</p><p><strong>–&gt; RAGAT message function:</strong>$\Large \pmb{c}^r_{(u,r,v)}=\phi(\pmb{e}_u,\pmb{e}_r,\pmb{e}_v,\pmb{\theta}_r)$</p><p>==$\pmb\theta_r=\pmb W_r=diag(\pmb w_r)$==</p><h2 id="A-RELATION-AWARE-MESSAGE-FUNCTIONS">A. RELATION AWARE MESSAGE FUNCTIONS</h2><p>$\Large \pmb{e}_{(ru)}=\pmb{W}_r \pmb{e}_u$</p><ol><li>sub:  $\Large\pmb{c}^r_{(u,r,v)}=e_{ru}-e_r=W_re_u-e_r$</li><li>mult: $\Large\pmb{c}^r_{(u,r,v)}=e_{ru}\circ e_r$; 哈达玛积: 对应元素相乘</li><li>corr: $\Large\pmb{c}^r_{(u,r,v)}=e_{ru}\star e_R$</li><li>concat: $\Large\pmb{c}^r_{(u,r,v)}=W_r[e_u;e_r]$</li><li>cross: $\Large\pmb{c}^r_{(u,r,v)}=W_re_u+W_r(e_u\circ e_r)$</li><li>AN ALGEBRAIC PERSPECTIVE ON CrossE:</li></ol><p>$\Large m_{}(u,r,v)=W_{dir®} \pmb{c}^r_{(u,r,v)}$</p><h2 id="B-ATTENTION-BASED-INFORMATION-AGGREGATION">B. ATTENTION-BASED INFORMATION AGGREGATION</h2><p>$\Large b_{u,r} = LeakyReLU (W_{att} m_{(u,r,v)})$</p><p>$\Large \alpha=softmax(b_{u,r})$</p><p>$\Large e_v’=f(\sum\limits_{(u,r\in N_{(v)})} \alpha_{(u,r)}m_{(u,r,v)})$</p><p><strong>Multi-head attention</strong></p><p>$\Large e_r’ = W_{rel} e_r$</p><h2 id="C-DECODER">C. DECODER</h2><ul><li>ConvE</li><li>InteractE</li></ul><h2 id="Experiment">Experiment</h2><p><strong>Result</strong></p><p>FB15k-237:</p><blockquote><p>MRR: Tail : 0.46139, Head : 0.26656, Avg : 0.36398<br>MR: Tail : 127.13, Head : 288.61, Avg : 207.87<br>Hit-1: Tail : 0.36529, Head : 0.17849, Avg : 0.27189<br>Hit-3: Tail : 0.50557, Head : 0.29268, Avg : 0.39913<br>Hit-10: Tail : 0.64849, Head : 0.4462, Avg : 0.54735</p></blockquote><p>WN18RR:</p><blockquote><p>MRR: Tail : 0.50927, Head : 0.4622, Avg : 0.48573<br>MR: Tail : 1954.2, Head : 2686.4, Avg : 2320.3<br>Hit-1: Tail : 0.46713, Head : 0.42406, Avg : 0.4456<br>Hit-3: Tail : 0.52999, Head : 0.47224, Avg : 0.50112<br>Hit-10: Tail : 0.58966, Head : 0.5402, Avg : 0.56493</p></blockquote><p>gcn_layer2-FB15k-237</p><blockquote><p>MRR: Tail : 0.45514, Head : 0.26578, Avg : 0.36046<br>MR: Tail : 142.61, Head : 248.08, Avg : 195.34<br>Hit-1: Tail : 0.3603, Head : 0.17952, Avg : 0.26991<br>Hit-3: Tail : 0.49731, Head : 0.29048, Avg : 0.3939<br>Hit-10: Tail : 0.64356, Head : 0.44132, Avg : 0.54244</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> 外挂标签 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练语言模型的前世今生</title>
      <link href="/posts/2013454d.html"/>
      <url>/posts/2013454d.html</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/nickchen121/p/16470569.html">https://www.cnblogs.com/nickchen121/p/16470569.html</a></p><h1>预训练</h1><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230417142506600.png" alt="image-20230417142506600" style="zoom:30%;" /><p>==浅层的参数是通用的==, 都是横竖撇捺</p><p>预训练: 通过一个已经训练好的模型A, 去完成一个小数据量的任务B (使用了模型A的浅层参数, 前提是任务A和任务B很相似)</p><p>任务B就可以用A的浅层参数, 后面的参数通过任务B训练: 1. 冻结Frozen(浅层参数不变); 2. 微调Fine-Tuning(浅层参数变)</p><h1>词向量</h1><h2 id="诞生">诞生</h2><p><strong>语言模型</strong></p><p>有两种任务:</p><ul><li>计算这句话出现的概率</li><li>计算每个词在句子的下一个位置出现的概率</li></ul><p><strong>神经网络语言模型</strong></p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230417145514082.png" alt="image-20230417145514082" style="zoom:50%;" /><p>用来预测在context下每个词的概率, 其中w~i~为one-hot编码, 矩阵C为随机初始化的参数, 令输入C(w~i~) = Cw~i~</p><p><strong>词向量</strong>: 矩阵C是可训练的, 当C训练好时, Cw~i~就可以很好的表示词</p><h2 id="Word2Vec">Word2Vec</h2><p>是神经网络语言模型</p><p>NNLM重点是预测下一个词, 而Word2Vec重点是得到一个Q矩阵</p><p>**缺点: **不能表示多义词</p><p>Word2Vec是预训练模型吗? 是</p><p>例如一个NLP的下游任务回答问题, 给定两个句子X, Y, 判断Y是不是X的答案: 先用one-hot, 再使用Word2Vec预训练好的Q矩阵直接得到词向量, 然后进行接下来的任务</p><h1>ELMo</h1><p>Embeddings from Language Models, 不只是训练Q矩阵, 还能把这个词的上下文信息融入到Q矩阵中</p><blockquote><p>ELMo 的本质思想是：我事先用语言模型学好一个单词的 Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用 Word Embedding 的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义再去调整单词的 Word Embedding 表示，这样经过调整后的 Word Embedding 更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以 ELMo 本身是个根据当前上下文对 Word Embedding 动态调整的思路。</p></blockquote><h1>Attention</h1><p>==可以并行==</p><h2 id="Attention本质">Attention本质</h2><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/attention-计算图.png" alt="attention-计算图" style="zoom:90%;" /><p>K (Key, 值在注意力机制下的关键信息) == V(Value, 值的全部信息), Q (Query, 对处理图像来说, Q就是观察图片的人)</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230419110048899.png" alt="image-20230419110048899" style="zoom:30%;" /><h2 id="Self-Attention">Self-Attention</h2><p>K$\approx$V$\approx$Q来源于同一个X, 三者同源, 也就是通过X找到X里面的关键点</p><p>通过三个参数矩阵$W_Q,W_K,W_V, Q=W_QX, K=W_KX, V=W_VX$</p><p>$ Z =  $<img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230421141646692.png" alt="image-20230421141646692" style="zoom:50%;" /></p><p>其中QK^T^是一个word to word的attention map, 加了softmax后行的和为1</p><h2 id="Masked-Self-Attention">Masked Self-Attention</h2><p>为什么要改进: 当我们做生成任务时, 也想对生产的这个单词做注意力计算, 但是, 生成模型生成单词是一个一个生成的, 当生成第一个的时候, 后面的单词是不知道的</p><h2 id="Multi-Head-Self-Attention">Multi-Head Self-Attention</h2><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230419141000182.png" alt="image-20230419141000182" style="zoom:33%;" /><p>X经过自注意力机制得到了Z, Z相比较X有了提升, 通过Multi-Head Self-Attention得到的Z^‘^,</p><p>多头: 将X对8个不同的矩阵W做线性变换, 得到8个Z</p><p>==这样原先在一个位置上的X, 去了空间上8个位置, 通过对8个点进行寻找, 找到更合适的位置==</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230421143545278.png" alt="image-20230421143545278" style="zoom:50%;" /><p>![image-20230421151209379](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230421151209379.png)</p><h2 id="Positional-Encoding">Positional Encoding</h2><p>Attention既然可以并行, 则说明没有了顺序关系, 因此需要加入位置编码</p><h1>Transformer</h1><p>NLP中预训练的目的, 其实就是为了生成词向量</p><p>预训练 $ \rightarrow $ NNLM $ \rightarrow $ Word2Vec $ \rightarrow $ ELMo $ \rightarrow $ Attention, 都属于预训练</p><p>Transformer就是attention的一个堆叠</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230420094349195.png" alt="image-20230420094349195" style="zoom:50%;" /><p>Outputs 是已经生成的单词, 因为生成单词是一个一个生成的</p><p>Linear层作用是将输出转换成词表的维度</p><h2 id="Encoder">Encoder</h2><p>编码器包括两个字层: Self-Attention, Feed Forward( 非线性计算, Relu(WX+b) )</p><p>Encoder也是在做词向量</p><h2 id="Decoder">Decoder</h2><p>翻译模型也是生成模型, 是一个一个生成的所以需要Masked Self-Attention,</p><h1>GPT</h1><p>ELMo是预训练词向量出来</p><p>GPT预训练一个模型出来( 猫狗分类鹅鸭分类那个例子 )</p><h1>Bert</h1><h2 id="MLM">MLM</h2><h2 id="NSP">NSP</h2><h2 id="下游任务改造">下游任务改造</h2>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> 外挂标签 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KG</title>
      <link href="/posts/2013454d.html"/>
      <url>/posts/2013454d.html</url>
      
        <content type="html"><![CDATA[<p>Knowledge graph embedding(KGE = KRL, knowledge representation learning = multi-relation learning = statistical relational learning) is organized from four aspects of representation space, scoring function, encoding models, and auxiliary information.</p><p>得分函数用于衡量事实的合理性, 也就是说对学习好的实体向量和关系向量进行评估</p><p>目前得分函数主要分为3类: 基于==距离==的得分函数、基于==语义匹配==的得分函数、基于==神经网络==的得分函数</p><h2 id="Distance-based-translational">Distance-based(translational)</h2><h3 id="transE">transE</h3><blockquote><p>Translating Embeddings for Modeling Multi-relational Data</p></blockquote><p>TransE就是讲知识图谱中的实体和关系看成两个Matrix:</p><ul><li>实体矩阵结构为n * d, 其中n表示实体数量，d表示每个实体向量的维度，矩阵中的每一行代表了一个实体的词向量；</li><li>关系矩阵结构为r * d, 其中r代表关系数量，d表示每个关系向量的维度, 矩阵中的每一行代表了一个关系的向量；</li></ul><p>知识图谱中的事实是用三元组(h, r, t)表示的, 我们希望h + r = t</p><p>我们需要构造负样本</p><p>![image-20230413145148427](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230413145148427.png)</p><p>**transE的缺点: **无法处理一对多的关系, 如果张三有很多仇人的话, 张三的所有仇人长得都很像</p><h3 id="transH">transH</h3><blockquote><p>Knowledge Graph Embedding by Translating on Hyperplanes</p></blockquote><p>TransH模型的目的就在于处理一对多/多对一的关系问题</p><p>模型的基本思想是针对每一个关系r，将h和t的向量表示投影到一个由向量w~r~确定的超平面得到向量h~⊥~和t~⊥~，在这个超平面上存在一个关系的表示向量r，与TransE一样通过训练使得h~⊥~ + r ≈ t~⊥~，因此每一个关系实际上是由向量w~r~和向量r共同表示的</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230520143943261.png" alt="image-20230520143943261" style="zoom:40%;" /> <img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230520143841502.png" alt="image-20230520143841502" style="zoom:30%;" /><p>score function: <img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230520143759227.png" alt="image-20230520143759227" style="zoom:20%;" /></p><h3 id="transR">transR</h3><blockquote><p>Learning Entity and Relation Embeddings for Knowledge Graph Completion</p></blockquote><p>TransE和TransH模型都假设实体和关系是在同一个语义空间的向量, 这样相似的实体会在空间中相近的位置</p><p>然而每一个实体都可以有很多方面，而不同的关系关注的是实体不同的方面。</p><p>因此，TransR模型对不同的关系建立各自的关系空间，在计算时先将实体映射到关系空间进行计算。因为是在关系空间做向量叠加，所以这个模型叫做TransR。</p><p>使用不同的映射矩阵M~r~定义从实体空间到==各种==关系空间的映射, 为==每一种关系==都定义了映射矩阵M~r~</p><p>transE中实体和关系都在同一个空间, 而transR中的实体在一个空间, 关系在另一个空间, 且维度不相等</p><p>使用不同的映射矩阵M~r~定义从实体空间到==各种==关系空间的映射, 为==每一种关系==都定义了映射矩阵M~r~</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230415123441512.png" alt="image-20230415123441512" style="zoom:40%;" /><p>score function: <img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230415123622841.png" alt="image-20230415123622841" style="zoom:50%;" /></p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230415123635733.png" alt="image-20230415123635733" style="zoom:50%;" /><p>training model: <img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230415123748614.png" alt="image-20230415123748614" style="zoom:50%;" /></p><h2 id="Semantic-Matching">Semantic Matching</h2><h3 id="RESCAL">RESCAL</h3><p>RESCAL基于矩阵分解, 实体关系张量X:</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230416121255245.png" alt="image-20230416121255245" style="zoom:50%;" /><p>X趋于稀疏, 因此RESCAL采用二元分解法, 以<strong>实体向量</strong>和<strong>关系方阵</strong>的形式捕获张量的固有结构</p><p>设实体关系张量X = { X~1~, X~2~, … , X~m~ } ( X~i~为n$\times$n矩阵 ), 则有矩阵分解X~i~ = AR~i~A^T^</p><p>其中A为n$\times$d实体向量矩阵, 每一个关系R~i~对应一个d$\times$d关系方阵</p><p>score function: f~r~(h, t) = h^T^M~r~t, 样本标签为0或1, 所以正样本结果越接近1越好</p><h3 id="DistMult">DistMult</h3><p>在RESCAL模型基础上, 令关系方阵M~r~为关系r的对角矩阵(diagonal matrix)M~r~ = diag®</p><p>极大似然估计的logistic优化<img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230416125552293.png" alt="image-20230416125552293" style="zoom:50%;" /></p><h3 id="HolE">HolE</h3><blockquote><p>Holographic Embeddings of Knowledge Graphs</p></blockquote><p>基于语义匹配的模型的抽象表示: <img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230522100328165.png" alt="image-20230522100328165" style="zoom:50%;" /></p><p>HolE提出了一种新的$\circ$操作: <img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230522100939686.png" alt="image-20230522100939686" style="zoom:35%;" /></p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230522101014169.png" alt="image-20230522101014169" style="zoom:50%;" /><p>所以, HolE模型评估三元组的概率:<img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230522101323472.png" alt="image-20230522101323472" style="zoom:50%;" /></p><p><strong>Circular correlation:</strong></p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230522101456699.png" alt="image-20230522101456699" style="zoom:50%;" /><p><strong>Circular convolution:</strong><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230522101640829.png" alt="image-20230522101640829" style="zoom:50%;" /></p><h2 id="neural-network">neural network</h2><h3 id="ConvE">ConvE</h3><blockquote><p>Convolutional 2D Knowledge Graph Embeddings</p></blockquote><p>用来做链接预测</p><p>思路: 将节点向量和关系向量转换成矩阵, 并且拼接在一起做卷积</p><p>scoring function: <img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230514184225698.png" alt="image-20230514184225698" style="zoom:50%;" /></p><p>![image-20230517142511981](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230517142511981.png)</p><h3 id="ConvKB">ConvKB</h3><blockquote><p>A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network</p></blockquote><p>score表示一个三元组是否有效</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230517140230132.png" alt="image-20230517140230132" style="zoom:40%;" /><h2 id="RGCN">RGCN</h2><blockquote><p>Modeling Relational Data with Graph Convolutional Networks</p><p><a href="https://readpaper.com/pdf-annotate/note?pdfId=4546278475005845505&amp;noteId=1738946434403740160">https://readpaper.com/pdf-annotate/note?pdfId=4546278475005845505&amp;noteId=1738946434403740160</a></p><p>Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling论文中提出的GCN解决Multi-Relational Graph方法存在参数过多的问题, R-GCN通过basis(基)和block-diagonal decomposition(块对角分解)解决了这一问题</p></blockquote><p>上述的得分函数模型单独使用本身算作编码模型</p><p>但我们使用编码器-解码器架构解决链路预测任务, 而编码器就是本章要讲的编码模型, 解码器就是上一章的得分函数, 例如R-GCN</p><p><strong>两大任务:</strong></p><ul><li>实体分类：为实体分配类型或类别属性。</li><li>链接预测：恢复丢失的三元组。</li></ul><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230514171206319.png" alt="image-20230514171206319" style="zoom:40%;" /><p>链路预测任务:</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230514175757713.png" alt="image-20230514175757713" style="zoom:50%;" /><p>==encoder==: R-GCN</p><p>==decoder==: DistMult的评分函数是 头向量$\times$关系矩阵$\times$尾向量 = 一个数字, 数字越大越好(可能值域不在0到1之间), 训练的时候连同关系矩阵一起训练</p><h2 id="SACN">SACN</h2><blockquote><p>End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion</p><p><a href="https://readpaper.com/pdf-annotate/note?pdfId=4546277606726197249&amp;noteId=1742940452317096448">https://readpaper.com/pdf-annotate/note?pdfId=4546277606726197249&amp;noteId=1742940452317096448</a></p></blockquote><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230515154044891.png" alt="image-20230515154044891" style="zoom:100%;" /><p>图中, Metrix Multiplication with entity embedding matrix这一步: 乘以实体嵌入矩阵(公式中是乘e~o~), 结果是e~s~实体在e~r~关系下, 是各个尾节点的概率</p><p>==encoder==: WGCN 每个节点的邻居有不同的权重</p><p>==decoder==: Conv-TransE 实体和关系放在一起做卷积</p><p>来源: <a href="https://zhuanlan.zhihu.com/p/611037206?utm_id=0">https://zhuanlan.zhihu.com/p/611037206?utm_id=0</a></p><h3 id="WGCN">WGCN</h3><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230515154439336.png" alt="image-20230515154439336" style="zoom:50%;" /><h3 id="Conv-TransE">Conv-TransE</h3><p>![截屏2023-05-15 15.46.25](/Users/zhangzizhao/Library/Application Support/typora-user-images/截屏2023-05-15 15.46.25.png)</p><h2 id="KBGAT">KBGAT</h2><blockquote><p>Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</p><p><a href="https://readpaper.com/pdf-annotate/note?pdfId=4500210584108294145&amp;noteId=1742957832339270400">https://readpaper.com/pdf-annotate/note?pdfId=4500210584108294145&amp;noteId=1742957832339270400</a></p></blockquote><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230517141133855.png" alt="image-20230517141133855" style="zoom:30%;" /><p>![image-20230517141236166](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230517141236166.png)</p><p>==Encoder==: 论文模型</p><p>==Decoder==: ConvKB</p><h2 id="CompGCN">CompGCN</h2><blockquote><p>Composition-based Multi-Relational Graph Convolutional Networks</p><p><a href="https://readpaper.com/pdf-annotate/note?pdfId=4556001509620326401&amp;noteId=1742963584156117760">https://readpaper.com/pdf-annotate/note?pdfId=4556001509620326401&amp;noteId=1742963584156117760</a></p></blockquote><p>![image-20230518100340281](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230518100340281.png)</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230518100206575.png" alt="image-20230518100206575" style="zoom:50%;" /><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230518100015387.png" alt="image-20230518100015387" style="zoom:50%;" /><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230518100255023.png" alt="image-20230518100255023" style="zoom:50%;" /><p><strong>CompGCN与其他模型的比较:</strong></p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230518100617303.png" alt="image-20230518100617303" style="zoom:50%;" /><p>训练一个节点时, 同时考虑到关系和==关系指向的节点==</p><p>使用了类似TF-IDF的策略挑选出top k个entity description中的关键词，然后如下图所示，做简单的加和作为该实体的Description-based Representations</p>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> 外挂标签 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GNN</title>
      <link href="/posts/2013454d.html"/>
      <url>/posts/2013454d.html</url>
      
        <content type="html"><![CDATA[<h1>word2vec</h1><h2 id="负采样方案">负采样方案</h2><p>任意两个单词, 看他们在语料库中是不是上下文关系, 是则为1不是则为0, 但是通过窗口扫描出来的所有标签全为1, 增加负样本, 就是增加不相邻的两个单词其标签为0</p><blockquote><p>“Thou shalt ==not== make a machine in the likeness of a human mind”</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230324144131346.png" alt="image-20230324144131346" style="zoom:60%;" /></blockquote><h2 id="word2vec训练流程">word2vec训练流程</h2><ol><li><p>随机生成两个矩阵, 维度是 词汇量$\times$词量量长度, 一个为Embedding矩阵, 把Embedding矩阵训练好后, Embeeding矩阵就是我们的最终结果,每次迭代训练仅更新输入向量; 另一个叫Context 矩阵, 与Embedding矩阵一样表示词表, 每次迭代训练时更新仅更新输出向量</p></li><li><p>word2vec训练时不仅更新模型参数, ==而且更新输入向量和输出向量==, 因此, 模型训练完成后也完成了词向量的嵌入(模型输入一个单预测下一个词)</p></li><li><p>我们认为相邻的词语意思相近, 意思相近的两个词向量点积比较大, 将输入的单词和模型输出的单词做点积, 与标签做对比就可计算损失, error = target - sigmoid(input * output):</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230324150105190.png" alt="image-20230324150105190" style="zoom:50%;" /></li></ol><h1>DeepWalk</h1><p>DeepWalk: Online Learning of Social Representations</p><p>Online指的是如果图中有新的节点新的连接出现, 不需要将全图重新训练一遍</p><ol><li><p>随机游走序列</p></li><li><p>随机初始化所有节点的embedding</p></li><li><p>loss function: <img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230327161136537.png" alt="image-20230327161136537" style="zoom:50%;" /></p><p>Note: Pr({…}) 指的是向训练好的skip-gram模型输入v~i~输出的概率</p></li><li><p>训练: 同时训练更新skip-gram参数和节点embedding, 通过$$\frac{\partial Loss}{\partial skip-gram参数}$$和$$\frac{\partial Loss}{\partial 节点embedding}$$更新</p></li></ol><p><strong>Deepwalk的缺点</strong>: 仅能反映相邻节点的社群相似信息, 无法反映节点的功能角色相似信息</p><h1>Node2Vec</h1><p>Node2Vec在DeepWalk完全随机游走的基础上, 增加p、q参数, 实现有偏好的随机游走.</p><h1>PageRank</h1><p>PageRank用来计算节点重要程度, 和节点相似度</p><ul><li><p>迭代求解线性方程组</p></li><li><p><strong>迭代左乘M矩阵</strong></p><p>![image-20230329134656209](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230329134656209.png)</p><p>r = M · r</p><p>图中的矩阵称为Stochastic matrix, r向量表示pagerank重要程度, r每次左乘代表更新一次pagerank</p></li><li><p>PageWalk</p></li><li><p>在图中随机游走, 访问的节点次数越多节点越重要</p></li><li><p>马尔可夫链</p></li></ul><p>被同一个用户访问过的节点, 更可能是相似的</p><p>用随机游走的访问次数反应节点的亲疏远近</p><p>比如要求Q的相似节点, 就以Q为起点模拟很多次随机游走, 看访问哪个节点次数更多, 随机游走每一步, 每一步都有概率随机传送回Q节点</p><h1>半监督分类</h1><p>semi- supervised node classification</p><ul><li>Transductive 直推式学习: 没有新的节点</li><li>Inductive 归纳式学习: 泛化预测新的节点</li></ul><p><strong>标签传播Label Propagation</strong></p><p>二分类问题, n节点是没有标签的, n节点的值取决于离他最近的几个节点的平均值. 把图中所有无标签的节点遍历一遍又一遍, 直到全图收敛.</p><p>缺点:1.仅用到网络连接信息, 没有用到节点属性特征. 2.不保证收敛</p><p><strong>Iterative Classfiers</strong></p><p><strong>Correct &amp; Smooth</strong></p><p><strong>消息传递Loopy Belief Propagation</strong></p><p><strong>Masked Lable Prediction</strong></p><h1>GCN</h1><blockquote><p>Semi-Supervised Classification with Graph Convolutional Networks</p></blockquote><p>图神经网络解决的是表示学习的问题, 把节点映射成d维向量</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230516102729838.png" alt="image-20230516102729838" style="zoom:50%;" /><h2 id="GCN架构">GCN架构</h2><p><strong>计算图初级版:</strong> 每个节点都可以构建出自己的计算图, 每个计算图就是训练gnn的一个样本点, 这个样本具有特征和标签, 然后用传统机器学习那一套去训练, 就可以得到每个节点的embedding, ==一个距离(一层, 计算图的层数)共享一个神经网络, 如图黑色的是一个神经网络, 白色的是一个==</p><p>![image-20230403103343707](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230403103343707.png)</p><p><strong>计算图的数学形式(1)</strong></p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230403113757035.png" alt="image-20230403113757035" style="zoom: 25%;" /><p>箭头左右都代表将v节点的邻居求平均, A为邻接矩阵, D为度矩阵(对角线为节点度数, 其他为0), Hk代表第k层(计算图的层)所有节点的嵌入向量</p><p>![image-20230403111948328](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230403111948328.png)</p><p>AH^k^代表对每个节点的邻居求和, 所以A~row~H^(k)^完成了对邻居求平均的过程</p><p>缺点: 仅求平均未考虑到节点的重要程度</p><p><strong>计算图的数学形式(2)</strong></p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230403111819754.png" alt="image-20230403111819754" style="zoom:50%;" /><p>对称矩阵, 既考虑了自己的度, 也考虑了对方的度</p><p>缺点: 特征值在(-1, 1), 向量左乘该矩阵后向量长度(幅值)会变小</p><p><strong>计算图的数学形式(3)</strong></p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230403113840618.png" alt="image-20230403113840618" style="zoom:33%;" /><p>特征值在[-1, 1], 最大特征值始终等于1(1永远是特征值), 向量不会变小???(没懂)</p><p>==这一步骤是将向量的邻居的信息整合到一起==</p><p>缺点: 只收集邻居的信息, 没有听自己内心的声音</p><p><strong>计算图终极版</strong></p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230403114810158.png" alt="image-20230403114810158" style="zoom:33%;" /><p>==一个图计算层, 与一层全连接层等价==</p><p>邻接矩阵A = A + E, 表示节点自己与自己相连, 同时度矩阵D对角线全加1</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230403115320325.png" alt="image-20230403115320325" style="zoom:40%;" /><p>整合邻居的信息与整合自己的信息分别使用两套参数</p><h2 id="如何训练GCN">如何训练GCN?</h2><p><strong>Supervised Training</strong></p><p><strong>Unsupervised Training</strong></p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230403151806613.png" alt="image-20230403151806613" style="zoom:50%;" /><h1>GCN代码</h1><p><strong>图数据集Cora</strong></p><p>cora包含7类机器学习的2708篇文章</p><ul><li>节点: 每个论文为一个节点</li><li>特征: 在选出的1435个关键词中, 该文章出现了哪些当中的词, 于是特征为1435维向量, 出现则标为1, 未出现则标为0</li><li>边: 文章之间的引用(有向图)</li><li>标签: 7类</li></ul><p><strong>划分训练集、验证集、测试集</strong></p><p>分两种情况:</p><ol><li><p>Transductive</p><ul><li><p>我们输入的==图结构==可以被训练集、验证集、测试集用到，但是我们训练所有节点的embedding的时候只用训练集节点的标签，而在验证的时候我们就使用刚刚训练集的出来的所有验证集的节点的embedding来做验证。</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230412142302304.png" alt="image-20230412142302304" style="zoom:50%;" /></li></ul></li><li><p>Inductive</p><ul><li><p>把一个整图划分为若干个子图</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230412142416685.png" alt="image-20230412142416685" style="zoom:50%;" /></li></ul></li></ol><p><strong>两层GCN</strong></p><p>![image-20230404165835480](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230404165835480.png)</p><p>其中<img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230404170526893.png" alt="image-20230404170526893" style="zoom:20%;" /></p><p><strong>代码的思路</strong></p><ol><li>节点整合邻居信息, 处理邻接矩阵, 求出A = D^-1^A, 该步完成了对A的归一化, 每行元素加起来等于1, 并且整合了邻居信息<ul><li>图的邻接矩阵A: Cora数据集A是有向图, 将A转化为无向图; A = A + E, 整合邻居和自己的信息</li><li>图的度矩阵D: D^-1^</li></ul></li><li>图神经网络<ul><li>确定网络层数, 该例子中为2层, 第一层为16个输出, 则W^(0)^维度为1433*16 , 第二层为输出层有7个隐藏单元W^(1)^维度为16*7</li><li>随机化参数矩阵W~k~</li><li>训练</li></ul></li></ol><h1>GIN</h1><blockquote><p>HOW POWERFUL ARE GRAPH NEURAL NETWORKS?</p></blockquote><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230424150047898.png" alt="image-20230424150047898" style="zoom:50%;" /><h1>GraphSAGE</h1><blockquote><p>Inductive Representation Learning on Large Graphs</p></blockquote><p>==优势在于对比GCN, 没有用到邻接矩阵==</p><p>GraphSAGE算法要解决transductive(直推式的)问题, GraphSAGE是inductive的, 之前都是保存了映射后的结果，而GraphSAGE保存了生成embedding的映射</p><p>GraphSAGE的核心：GraphSAGE不是试图学习一个图上所有node的embedding，而是学习一个为每个node产生embedding的映射。</p><p>SAGE: Sample and Aggregate</p><h2 id="GraphSAGE算法"><strong>GraphSAGE算法</strong></h2><p>![image-20230412151341228](/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230412151341228.png)</p><h2 id="Neighborhood-definition-采样邻居顶点"><strong>Neighborhood definition - 采样邻居顶点</strong></h2><p>出于对计算效率的考虑，对每个顶点采样一定数量的邻居顶点作为待聚合信息的顶点。设需要的邻居数量，即采样数量为S，若顶点邻居数少于S,则采用有放回的抽样方法，直到采样出S个顶点。若顶点邻居数大于S，则采用无放回的抽样。(即采用有放回的重采样/负采样方法达到S)</p><p>实验发现, K=2时效果就很好了, S1 * S2 ≤ 500即每次只需要扩展20几个邻居性能较高</p><h2 id="aggregator聚合函数的选取"><strong>aggregator聚合函数的选取</strong></h2><p>在图中顶点的邻居是无序的，所以希望构造出的聚合函数是==对称的==（即也就是对它输入的各种排列，函数的输出结果不变）</p><p><strong>Mean aggregator</strong></p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230412152149266.png" alt="image-20230412152149266" style="zoom:50%;" /><p><strong>LSTM aggregator</strong></p><p><strong>Pooling aggregator</strong></p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230412152249144.png" alt="image-20230412152249144" style="zoom:50%;" /><p>pooling聚合器，它既是对称的，又是可训练的。Pooling aggregator 先对目标顶点的邻居顶点的embedding向量进行一次非线性变换，之后进行一次pooling操作(max pooling or mean pooling, 两者效果差别不大)，将得到结果与目标顶点的表示向量拼接，最后再经过一次非线性变换得到目标顶点的第k层表示向量。</p><h2 id="Mini-batch">Mini-batch</h2><p>对与一个大图, 我们用GraphSAGE训练时不需训练整个图, 可以选择一种的部分节点进行训练</p><h2 id="Learning-the-parameters-of-GraphSAGE">Learning the parameters of GraphSAGE</h2><p><strong>无监督损失函数(没看懂)</strong></p><p>基于图的损失函数倾向于使得相邻的顶点有相似的表示，但这会使相互远离的顶点的表示差异变大</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230412152559114.png" alt="image-20230412152559114" style="zoom:50%;" /><ul><li>节点v是节点u随机游走到的邻居</li><li>Q是负样本的数目</li><li>P~n~是负采样的概率分布, 类似word2vec中的负采样</li><li>Z~vn~负采样节点</li></ul><h1>GAT</h1><blockquote><p>Graph Attention Network</p></blockquote><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230425092939233.png" alt="image-20230425092939233" style="zoom:50%;" /><p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230425093745789.png" alt="image-20230425093745789" style="zoom:10%;" />是注意力机制的==参数==</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230425093705029.png" alt="image-20230425093705029" style="zoom:50%;" /><p>加权求和的消息聚合</p><p><strong>multi-head attention</strong></p><p>拼接每层单头消息传递: <img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230425095327080.png" alt="image-20230425095327080" style="zoom:50%;" /></p><p>平均每层单头消息传递: <img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230516101746836.png" alt="image-20230516101746836" style="zoom:45%;" /></p><p>Notes: 1. 以上W都是同一个W; 2. 如果最后的预测层是softmax则需要平均, 如果是全连接层, 则无所谓</p><h1>图网络分类</h1><ol><li>Recurrent Graph Neural Networks</li><li>Graph Convolution Networks</li><li>Graph Attention Networks</li><li>Graph Auto-encoder<ul><li>GAE</li><li>VGAE</li></ul></li><li>Graph Spatial-Temporal Network</li><li>Graph Generative Network</li><li>Graph Reinforcement Network</li><li>Graph Adversarial Methods</li></ol><h1>HAN</h1><blockquote><p>Heterogeneous graph attention network</p><p><a href="https://readpaper.com/pdf-annotate/note?pdfId=4546275151317721089&amp;noteId=1754788902093274368">https://readpaper.com/pdf-annotate/note?pdfId=4546275151317721089&amp;noteId=1754788902093274368</a></p></blockquote><p>知识图谱是一种图的类型，属于异构图的一种，节点和边有不同的种类。</p><p>图神经网络GNNs是处理图结构数据的工具，可以用它来学习生成知识图谱节点或整个图的embedding，其中包含了知识图谱的图结构和图谱信息。这些embedding可以用于其他任务，比如知识图谱的completion，link prediction, 推荐系统等等。</p><p>Heterogeneous graph attention network,</p><img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230426093955332.png" alt="image-20230426093955332" style="zoom:50%;" /><ul><li>(a) Node-level Aggregating<ul><li>与GAT相同</li></ul></li><li>(b) Semantic-level Aggregating<ul><li>求得每个Meta-path的权重: <img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230426094229668.png" alt="image-20230426094229668" style="zoom:30%;" /></li><li>归一化: <img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230426094254198.png" alt="image-20230426094254198" style="zoom:30%;" /></li><li>加权求和: <img src="/Users/zhangzizhao/Library/Application Support/typora-user-images/image-20230426094313714.png" alt="image-20230426094313714" style="zoom:30%;" /></li></ul></li></ul><h1>GTN</h1><h1>metapath2vec</h1><h1>GATNE</h1>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> 外挂标签 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
